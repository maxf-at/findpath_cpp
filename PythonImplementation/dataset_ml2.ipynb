{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import RNA\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import SVG, display\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "import difflib\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from helper import print_moves\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('../')\n",
    "from pretty_print_path import print_moves\n",
    "import findpath_librna\n",
    "import findpath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3451 train examples\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ijd</th>\n",
       "      <th>thisclose</th>\n",
       "      <th>lastclose</th>\n",
       "      <th>cd</th>\n",
       "      <th>en_pos</th>\n",
       "      <th>i_shift</th>\n",
       "      <th>j_shift</th>\n",
       "      <th>insert_or_delete</th>\n",
       "      <th>balance</th>\n",
       "      <th>...</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>...</td>\n",
       "      <td>6.618096e-04</td>\n",
       "      <td>3.768748e-02</td>\n",
       "      <td>3.131309e-01</td>\n",
       "      <td>0.379594</td>\n",
       "      <td>0.067139</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>3.583778e-09</td>\n",
       "      <td>2.872489e-13</td>\n",
       "      <td>3.359235e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>...</td>\n",
       "      <td>6.618096e-04</td>\n",
       "      <td>3.768748e-02</td>\n",
       "      <td>3.131309e-01</td>\n",
       "      <td>0.379594</td>\n",
       "      <td>0.067139</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>3.583778e-09</td>\n",
       "      <td>2.872489e-13</td>\n",
       "      <td>3.359235e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>...</td>\n",
       "      <td>6.618096e-04</td>\n",
       "      <td>3.768748e-02</td>\n",
       "      <td>3.131309e-01</td>\n",
       "      <td>0.379594</td>\n",
       "      <td>0.067139</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>3.583778e-09</td>\n",
       "      <td>2.872489e-13</td>\n",
       "      <td>3.359235e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>...</td>\n",
       "      <td>6.618096e-04</td>\n",
       "      <td>3.768748e-02</td>\n",
       "      <td>3.131309e-01</td>\n",
       "      <td>0.379594</td>\n",
       "      <td>0.067139</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>3.583778e-09</td>\n",
       "      <td>2.872489e-13</td>\n",
       "      <td>3.359235e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>...</td>\n",
       "      <td>6.618096e-04</td>\n",
       "      <td>3.768748e-02</td>\n",
       "      <td>3.131309e-01</td>\n",
       "      <td>0.379594</td>\n",
       "      <td>0.067139</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>3.583778e-09</td>\n",
       "      <td>2.872489e-13</td>\n",
       "      <td>3.359235e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4926</th>\n",
       "      <td>1</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>...</td>\n",
       "      <td>7.875288e-28</td>\n",
       "      <td>2.585954e-14</td>\n",
       "      <td>7.441132e-09</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.009191</td>\n",
       "      <td>0.028921</td>\n",
       "      <td>4.183780e-02</td>\n",
       "      <td>8.569017e-02</td>\n",
       "      <td>1.100865e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4927</th>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.464808e-21</td>\n",
       "      <td>8.502979e-15</td>\n",
       "      <td>2.446749e-09</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.003022</td>\n",
       "      <td>0.009516</td>\n",
       "      <td>1.548947e-02</td>\n",
       "      <td>9.531556e-02</td>\n",
       "      <td>4.157920e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4928</th>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.663043</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.464808e-21</td>\n",
       "      <td>8.502979e-15</td>\n",
       "      <td>2.446749e-09</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.003022</td>\n",
       "      <td>0.009516</td>\n",
       "      <td>1.548947e-02</td>\n",
       "      <td>9.531556e-02</td>\n",
       "      <td>4.157920e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4929</th>\n",
       "      <td>1</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.717391</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.464808e-21</td>\n",
       "      <td>8.502979e-15</td>\n",
       "      <td>2.446749e-09</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.003022</td>\n",
       "      <td>0.009516</td>\n",
       "      <td>1.548947e-02</td>\n",
       "      <td>9.531556e-02</td>\n",
       "      <td>4.157920e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4930</th>\n",
       "      <td>1</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.464808e-21</td>\n",
       "      <td>8.502979e-15</td>\n",
       "      <td>2.446749e-09</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.003022</td>\n",
       "      <td>0.009516</td>\n",
       "      <td>1.548947e-02</td>\n",
       "      <td>9.531556e-02</td>\n",
       "      <td>4.157920e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4931 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target       ijd  thisclose  lastclose  cd    en_pos  i_shift  j_shift  \\\n",
       "0          1  0.363636   0.285714   0.142857   1  0.000000        0        0   \n",
       "1          1  0.181818   0.333333   0.142857   1  0.034483        0        1   \n",
       "2          0  0.818182   0.238095   0.142857   1  0.034483        0        0   \n",
       "3          0  0.954545   0.047619   0.142857   1  0.103448        0        0   \n",
       "4          1  0.636364   0.333333   0.142857   1  0.206897        0        0   \n",
       "...      ...       ...        ...        ...  ..       ...      ...      ...   \n",
       "4926       1  0.166667   1.000000   0.000000   0  1.000000        1        0   \n",
       "4927       1  0.200000   1.000000   1.000000   0  0.000000        0        0   \n",
       "4928       1  0.200000   1.000000   1.000000   1  0.663043        0        1   \n",
       "4929       1  0.400000   1.000000   1.000000   0  0.717391        0        1   \n",
       "4930       1  0.600000   1.000000   1.000000   0  1.000000        1        0   \n",
       "\n",
       "      insert_or_delete   balance  ...            10            11  \\\n",
       "0                    0  0.117647  ...  6.618096e-04  3.768748e-02   \n",
       "1                    0  0.117647  ...  6.618096e-04  3.768748e-02   \n",
       "2                    0  0.117647  ...  6.618096e-04  3.768748e-02   \n",
       "3                    0  0.117647  ...  6.618096e-04  3.768748e-02   \n",
       "4                    0  0.117647  ...  6.618096e-04  3.768748e-02   \n",
       "...                ...       ...  ...           ...           ...   \n",
       "4926                 1  0.035714  ...  7.875288e-28  2.585954e-14   \n",
       "4927                 1  0.000000  ...  1.464808e-21  8.502979e-15   \n",
       "4928                 1  0.000000  ...  1.464808e-21  8.502979e-15   \n",
       "4929                 1  0.000000  ...  1.464808e-21  8.502979e-15   \n",
       "4930                 1  0.000000  ...  1.464808e-21  8.502979e-15   \n",
       "\n",
       "                12        13        14        15        16            17  \\\n",
       "0     3.131309e-01  0.379594  0.067139  0.001733  0.000007  3.583778e-09   \n",
       "1     3.131309e-01  0.379594  0.067139  0.001733  0.000007  3.583778e-09   \n",
       "2     3.131309e-01  0.379594  0.067139  0.001733  0.000007  3.583778e-09   \n",
       "3     3.131309e-01  0.379594  0.067139  0.001733  0.000007  3.583778e-09   \n",
       "4     3.131309e-01  0.379594  0.067139  0.001733  0.000007  3.583778e-09   \n",
       "...            ...       ...       ...       ...       ...           ...   \n",
       "4926  7.441132e-09  0.000005  0.000524  0.009191  0.028921  4.183780e-02   \n",
       "4927  2.446749e-09  0.000002  0.000172  0.003022  0.009516  1.548947e-02   \n",
       "4928  2.446749e-09  0.000002  0.000172  0.003022  0.009516  1.548947e-02   \n",
       "4929  2.446749e-09  0.000002  0.000172  0.003022  0.009516  1.548947e-02   \n",
       "4930  2.446749e-09  0.000002  0.000172  0.003022  0.009516  1.548947e-02   \n",
       "\n",
       "                18            19  \n",
       "0     2.872489e-13  3.359235e-18  \n",
       "1     2.872489e-13  3.359235e-18  \n",
       "2     2.872489e-13  3.359235e-18  \n",
       "3     2.872489e-13  3.359235e-18  \n",
       "4     2.872489e-13  3.359235e-18  \n",
       "...            ...           ...  \n",
       "4926  8.569017e-02  1.100865e-01  \n",
       "4927  9.531556e-02  4.157920e-01  \n",
       "4928  9.531556e-02  4.157920e-01  \n",
       "4929  9.531556e-02  4.157920e-01  \n",
       "4930  9.531556e-02  4.157920e-01  \n",
       "\n",
       "[4931 rows x 30 columns]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file = \"dataset_105_train.csv\"\n",
    "input_file2 = \"dataset_105_vec_train.csv\"\n",
    "\n",
    "dataframe = pd.read_csv(input_file, index_col=0)\n",
    "# dataframe['target'] = np.where(dataframe[\"3\"]==1, 1, 0)\n",
    "dataframe = dataframe.drop(labels=\"s\", axis=1)\n",
    "dataframe = dataframe.drop(labels=\"i\", axis=1)\n",
    "dataframe = dataframe.drop(labels=\"j\", axis=1)\n",
    "# dataframe = dataframe.drop(labels=\"found\", axis=1)\n",
    "\n",
    "# dataframe\n",
    "\n",
    "vec_dataframe = pd.read_csv(input_file2, index_col=0)\n",
    "# vec_dataframe\n",
    "dataframe = pd.concat([dataframe, vec_dataframe], axis=1)\n",
    "\n",
    "\n",
    "# new\n",
    "train, _ = train_test_split(dataframe, test_size=0.3)\n",
    "print(len(train), 'train examples')\n",
    "\n",
    "# old\n",
    "# train, test = train_test_split(dataframe, test_size=0.2)\n",
    "# train, val = train_test_split(train, test_size=0.2)\n",
    "# print(len(train), 'train examples')\n",
    "# print(len(val), 'validation examples')\n",
    "# print(len(test), 'test examples')\n",
    "\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3451 train examples\n",
      "1381 validation examples\n",
      "2070 test examples\n"
     ]
    }
   ],
   "source": [
    "input_file = \"dataset_105_test.csv\"\n",
    "input_file2 = \"dataset_105_vec_test.csv\"\n",
    "\n",
    "dataframe = pd.read_csv(input_file, index_col=0)\n",
    "# dataframe['target'] = np.where(dataframe[\"3\"]==1, 1, 0)\n",
    "dataframe = dataframe.drop(labels=\"s\", axis=1)\n",
    "dataframe = dataframe.drop(labels=\"i\", axis=1)\n",
    "dataframe = dataframe.drop(labels=\"j\", axis=1)\n",
    "# dataframe = dataframe.drop(labels=\"found\", axis=1)\n",
    "\n",
    "# dataframe\n",
    "\n",
    "vec_dataframe = pd.read_csv(input_file2, index_col=0)\n",
    "# vec_dataframe\n",
    "dataframe = pd.concat([dataframe, vec_dataframe], axis=1)\n",
    "\n",
    "\n",
    "_, test = train_test_split(dataframe, test_size=0.2)\n",
    "test, val = train_test_split(train, test_size=0.4)\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop('target')\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds\n",
    "\n",
    "batch_size = 5 # A small batch sized is used for demonstration purposes\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "# ## Understand the input pipeline\n",
    "# # Now that we have created the input pipeline, let's call it to see the format of the data it returns. We have used a small batch size to keep the output readable.\n",
    "\n",
    "# for feature_batch, label_batch in train_ds.take(1):\n",
    "#   print('Every feature:', list(feature_batch.keys()))\n",
    "#   print('A batch of ages:', feature_batch['4'])\n",
    "#   print('A batch of targets:', label_batch )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this batch to demonstrate several types of feature columns\n",
    "example_batch = next(iter(train_ds))[0]\n",
    "\n",
    "# A utility method to create a feature column\n",
    "# and to transform a batch of data\n",
    "def demo(feature_column):\n",
    "  feature_layer = layers.DenseFeatures(feature_column)\n",
    "  # print(feature_layer(example_batch).numpy())\n",
    "\n",
    "# # numeric columns\n",
    "photo_count = feature_column.numeric_column('4')\n",
    "demo(photo_count)\n",
    "photo_count2 = feature_column.numeric_column('5')\n",
    "\n",
    "crossed_feature = feature_column.crossed_column(['4', '1'], hash_bucket_size=10)\n",
    "demo(feature_column.indicator_column(crossed_feature))\n",
    "\n",
    "# # column 7 is a catigorical column\n",
    "# animal_type = feature_column.categorical_column_with_vocabulary_list(\n",
    "#       '7', [0, 1])\n",
    "# animal_type_one_hot = feature_column.indicator_column(animal_type)\n",
    "# demo(animal_type_one_hot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose which columns to use\n",
    "We have seen how to use several types of feature columns. Now we will use them to train a model. The goal of this tutorial is to show you the complete code (e.g. mechanics) needed to work with feature columns. We have selected a few columns to train our model below arbitrarily.\n",
    "\n",
    "Key point: If your aim is to build an accurate model, try a larger dataset of your own, and think carefully about which features are the most meaningful to include, and how they should be represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "\n",
    "# ijd\tthisclose\tlastclose\tcd\ten_pos\n",
    "\n",
    "# numeric cols\n",
    "for header in ['ijd',\t'thisclose',\t'lastclose',\t'en_pos', 'i_shift',\t'j_shift',\t'insert_or_delete']:\n",
    "  feature_columns.append(feature_column.numeric_column(header))\n",
    "\n",
    "# numeric cols\n",
    "for header in ['0',\t'1',\t'2',\t'3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19']:\n",
    "  feature_columns.append(feature_column.numeric_column(header))\n",
    "\n",
    "# categoriacal indicator_columns\n",
    "indicator_column_names = ['cd']\n",
    "for col_name in indicator_column_names:\n",
    "  categorical_column = feature_column.categorical_column_with_vocabulary_list(\n",
    "      col_name, dataframe[col_name].unique())\n",
    "  indicator_column = feature_column.indicator_column(categorical_column)\n",
    "  feature_columns.append(indicator_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use lowest energy as indicator\n",
    "\n",
    "feature_columns = []\n",
    "\n",
    "# numeric cols\n",
    "for header in ['en_pos']:\n",
    "  feature_columns.append(feature_column.numeric_column(header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NumericColumn(key='ijd', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='thisclose', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='lastclose', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='en_pos', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='i_shift', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='j_shift', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='insert_or_delete', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='0', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='1', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='2', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='3', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='4', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='5', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='6', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='7', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='8', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='9', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='10', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='11', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='12', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='13', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='14', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='15', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='16', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='17', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='18', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='19', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='cd', vocabulary_list=(1, 0), dtype=tf.int64, default_value=-1, num_oov_buckets=0))]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a feature layer\n",
    "Now that we have defined our feature columns, we will use a [DenseFeatures](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/DenseFeatures) layer to input them to our Keras model.\n",
    "\n",
    "Earlier, we used a small batch size to demonstrate how feature columns worked. We create a new input pipeline with a larger batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "\n",
    "# batch_size = 32\n",
    "batch_size = 64\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'ijd': <tf.Tensor 'ExpandDims_24:0' shape=(None, 1) dtype=float64>, 'thisclose': <tf.Tensor 'ExpandDims_28:0' shape=(None, 1) dtype=float64>, 'lastclose': <tf.Tensor 'ExpandDims_27:0' shape=(None, 1) dtype=float64>, 'cd': <tf.Tensor 'ExpandDims_21:0' shape=(None, 1) dtype=int64>, 'en_pos': <tf.Tensor 'ExpandDims_22:0' shape=(None, 1) dtype=float64>, 'i_shift': <tf.Tensor 'ExpandDims_23:0' shape=(None, 1) dtype=int64>, 'j_shift': <tf.Tensor 'ExpandDims_26:0' shape=(None, 1) dtype=int64>, 'insert_or_delete': <tf.Tensor 'ExpandDims_25:0' shape=(None, 1) dtype=int64>, 'balance': <tf.Tensor 'ExpandDims_20:0' shape=(None, 1) dtype=float64>, '0': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float64>, '1': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=float64>, '2': <tf.Tensor 'ExpandDims_12:0' shape=(None, 1) dtype=float64>, '3': <tf.Tensor 'ExpandDims_13:0' shape=(None, 1) dtype=float64>, '4': <tf.Tensor 'ExpandDims_14:0' shape=(None, 1) dtype=float64>, '5': <tf.Tensor 'ExpandDims_15:0' shape=(None, 1) dtype=float64>, '6': <tf.Tensor 'ExpandDims_16:0' shape=(None, 1) dtype=float64>, '7': <tf.Tensor 'ExpandDims_17:0' shape=(None, 1) dtype=float64>, '8': <tf.Tensor 'ExpandDims_18:0' shape=(None, 1) dtype=float64>, '9': <tf.Tensor 'ExpandDims_19:0' shape=(None, 1) dtype=float64>, '10': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=float64>, '11': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=float64>, '12': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=float64>, '13': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=float64>, '14': <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=float64>, '15': <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=float64>, '16': <tf.Tensor 'ExpandDims_8:0' shape=(None, 1) dtype=float64>, '17': <tf.Tensor 'ExpandDims_9:0' shape=(None, 1) dtype=float64>, '18': <tf.Tensor 'ExpandDims_10:0' shape=(None, 1) dtype=float64>, '19': <tf.Tensor 'ExpandDims_11:0' shape=(None, 1) dtype=float64>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'ijd': <tf.Tensor 'ExpandDims_24:0' shape=(None, 1) dtype=float64>, 'thisclose': <tf.Tensor 'ExpandDims_28:0' shape=(None, 1) dtype=float64>, 'lastclose': <tf.Tensor 'ExpandDims_27:0' shape=(None, 1) dtype=float64>, 'cd': <tf.Tensor 'ExpandDims_21:0' shape=(None, 1) dtype=int64>, 'en_pos': <tf.Tensor 'ExpandDims_22:0' shape=(None, 1) dtype=float64>, 'i_shift': <tf.Tensor 'ExpandDims_23:0' shape=(None, 1) dtype=int64>, 'j_shift': <tf.Tensor 'ExpandDims_26:0' shape=(None, 1) dtype=int64>, 'insert_or_delete': <tf.Tensor 'ExpandDims_25:0' shape=(None, 1) dtype=int64>, 'balance': <tf.Tensor 'ExpandDims_20:0' shape=(None, 1) dtype=float64>, '0': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float64>, '1': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=float64>, '2': <tf.Tensor 'ExpandDims_12:0' shape=(None, 1) dtype=float64>, '3': <tf.Tensor 'ExpandDims_13:0' shape=(None, 1) dtype=float64>, '4': <tf.Tensor 'ExpandDims_14:0' shape=(None, 1) dtype=float64>, '5': <tf.Tensor 'ExpandDims_15:0' shape=(None, 1) dtype=float64>, '6': <tf.Tensor 'ExpandDims_16:0' shape=(None, 1) dtype=float64>, '7': <tf.Tensor 'ExpandDims_17:0' shape=(None, 1) dtype=float64>, '8': <tf.Tensor 'ExpandDims_18:0' shape=(None, 1) dtype=float64>, '9': <tf.Tensor 'ExpandDims_19:0' shape=(None, 1) dtype=float64>, '10': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=float64>, '11': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=float64>, '12': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=float64>, '13': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=float64>, '14': <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=float64>, '15': <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=float64>, '16': <tf.Tensor 'ExpandDims_8:0' shape=(None, 1) dtype=float64>, '17': <tf.Tensor 'ExpandDims_9:0' shape=(None, 1) dtype=float64>, '18': <tf.Tensor 'ExpandDims_10:0' shape=(None, 1) dtype=float64>, '19': <tf.Tensor 'ExpandDims_11:0' shape=(None, 1) dtype=float64>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "53/54 [============================>.] - ETA: 0s - loss: 0.5443 - accuracy: 0.7005WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'ijd': <tf.Tensor 'ExpandDims_24:0' shape=(None, 1) dtype=float64>, 'thisclose': <tf.Tensor 'ExpandDims_28:0' shape=(None, 1) dtype=float64>, 'lastclose': <tf.Tensor 'ExpandDims_27:0' shape=(None, 1) dtype=float64>, 'cd': <tf.Tensor 'ExpandDims_21:0' shape=(None, 1) dtype=int64>, 'en_pos': <tf.Tensor 'ExpandDims_22:0' shape=(None, 1) dtype=float64>, 'i_shift': <tf.Tensor 'ExpandDims_23:0' shape=(None, 1) dtype=int64>, 'j_shift': <tf.Tensor 'ExpandDims_26:0' shape=(None, 1) dtype=int64>, 'insert_or_delete': <tf.Tensor 'ExpandDims_25:0' shape=(None, 1) dtype=int64>, 'balance': <tf.Tensor 'ExpandDims_20:0' shape=(None, 1) dtype=float64>, '0': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float64>, '1': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=float64>, '2': <tf.Tensor 'ExpandDims_12:0' shape=(None, 1) dtype=float64>, '3': <tf.Tensor 'ExpandDims_13:0' shape=(None, 1) dtype=float64>, '4': <tf.Tensor 'ExpandDims_14:0' shape=(None, 1) dtype=float64>, '5': <tf.Tensor 'ExpandDims_15:0' shape=(None, 1) dtype=float64>, '6': <tf.Tensor 'ExpandDims_16:0' shape=(None, 1) dtype=float64>, '7': <tf.Tensor 'ExpandDims_17:0' shape=(None, 1) dtype=float64>, '8': <tf.Tensor 'ExpandDims_18:0' shape=(None, 1) dtype=float64>, '9': <tf.Tensor 'ExpandDims_19:0' shape=(None, 1) dtype=float64>, '10': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=float64>, '11': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=float64>, '12': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=float64>, '13': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=float64>, '14': <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=float64>, '15': <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=float64>, '16': <tf.Tensor 'ExpandDims_8:0' shape=(None, 1) dtype=float64>, '17': <tf.Tensor 'ExpandDims_9:0' shape=(None, 1) dtype=float64>, '18': <tf.Tensor 'ExpandDims_10:0' shape=(None, 1) dtype=float64>, '19': <tf.Tensor 'ExpandDims_11:0' shape=(None, 1) dtype=float64>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "54/54 [==============================] - 1s 7ms/step - loss: 0.5432 - accuracy: 0.7007 - val_loss: 0.4830 - val_accuracy: 0.7402\n",
      "Epoch 2/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.4737 - accuracy: 0.7664 - val_loss: 0.4688 - val_accuracy: 0.7617\n",
      "Epoch 3/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.4513 - accuracy: 0.7722 - val_loss: 0.4482 - val_accuracy: 0.7896\n",
      "Epoch 4/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.4247 - accuracy: 0.7914 - val_loss: 0.4179 - val_accuracy: 0.7871\n",
      "Epoch 5/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.4003 - accuracy: 0.8024 - val_loss: 0.3994 - val_accuracy: 0.8226\n",
      "Epoch 6/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.3748 - accuracy: 0.8206 - val_loss: 0.3789 - val_accuracy: 0.8188\n",
      "Epoch 7/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.3606 - accuracy: 0.8287 - val_loss: 0.3582 - val_accuracy: 0.8352\n",
      "Epoch 8/150\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.3414 - accuracy: 0.8398 - val_loss: 0.3484 - val_accuracy: 0.8302\n",
      "Epoch 9/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.3164 - accuracy: 0.8569 - val_loss: 0.3446 - val_accuracy: 0.8238\n",
      "Epoch 10/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.3053 - accuracy: 0.8618 - val_loss: 0.3248 - val_accuracy: 0.8542\n",
      "Epoch 11/150\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.2923 - accuracy: 0.8670 - val_loss: 0.3099 - val_accuracy: 0.8657\n",
      "Epoch 12/150\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.2749 - accuracy: 0.8771 - val_loss: 0.3054 - val_accuracy: 0.8454\n",
      "Epoch 13/150\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.2644 - accuracy: 0.8853 - val_loss: 0.3450 - val_accuracy: 0.8657\n",
      "Epoch 14/150\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.2645 - accuracy: 0.8789 - val_loss: 0.2952 - val_accuracy: 0.8783\n",
      "Epoch 15/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.2396 - accuracy: 0.8945 - val_loss: 0.2780 - val_accuracy: 0.8707\n",
      "Epoch 16/150\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.2366 - accuracy: 0.8983 - val_loss: 0.2693 - val_accuracy: 0.8695\n",
      "Epoch 17/150\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.2231 - accuracy: 0.9015 - val_loss: 0.2953 - val_accuracy: 0.8593\n",
      "Epoch 18/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.2267 - accuracy: 0.8989 - val_loss: 0.2730 - val_accuracy: 0.8809\n",
      "Epoch 19/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.2146 - accuracy: 0.9038 - val_loss: 0.2565 - val_accuracy: 0.8834\n",
      "Epoch 20/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.2074 - accuracy: 0.9125 - val_loss: 0.2513 - val_accuracy: 0.8783\n",
      "Epoch 21/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.1999 - accuracy: 0.9139 - val_loss: 0.2531 - val_accuracy: 0.8935\n",
      "Epoch 22/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.1904 - accuracy: 0.9131 - val_loss: 0.2436 - val_accuracy: 0.8720\n",
      "Epoch 23/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.1839 - accuracy: 0.9232 - val_loss: 0.2461 - val_accuracy: 0.8961\n",
      "Epoch 24/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.1764 - accuracy: 0.9270 - val_loss: 0.2260 - val_accuracy: 0.9024\n",
      "Epoch 25/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.1694 - accuracy: 0.9264 - val_loss: 0.2207 - val_accuracy: 0.8872\n",
      "Epoch 26/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.1666 - accuracy: 0.9273 - val_loss: 0.2212 - val_accuracy: 0.9037\n",
      "Epoch 27/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.1596 - accuracy: 0.9302 - val_loss: 0.2191 - val_accuracy: 0.9125\n",
      "Epoch 28/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.1647 - accuracy: 0.9296 - val_loss: 0.2489 - val_accuracy: 0.9062\n",
      "Epoch 29/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.1554 - accuracy: 0.9310 - val_loss: 0.2125 - val_accuracy: 0.9100\n",
      "Epoch 30/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.1457 - accuracy: 0.9363 - val_loss: 0.2176 - val_accuracy: 0.9024\n",
      "Epoch 31/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.1425 - accuracy: 0.9383 - val_loss: 0.2176 - val_accuracy: 0.8986\n",
      "Epoch 32/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.1471 - accuracy: 0.9368 - val_loss: 0.2081 - val_accuracy: 0.9062\n",
      "Epoch 33/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.1293 - accuracy: 0.9449 - val_loss: 0.2191 - val_accuracy: 0.9075\n",
      "Epoch 34/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.1331 - accuracy: 0.9400 - val_loss: 0.2028 - val_accuracy: 0.9189\n",
      "Epoch 35/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.1283 - accuracy: 0.9438 - val_loss: 0.2103 - val_accuracy: 0.9163\n",
      "Epoch 36/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.1271 - accuracy: 0.9455 - val_loss: 0.2100 - val_accuracy: 0.9075\n",
      "Epoch 37/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.1207 - accuracy: 0.9490 - val_loss: 0.1896 - val_accuracy: 0.9303\n",
      "Epoch 38/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.1192 - accuracy: 0.9476 - val_loss: 0.2056 - val_accuracy: 0.9125\n",
      "Epoch 39/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.1098 - accuracy: 0.9528 - val_loss: 0.2123 - val_accuracy: 0.8961\n",
      "Epoch 40/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.1128 - accuracy: 0.9490 - val_loss: 0.2108 - val_accuracy: 0.9037\n",
      "Epoch 41/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.1080 - accuracy: 0.9525 - val_loss: 0.1851 - val_accuracy: 0.9227\n",
      "Epoch 42/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.1152 - accuracy: 0.9493 - val_loss: 0.1819 - val_accuracy: 0.9265\n",
      "Epoch 43/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0958 - accuracy: 0.9594 - val_loss: 0.2011 - val_accuracy: 0.9062\n",
      "Epoch 44/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.1005 - accuracy: 0.9571 - val_loss: 0.2001 - val_accuracy: 0.9189\n",
      "Epoch 45/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0948 - accuracy: 0.9632 - val_loss: 0.1965 - val_accuracy: 0.9290\n",
      "Epoch 46/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0911 - accuracy: 0.9658 - val_loss: 0.1864 - val_accuracy: 0.9227\n",
      "Epoch 47/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0972 - accuracy: 0.9589 - val_loss: 0.1887 - val_accuracy: 0.9202\n",
      "Epoch 48/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0942 - accuracy: 0.9609 - val_loss: 0.1955 - val_accuracy: 0.9240\n",
      "Epoch 49/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0937 - accuracy: 0.9638 - val_loss: 0.1987 - val_accuracy: 0.9189\n",
      "Epoch 50/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0923 - accuracy: 0.9618 - val_loss: 0.1978 - val_accuracy: 0.9202\n",
      "Epoch 51/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0813 - accuracy: 0.9658 - val_loss: 0.1863 - val_accuracy: 0.9227\n",
      "Epoch 52/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0831 - accuracy: 0.9638 - val_loss: 0.1895 - val_accuracy: 0.9316\n",
      "Epoch 53/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0803 - accuracy: 0.9661 - val_loss: 0.2069 - val_accuracy: 0.9151\n",
      "Epoch 54/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0869 - accuracy: 0.9649 - val_loss: 0.1767 - val_accuracy: 0.9354\n",
      "Epoch 55/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0805 - accuracy: 0.9649 - val_loss: 0.1954 - val_accuracy: 0.9227\n",
      "Epoch 56/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0651 - accuracy: 0.9725 - val_loss: 0.1903 - val_accuracy: 0.9341\n",
      "Epoch 57/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0701 - accuracy: 0.9722 - val_loss: 0.1966 - val_accuracy: 0.9366\n",
      "Epoch 58/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0752 - accuracy: 0.9707 - val_loss: 0.1936 - val_accuracy: 0.9278\n",
      "Epoch 59/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0684 - accuracy: 0.9719 - val_loss: 0.1734 - val_accuracy: 0.9303\n",
      "Epoch 60/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0757 - accuracy: 0.9675 - val_loss: 0.1784 - val_accuracy: 0.9316\n",
      "Epoch 61/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0704 - accuracy: 0.9707 - val_loss: 0.2111 - val_accuracy: 0.9341\n",
      "Epoch 62/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0823 - accuracy: 0.9658 - val_loss: 0.3343 - val_accuracy: 0.8999\n",
      "Epoch 63/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0805 - accuracy: 0.9658 - val_loss: 0.1699 - val_accuracy: 0.9404\n",
      "Epoch 64/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0684 - accuracy: 0.9748 - val_loss: 0.1745 - val_accuracy: 0.9379\n",
      "Epoch 65/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0601 - accuracy: 0.9751 - val_loss: 0.1991 - val_accuracy: 0.9303\n",
      "Epoch 66/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0586 - accuracy: 0.9757 - val_loss: 0.1852 - val_accuracy: 0.9341\n",
      "Epoch 67/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0568 - accuracy: 0.9751 - val_loss: 0.1915 - val_accuracy: 0.9379\n",
      "Epoch 68/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0560 - accuracy: 0.9788 - val_loss: 0.2068 - val_accuracy: 0.9354\n",
      "Epoch 69/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0808 - accuracy: 0.9649 - val_loss: 0.1973 - val_accuracy: 0.9341\n",
      "Epoch 70/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0663 - accuracy: 0.9725 - val_loss: 0.1976 - val_accuracy: 0.9240\n",
      "Epoch 71/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0675 - accuracy: 0.9713 - val_loss: 0.2092 - val_accuracy: 0.9303\n",
      "Epoch 72/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0552 - accuracy: 0.9759 - val_loss: 0.2069 - val_accuracy: 0.9214\n",
      "Epoch 73/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0551 - accuracy: 0.9759 - val_loss: 0.1849 - val_accuracy: 0.9455\n",
      "Epoch 74/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0469 - accuracy: 0.9817 - val_loss: 0.2018 - val_accuracy: 0.9392\n",
      "Epoch 75/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0612 - accuracy: 0.9757 - val_loss: 0.2481 - val_accuracy: 0.9252\n",
      "Epoch 76/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0476 - accuracy: 0.9820 - val_loss: 0.1961 - val_accuracy: 0.9392\n",
      "Epoch 77/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0441 - accuracy: 0.9812 - val_loss: 0.1986 - val_accuracy: 0.9379\n",
      "Epoch 78/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0470 - accuracy: 0.9817 - val_loss: 0.1959 - val_accuracy: 0.9341\n",
      "Epoch 79/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0450 - accuracy: 0.9806 - val_loss: 0.2036 - val_accuracy: 0.9442\n",
      "Epoch 80/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0452 - accuracy: 0.9815 - val_loss: 0.2073 - val_accuracy: 0.9430\n",
      "Epoch 81/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0536 - accuracy: 0.9780 - val_loss: 0.2110 - val_accuracy: 0.9366\n",
      "Epoch 82/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0472 - accuracy: 0.9800 - val_loss: 0.2718 - val_accuracy: 0.9227\n",
      "Epoch 83/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0623 - accuracy: 0.9728 - val_loss: 0.2147 - val_accuracy: 0.9404\n",
      "Epoch 84/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0422 - accuracy: 0.9829 - val_loss: 0.1844 - val_accuracy: 0.9468\n",
      "Epoch 85/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0431 - accuracy: 0.9817 - val_loss: 0.2027 - val_accuracy: 0.9493\n",
      "Epoch 86/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0497 - accuracy: 0.9783 - val_loss: 0.2239 - val_accuracy: 0.9341\n",
      "Epoch 87/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0530 - accuracy: 0.9788 - val_loss: 0.2050 - val_accuracy: 0.9202\n",
      "Epoch 88/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0511 - accuracy: 0.9806 - val_loss: 0.1912 - val_accuracy: 0.9392\n",
      "Epoch 89/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0574 - accuracy: 0.9751 - val_loss: 0.2063 - val_accuracy: 0.9455\n",
      "Epoch 90/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0438 - accuracy: 0.9803 - val_loss: 0.2327 - val_accuracy: 0.9430\n",
      "Epoch 91/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0374 - accuracy: 0.9841 - val_loss: 0.2220 - val_accuracy: 0.9468\n",
      "Epoch 92/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0420 - accuracy: 0.9829 - val_loss: 0.1913 - val_accuracy: 0.9366\n",
      "Epoch 93/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0381 - accuracy: 0.9855 - val_loss: 0.2438 - val_accuracy: 0.9442\n",
      "Epoch 94/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0377 - accuracy: 0.9841 - val_loss: 0.2117 - val_accuracy: 0.9404\n",
      "Epoch 95/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0413 - accuracy: 0.9823 - val_loss: 0.2179 - val_accuracy: 0.9468\n",
      "Epoch 96/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0334 - accuracy: 0.9861 - val_loss: 0.2005 - val_accuracy: 0.9430\n",
      "Epoch 97/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0361 - accuracy: 0.9867 - val_loss: 0.2031 - val_accuracy: 0.9316\n",
      "Epoch 98/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0478 - accuracy: 0.9800 - val_loss: 0.2340 - val_accuracy: 0.9278\n",
      "Epoch 99/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0520 - accuracy: 0.9806 - val_loss: 0.2419 - val_accuracy: 0.9303\n",
      "Epoch 100/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0419 - accuracy: 0.9829 - val_loss: 0.2222 - val_accuracy: 0.9366\n",
      "Epoch 101/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0490 - accuracy: 0.9803 - val_loss: 0.2626 - val_accuracy: 0.9316\n",
      "Epoch 102/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0454 - accuracy: 0.9815 - val_loss: 0.1988 - val_accuracy: 0.9442\n",
      "Epoch 103/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0296 - accuracy: 0.9881 - val_loss: 0.2076 - val_accuracy: 0.9430\n",
      "Epoch 104/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0517 - accuracy: 0.9768 - val_loss: 0.2725 - val_accuracy: 0.9087\n",
      "Epoch 105/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0774 - accuracy: 0.9649 - val_loss: 0.1953 - val_accuracy: 0.9430\n",
      "Epoch 106/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0334 - accuracy: 0.9855 - val_loss: 0.2210 - val_accuracy: 0.9455\n",
      "Epoch 107/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0295 - accuracy: 0.9899 - val_loss: 0.2065 - val_accuracy: 0.9468\n",
      "Epoch 108/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0349 - accuracy: 0.9867 - val_loss: 0.1963 - val_accuracy: 0.9404\n",
      "Epoch 109/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0309 - accuracy: 0.9867 - val_loss: 0.2178 - val_accuracy: 0.9430\n",
      "Epoch 110/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0280 - accuracy: 0.9901 - val_loss: 0.2252 - val_accuracy: 0.9531\n",
      "Epoch 111/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0342 - accuracy: 0.9864 - val_loss: 0.2099 - val_accuracy: 0.9468\n",
      "Epoch 112/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0293 - accuracy: 0.9904 - val_loss: 0.2350 - val_accuracy: 0.9455\n",
      "Epoch 113/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0326 - accuracy: 0.9864 - val_loss: 0.2463 - val_accuracy: 0.9328\n",
      "Epoch 114/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0367 - accuracy: 0.9849 - val_loss: 0.2252 - val_accuracy: 0.9392\n",
      "Epoch 115/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0365 - accuracy: 0.9861 - val_loss: 0.2318 - val_accuracy: 0.9366\n",
      "Epoch 116/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0325 - accuracy: 0.9875 - val_loss: 0.2317 - val_accuracy: 0.9455\n",
      "Epoch 117/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0254 - accuracy: 0.9910 - val_loss: 0.2221 - val_accuracy: 0.9493\n",
      "Epoch 118/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0357 - accuracy: 0.9852 - val_loss: 0.2377 - val_accuracy: 0.9341\n",
      "Epoch 119/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0344 - accuracy: 0.9875 - val_loss: 0.2210 - val_accuracy: 0.9442\n",
      "Epoch 120/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0370 - accuracy: 0.9861 - val_loss: 0.2273 - val_accuracy: 0.9455\n",
      "Epoch 121/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0402 - accuracy: 0.9846 - val_loss: 0.2355 - val_accuracy: 0.9417\n",
      "Epoch 122/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0388 - accuracy: 0.9867 - val_loss: 0.2415 - val_accuracy: 0.9430\n",
      "Epoch 123/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0369 - accuracy: 0.9846 - val_loss: 0.2553 - val_accuracy: 0.9417\n",
      "Epoch 124/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0362 - accuracy: 0.9832 - val_loss: 0.2351 - val_accuracy: 0.9468\n",
      "Epoch 125/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0257 - accuracy: 0.9904 - val_loss: 0.2351 - val_accuracy: 0.9430\n",
      "Epoch 126/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0331 - accuracy: 0.9855 - val_loss: 0.2591 - val_accuracy: 0.9392\n",
      "Epoch 127/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0502 - accuracy: 0.9797 - val_loss: 0.2616 - val_accuracy: 0.9354\n",
      "Epoch 128/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0361 - accuracy: 0.9852 - val_loss: 0.2229 - val_accuracy: 0.9468\n",
      "Epoch 129/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0320 - accuracy: 0.9861 - val_loss: 0.2506 - val_accuracy: 0.9468\n",
      "Epoch 130/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0220 - accuracy: 0.9922 - val_loss: 0.2179 - val_accuracy: 0.9417\n",
      "Epoch 131/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0250 - accuracy: 0.9896 - val_loss: 0.2311 - val_accuracy: 0.9493\n",
      "Epoch 132/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0406 - accuracy: 0.9852 - val_loss: 0.2958 - val_accuracy: 0.9392\n",
      "Epoch 133/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0585 - accuracy: 0.9768 - val_loss: 0.2568 - val_accuracy: 0.9430\n",
      "Epoch 134/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0430 - accuracy: 0.9829 - val_loss: 0.2894 - val_accuracy: 0.9392\n",
      "Epoch 135/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0290 - accuracy: 0.9893 - val_loss: 0.2538 - val_accuracy: 0.9392\n",
      "Epoch 136/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0236 - accuracy: 0.9904 - val_loss: 0.2316 - val_accuracy: 0.9480\n",
      "Epoch 137/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0176 - accuracy: 0.9954 - val_loss: 0.2377 - val_accuracy: 0.9442\n",
      "Epoch 138/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0295 - accuracy: 0.9870 - val_loss: 0.2547 - val_accuracy: 0.9392\n",
      "Epoch 139/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0260 - accuracy: 0.9910 - val_loss: 0.2263 - val_accuracy: 0.9442\n",
      "Epoch 140/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0244 - accuracy: 0.9922 - val_loss: 0.2247 - val_accuracy: 0.9506\n",
      "Epoch 141/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0231 - accuracy: 0.9904 - val_loss: 0.2315 - val_accuracy: 0.9468\n",
      "Epoch 142/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0216 - accuracy: 0.9904 - val_loss: 0.2345 - val_accuracy: 0.9468\n",
      "Epoch 143/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0179 - accuracy: 0.9933 - val_loss: 0.2314 - val_accuracy: 0.9506\n",
      "Epoch 144/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0195 - accuracy: 0.9939 - val_loss: 0.2290 - val_accuracy: 0.9506\n",
      "Epoch 145/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0179 - accuracy: 0.9936 - val_loss: 0.2278 - val_accuracy: 0.9455\n",
      "Epoch 146/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0180 - accuracy: 0.9936 - val_loss: 0.2516 - val_accuracy: 0.9468\n",
      "Epoch 147/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0242 - accuracy: 0.9919 - val_loss: 0.2316 - val_accuracy: 0.9493\n",
      "Epoch 148/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0228 - accuracy: 0.9901 - val_loss: 0.3122 - val_accuracy: 0.9366\n",
      "Epoch 149/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0346 - accuracy: 0.9855 - val_loss: 0.2827 - val_accuracy: 0.9404\n",
      "Epoch 150/150\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0436 - accuracy: 0.9815 - val_loss: 0.2590 - val_accuracy: 0.9417\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f081c918310>"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 150\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  feature_layer,\n",
    "  # layers.Dense(128, activation='relu'),\n",
    "  # layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(256, activation='relu'),\n",
    "  layers.Dense(256, activation='relu'),\n",
    "  layers.Dropout(.1),\n",
    "  layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_ds,\n",
    "          validation_data=val_ds,\n",
    "          epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1718 - accuracy: 0.9574\n",
      "Accuracy 0.957446813583374\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 2ms/step - loss: 0.4810 - accuracy: 0.8480\n",
      "Accuracy 0.848024308681488\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 964us/step - loss: 0.5752 - accuracy: 0.7214\n",
      "Accuracy 0.7213779091835022\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 31/31 [==============================] - 0s 1ms/step - loss: 0.6488 - accuracy: 0.7335\n",
    "# Accuracy 0.7335359454154968\n",
    "\n",
    "# 31/31 [==============================] - 0s 862us/step - loss: 0.5798 - accuracy: 0.6950\n",
    "# Accuracy 0.695035457611084"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model as a SavedModel.\n",
    "# !mkdir -p saved_model\n",
    "# model.save('saved_model/my_model')\n",
    "\n",
    "# from tensorflow.keras.models import Sequential, save_model, load_model\n",
    "\n",
    "# filepath = './saved_model'\n",
    "# save_model(model, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morgan & Higgs: Findpath with SW 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True) # no scientific notation\n",
    "\n",
    "# import feature_generation\n",
    "from features import ij_distance, new_move_dist, plt_moves, config_distance, balance_in_all_things, return_shift_moves\n",
    "\n",
    "from process_features import fp_call, find_moves, process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_file = \"dataset_100.csv\"\n",
    "# df = pd.read_csv(input_file)\n",
    "\n",
    "# sequence, s1, s2 = df.loc[0]\n",
    "\n",
    "\n",
    "# search_width_multiplier = 4\n",
    "# fp = findpath.findpath_single(sequence, s1, s2, search_width_multiplier=search_width_multiplier, mp=True)\n",
    "# result = fp.get_en()/100.0\n",
    "# path = fp.get_path()\n",
    "\n",
    "# s = s1\n",
    "# pt2 = list(RNA.ptable(s2))\n",
    "# fc = RNA.fold_compound(sequence)\n",
    "\n",
    "\n",
    "# def find_moves(s_ptable, t_ptable):\n",
    "#     \"\"\"\n",
    "#     generator function, yields possible structures 1 move away\n",
    "#     from the original structure by finding fitting i and j with\n",
    "#     RNA pair and loop tables\n",
    "#     s_ptable: current ptable\n",
    "#     t_ptable: s2 end ptable\n",
    "#     \"\"\"\n",
    "#     # loop table\n",
    "#     ls = RNA.loopidx_from_ptable(s_ptable)\n",
    "#     for i in range(len(s_ptable)):\n",
    "#         if i == 0:\n",
    "#             continue\n",
    "#         if s_ptable[i] == 0 and t_ptable[i] > i:\n",
    "#             j = t_ptable[i]\n",
    "#             # found j has to be empty and currently on the same loop as i\n",
    "#             if s_ptable[j] == 0 and ls[i] == ls[j]:\n",
    "#                 yield i, j\n",
    "#         # test for bp removal: i has to be paired with a different j in s2\n",
    "#         j = s_ptable[i]\n",
    "#         # dont remove things which are present in s2\n",
    "#         if s_ptable[i] > i and s_ptable[i] != s_ptable[j] and\\\n",
    "#                 s_ptable[i] != t_ptable[i] and s_ptable[j] != t_ptable[j]:\n",
    "#             yield -i, -j\n",
    "\n",
    "\n",
    "# def fp_call(sequence, s1, s2, search_width_multiplier = 20):    \n",
    "#     fp = findpath.findpath_single(sequence, s1, s2, search_width_multiplier=search_width_multiplier, mp=True)\n",
    "#     result = fp.get_en()/100.0\n",
    "#     path = fp.get_path()\n",
    "#     # return result, path\n",
    "#     return result, path\n",
    "\n",
    "\n",
    "# def ij_distance(last_move, this_move, ij_moves):\n",
    "#     # how far is the last move away from the current move.\n",
    "#     # it is likely, that the next move is close to the last one\n",
    "#     # there are better distance metrices probably...\n",
    "\n",
    "#     # ij move list is supposed to be sorted, find indices\n",
    "    \n",
    "#     ijmoves = ij_moves + [last_move]\n",
    "#     ijmoves.sort(key=lambda x: (x[0], x[1]))\n",
    "\n",
    "#     pos_old = ijmoves.index(last_move)\n",
    "#     pos_new = ijmoves.index(this_move)\n",
    "\n",
    "#     distance = abs(pos_old-pos_new)/len(ijmoves)\n",
    "    \n",
    "    \n",
    "#     # moves left in vicinity out of total moves\n",
    "    \n",
    "#     thisi, thisj = this_move\n",
    "#     lasti, lastj = last_move\n",
    "#     thisclose = 0\n",
    "#     lastclose = 0\n",
    "\n",
    "#     for i, j in ij_moves:\n",
    "#         if (abs(i-thisi) < 5) and (abs(j-thisj) < 5):\n",
    "#             thisclose += 1\n",
    "#         if (abs(i-lasti) < 5) and (abs(j-lastj) < 5):\n",
    "#             lastclose += 1\n",
    "    \n",
    "\n",
    "#     thisclose /= len(ij_moves)\n",
    "#     lastclose /= len(ij_moves)\n",
    "\n",
    "#     # print (\"thisclose\", thisclose, lastclose, distance)\n",
    "#     return distance, thisclose, lastclose\n",
    "\n",
    "#     print (distance)\n",
    "\n",
    "# # sample call\n",
    "# ij_moves = [(3, 62), (4, 61), (6, 60), (7, 59), (9, 57), (10, 56), (11, 55), (12, 54), (15, 52), (16, 51), (17, 50), (18, 39), (19, 38), (20, 36), (21, 35), (22, 34), (23, 33), (24, 32), (42, 49), (43, 48), (64, 99)]\n",
    "# last_move = (2, 63)\n",
    "# this_move = (6, 60)\n",
    "# ij_distance(last_move, this_move, ij_moves)\n",
    "\n",
    "# # \n",
    "\n",
    "# def config_distance(pt, move):\n",
    "#     \"\"\"\n",
    "#     are we extending / removing the outside/inside layer of a loop or adding something in the middle?\n",
    "#     \"\"\"\n",
    "#     i = move[0]\n",
    "#     j = move[1]\n",
    "#     points = 0\n",
    "\n",
    "#     # if we're extending from outside to inside, the position i+1 and j-1 should be ideally unpaired\n",
    "#     # inside to outside: i-1 and j+1 should be ideally unpaired\n",
    "\n",
    "#     if i>0:\n",
    "#         # print (\"add\") \n",
    "#         # outside/inside paired?        \n",
    "#         if j+1 < pt[0] and i-1 > 0: # outside - boundary check\n",
    "#             if pt[i-1] == j+1:\n",
    "#                 points += 1\n",
    "#         if pt[i+1] == j-1:\n",
    "#             points += 1\n",
    "#     if i<0:\n",
    "#         # print (\"del\")\n",
    "#         i, j = -i, -j\n",
    "#         # outside/inside paired?\n",
    "#         if j+1 < pt[0] and i-1 > 0: # outside - boundary check\n",
    "#             if pt[i-1] == j+1:\n",
    "#                 points += 1\n",
    "#         if pt[i+1] == j-1:\n",
    "#             points += 1\n",
    "#         elif pt[i+1] == 0 and pt[j-1] == 0:\n",
    "#             pass\n",
    "\n",
    "#     if points == 2:\n",
    "#         points = 0\n",
    "#     return points\n",
    "\n",
    "\n",
    "# s = s1\n",
    "# lasts = s\n",
    "# lasti = None\n",
    "# lastj = None\n",
    "\n",
    "# for e, (a,b, en) in enumerate(path):\n",
    "#     if (a,b) == (0,0):\n",
    "#         continue  \n",
    "\n",
    "#     # check where we can go, compare with our best move. \n",
    "#     pt = list(RNA.ptable(s))\n",
    "\n",
    "#     # check available moves, save them, sort them    \n",
    "#     avail_moves = []\n",
    "#     ij_moves = []\n",
    "#     found_pos = None\n",
    "\n",
    "#     for pos, (i,j) in enumerate(find_moves(pt, pt2)):    \n",
    "#         next_en = fc.eval_move_pt(pt, i, j)\n",
    "#         # mark where we found our move\n",
    "#         found = (i,j) == (a,b)\n",
    "#         avail_moves.append((i, j, next_en, found))\n",
    "#         ij_moves.append((abs(i),abs(j)))\n",
    "\n",
    "#     # sort moves independent of delete insert moves\n",
    "\n",
    "#     ij_moves.sort(key=lambda x: (x[0], x[1]))\n",
    "\n",
    "\n",
    "#     avail_moves.sort(key=lambda x: x[2])\n",
    "#     found_list = [x[3] for x in avail_moves]\n",
    "#     en_list = np.array([[x[2] for x in avail_moves]])\n",
    "#     en_list_scaled = min_max_scaler.fit_transform(en_list.T).T[0]\n",
    "    \n",
    "\n",
    "#     # find where our move is after sorting\n",
    "#     found_pos = found_list.index(True)\n",
    "#     rel_pos = found_pos * 1.0 / len(found_list)\n",
    "\n",
    "#     # print (e, a,b, 'found at pos:', found_pos, 'of', len(avail_moves), ':',  1-rel_pos)\n",
    "#     # print (avail_moves, a, b)\n",
    "\n",
    "#     # if s != lasts:\n",
    "#         # print (\"---\") \n",
    "#         # print (s)\n",
    "#         # print (\"---\") \n",
    "\n",
    "#     # for every move we take we have to run a new findpath, see if this move will yield the ideal result\n",
    "    \n",
    "#     for pos, (i,j, en, found) in enumerate(avail_moves):\n",
    "#         if i > 0:\n",
    "#             snew = s[:i-1] + \"(\" + s[i:j-1] + \")\" + s[j:]\n",
    "#         if i < 0:\n",
    "#             snew = s[:-i-1] + \".\" + s[-i:-j-1] + \".\" + s[-j:]\n",
    "#         ptnew = list(RNA.ptable(snew))\n",
    "\n",
    "#         result_new, path = fp_call(sequence, snew, s2)\n",
    "\n",
    "#         if result_new <= result:\n",
    "#             pos_result = 1\n",
    "#         else:\n",
    "#             pos_result = 0\n",
    "\n",
    "#         if found: found = \"<-- taken\"\n",
    "#         else: found = \"\"\n",
    "\n",
    "#         this_move = (abs(i), abs(j))\n",
    "#         last_move = (lasti, lastj)\n",
    "\n",
    "#         if lasti:\n",
    "#             # print (this_move, last_move, ij_moves)\n",
    "#             ijd, thisclose, lastclose = ij_distance(last_move, this_move, ij_moves)\n",
    "#         else:\n",
    "#             ijd, thisclose, lastclose = 0, 0, 0\n",
    "\n",
    "#         cd = config_distance(pt, this_move)\n",
    "\n",
    "#         if lasti:\n",
    "#             print (f' {snew[0:20]}, {i}, {j}, {result_new}/{pos_result}: {ijd:2.2f}, {thisclose:2.2f}, {lastclose:2.2f}, {cd}, {en}, {en_list_scaled[pos]:2.2f} {found}')\n",
    "\n",
    "#             sample = 1, ijd, thisclose, lastclose, cd, en_list_scaled[pos], 1\n",
    "#             sample_test_ds = sample_to_dataset(sample, test)\n",
    "#             result_predictor = model.predict(sample_test_ds)[0][0]\n",
    "\n",
    "#             print (f' {1}, {ijd:2.2f}, {thisclose:2.2f}, {lastclose:2.2f}, {cd}, {en_list_scaled[pos]:2.2f}, 1: prediction: {result_predictor}')\n",
    "\n",
    "\n",
    "\n",
    "#     # if e==63:\n",
    "#     #     print (avail_moves)\n",
    "\n",
    "#     # update s for the next iteration\n",
    "    \n",
    "#     lasts = s\n",
    "#     lasti = abs(a)\n",
    "#     lastj = abs(b)\n",
    "#     if a > 0:\n",
    "#         s = s[:a-1] + \"(\" + s[a:b-1] + \")\" + s[b:]\n",
    "#     if a < 0:\n",
    "#         s = s[:-a-1] + \".\" + s[-a:-b-1] + \".\" + s[-b:]\n",
    "\n",
    "#     if e>1: \n",
    "#         break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.633],\n",
       "       [15.434],\n",
       "       [-4.084],\n",
       "       ...,\n",
       "       [-3.661],\n",
       "       [ 5.052],\n",
       "       [-3.358]], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A few random samples\n",
    "\n",
    "# Generate predictions for samples\n",
    "predictions = model.predict(test_ds)\n",
    "predictions\n",
    "# sample_test\n",
    "# test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.1009778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.101]], dtype=float32)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A utility method to create a tf.data from a dictionary\n",
    "def dict_to_dataset(inputdict, labels, shuffle=True, batch_size=32):\n",
    "  # dataframe = dataframe.copy()\n",
    "  # labels = dataframe.pop('target')\n",
    "  ds = tf.data.Dataset.from_tensor_slices((inputdict, labels))\n",
    "  # ds = tf.data.Dataset.from_tensor_slices([45240,  0.5,  0.142857,  0.047619,  0,  0.930233,       0])\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))  \n",
    "  print ('ds', ds)\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds\n",
    "\n",
    "def sample_to_dataset(sample, test):\n",
    "\n",
    "  sample_test = test.iloc[0:1].copy()\n",
    "  row0 = sample_test.index[0]\n",
    "\n",
    "  # print (sample_test)\n",
    "\n",
    "  for key, value in sample.items():\n",
    "    sample_test.at[row0, key] = value\n",
    "# \n",
    "  # print (sample_test)\n",
    "\n",
    "  # print (i, j)\n",
    "  # sample_test.at[row0 ,'Unnamed: 0'] = 2 # irrelevant\n",
    "  # sample_test.at[row0 ,'4'] = sample[1]\n",
    "  # sample_test.at[row0 ,'5'] = sample[2]\n",
    "  # sample_test.at[row0 ,'6'] = sample[3]\n",
    "  # sample_test.at[row0 ,'7'] = sample[4]\n",
    "  # sample_test.at[row0 ,'8'] = sample[5]\n",
    "  # sample_test.at[row0 ,'target'] = 2 # irrelevant\n",
    "\n",
    "  sample_test_ds = df_to_dataset(sample_test, shuffle=False, batch_size=batch_size)\n",
    "  return sample_test_ds\n",
    "\n",
    "\n",
    "sample = { \"target\": 0,\n",
    "           \"ijd\":    1,\n",
    "           \"thisclose\": 0.2,\n",
    "           \"lastclose\": 0.5,\n",
    "           \"cd\":        0.5,\n",
    "           \"en_pos\":    0.7,\n",
    "           \"i_shift\":   1, \n",
    "           \"j_shift\":   1,\n",
    "           \"insert_or_delete\": 0,\n",
    "           \"balance\":   1,\n",
    "           \"19\": 0.5\n",
    "}\n",
    "\n",
    "# sample = [1, 0.82, 0.44, 0.24, 1, 0.05, 1]\n",
    "# sample = 1, 0.36, 0.29, 0.14, 1, 0.00, 1\n",
    "sample_test_ds = sample_to_dataset(sample, test)\n",
    "\n",
    "# print (sample_test_ds)\n",
    "\n",
    "result_predictor = model.predict(sample_test_ds)[0][0]\n",
    "print (result_predictor)\n",
    "\n",
    "\n",
    "predictions = model.predict(sample_test_ds)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCUUUGGAUUUCCGGGCCAUUAACGCACCCCGUCUAGUAUCCCACUAUGGCAAGCUAACUUAAAGGUCUAGAAAGCCGAACCAGCUCCCGUCUAGACCGU\n",
      "((((.((....))(((............)))(((((((.....)))).))))))).........((((((((....(......)......))))))))..\n",
      "...((((.(((....((((...(((.....))).((((.....)))))))))))))))......((((((((.(((.......)))....))))))))..\n",
      "GCUUUGGAUUUCCGGGCCAUUAACGCACCCCGUCUAGUAUCCCACUAUGGCAAGCUAACUUAAAGGUCUAGAAAGCCGAACCAGCUCCCGUCUAGACCGU\n",
      "((((.((....))(((............)))(((((((.....)))).))))))).........((((((((....(......)......)))))))).. \u001b[93m[   0,    0 ]\u001b[0m -19.30\n",
      "((((.((....))(((............)))(((((((.....)))).))))))).........((((((((....\u001b[93m\u001b[1m.\u001b[0m......\u001b[93m\u001b[1m.\u001b[0m......)))))))).. \u001b[93m[ -77,  -84 ]\u001b[0m -19.20\n",
      "(((\u001b[93m\u001b[1m.\u001b[0m.((....))(((............)))(((((((.....)))).)))\u001b[93m\u001b[1m.\u001b[0m))).........((((((((..................)))))))).. \u001b[93m[  -4,  -52 ]\u001b[0m -18.40\n",
      "((\u001b[93m\u001b[1m.\u001b[0m..((....))(((............)))(((((((.....)))).))).\u001b[93m\u001b[1m.\u001b[0m)).........((((((((..................)))))))).. \u001b[93m[  -3,  -53 ]\u001b[0m -17.10\n",
      "((...((....))(((............)))(((((((.....)))).)))..)).........((((((((...\u001b[93m\u001b[1m(\u001b[0m.......\u001b[93m\u001b[1m)\u001b[0m......)))))))).. \u001b[93m[  76,   84 ]\u001b[0m -14.80\n",
      "((...((....))(((............)))(((((((.....)))).)))..)).........((((((((..\u001b[93m\u001b[1m(\u001b[0m(.......)\u001b[93m\u001b[1m)\u001b[0m.....)))))))).. \u001b[93m[  75,   85 ]\u001b[0m -18.50\n",
      "((...((....))(((............)))(((((((.....)))).)))..)).........((((((((.\u001b[93m\u001b[1m(\u001b[0m((.......))\u001b[93m\u001b[1m)\u001b[0m....)))))))).. \u001b[93m[  74,   86 ]\u001b[0m -19.20\n",
      "((...((....))((\u001b[93m\u001b[1m.\u001b[0m............\u001b[93m\u001b[1m.\u001b[0m))(((((((.....)))).)))..)).........((((((((.(((.......)))....)))))))).. \u001b[93m[ -16,  -29 ]\u001b[0m -16.50\n",
      "((...(\u001b[93m\u001b[1m.\u001b[0m....\u001b[93m\u001b[1m.\u001b[0m)((..............))(((((((.....)))).)))..)).........((((((((.(((.......)))....)))))))).. \u001b[93m[  -7,  -12 ]\u001b[0m -13.40\n",
      "((...\u001b[93m\u001b[1m.\u001b[0m......\u001b[93m\u001b[1m.\u001b[0m((..............))(((((((.....)))).)))..)).........((((((((.(((.......)))....)))))))).. \u001b[93m[  -6,  -13 ]\u001b[0m -14.90\n",
      "(\u001b[93m\u001b[1m.\u001b[0m...........((..............))(((((((.....)))).)))..\u001b[93m\u001b[1m.\u001b[0m).........((((((((.(((.......)))....)))))))).. \u001b[93m[  -2,  -54 ]\u001b[0m -11.60\n",
      "\u001b[93m\u001b[1m.\u001b[0m............((..............))(((((((.....)))).)))...\u001b[93m\u001b[1m.\u001b[0m.........((((((((.(((.......)))....)))))))).. \u001b[93m[  -1,  -55 ]\u001b[0m -15.90\n",
      ".............(\u001b[93m\u001b[1m.\u001b[0m..............\u001b[93m\u001b[1m.\u001b[0m)(((((((.....)))).))).............((((((((.(((.......)))....)))))))).. \u001b[93m[ -15,  -30 ]\u001b[0m -12.50\n",
      ".............\u001b[93m\u001b[1m.\u001b[0m................\u001b[93m\u001b[1m.\u001b[0m(((((((.....)))).))).............((((((((.(((.......)))....)))))))).. \u001b[93m[ -14,  -31 ]\u001b[0m -16.50\n",
      "...............................((\u001b[93m\u001b[1m.\u001b[0m((((.....)))).\u001b[93m\u001b[1m.\u001b[0m)).............((((((((.(((.......)))....)))))))).. \u001b[93m[ -34,  -49 ]\u001b[0m -13.00\n",
      "...............................(\u001b[93m\u001b[1m.\u001b[0m.((((.....))))..\u001b[93m\u001b[1m.\u001b[0m).............((((((((.(((.......)))....)))))))).. \u001b[93m[ -33,  -50 ]\u001b[0m -10.90\n",
      "...............................\u001b[93m\u001b[1m.\u001b[0m..((((.....))))...\u001b[93m\u001b[1m.\u001b[0m.............((((((((.(((.......)))....)))))))).. \u001b[93m[ -32,  -51 ]\u001b[0m -13.00\n",
      ".......................\u001b[93m\u001b[1m(\u001b[0m.......\u001b[93m\u001b[1m)\u001b[0m..((((.....)))).................((((((((.(((.......)))....)))))))).. \u001b[93m[  24,   32 ]\u001b[0m  -9.60\n",
      ".......................(\u001b[93m\u001b[1m(\u001b[0m.....\u001b[93m\u001b[1m)\u001b[0m)..((((.....)))).................((((((((.(((.......)))....)))))))).. \u001b[93m[  25,   31 ]\u001b[0m -11.50\n",
      "......................\u001b[93m\u001b[1m(\u001b[0m((.....))\u001b[93m\u001b[1m)\u001b[0m.((((.....)))).................((((((((.(((.......)))....)))))))).. \u001b[93m[  23,   33 ]\u001b[0m -12.80\n",
      ".................\u001b[93m\u001b[1m(\u001b[0m....(((.....))).((((.....)))).\u001b[93m\u001b[1m)\u001b[0m...............((((((((.(((.......)))....)))))))).. \u001b[93m[  18,   49 ]\u001b[0m \u001b[91m\u001b[1m -9.20\u001b[0m\n",
      "................\u001b[93m\u001b[1m(\u001b[0m(....(((.....))).((((.....)))).)\u001b[93m\u001b[1m)\u001b[0m..............((((((((.(((.......)))....)))))))).. \u001b[93m[  17,   50 ]\u001b[0m -12.10\n",
      "...............\u001b[93m\u001b[1m(\u001b[0m((....(((.....))).((((.....)))).))\u001b[93m\u001b[1m)\u001b[0m.............((((((((.(((.......)))....)))))))).. \u001b[93m[  16,   51 ]\u001b[0m -15.80\n",
      "...............(((\u001b[93m\u001b[1m(\u001b[0m...(((.....))).((((.....))))\u001b[93m\u001b[1m)\u001b[0m))).............((((((((.(((.......)))....)))))))).. \u001b[93m[  19,   48 ]\u001b[0m -16.50\n",
      "......\u001b[93m\u001b[1m(\u001b[0m........((((...(((.....))).((((.....))))))))...\u001b[93m\u001b[1m)\u001b[0m.........((((((((.(((.......)))....)))))))).. \u001b[93m[   7,   55 ]\u001b[0m -12.10\n",
      ".....\u001b[93m\u001b[1m(\u001b[0m(........((((...(((.....))).((((.....))))))))...)\u001b[93m\u001b[1m)\u001b[0m........((((((((.(((.......)))....)))))))).. \u001b[93m[   6,   56 ]\u001b[0m -13.50\n",
      "....\u001b[93m\u001b[1m(\u001b[0m((........((((...(((.....))).((((.....))))))))...))\u001b[93m\u001b[1m)\u001b[0m.......((((((((.(((.......)))....)))))))).. \u001b[93m[   5,   57 ]\u001b[0m -14.70\n",
      "...\u001b[93m\u001b[1m(\u001b[0m(((........((((...(((.....))).((((.....))))))))...)))\u001b[93m\u001b[1m)\u001b[0m......((((((((.(((.......)))....)))))))).. \u001b[93m[   4,   58 ]\u001b[0m -15.30\n",
      "...((((.\u001b[93m\u001b[1m(\u001b[0m......((((...(((.....))).((((.....))))))))..\u001b[93m\u001b[1m)\u001b[0m))))......((((((((.(((.......)))....)))))))).. \u001b[93m[   9,   54 ]\u001b[0m -13.40\n",
      "...((((.(\u001b[93m\u001b[1m(\u001b[0m.....((((...(((.....))).((((.....)))))))).\u001b[93m\u001b[1m)\u001b[0m)))))......((((((((.(((.......)))....)))))))).. \u001b[93m[  10,   53 ]\u001b[0m -13.50\n",
      "...((((.((\u001b[93m\u001b[1m(\u001b[0m....((((...(((.....))).((((.....))))))))\u001b[93m\u001b[1m)\u001b[0m))))))......((((((((.(((.......)))....)))))))).. \u001b[93m[  11,   52 ]\u001b[0m -15.40\n",
      "S:  -9.20 kcal/mol | B:  10.10 kcal/mol | E[start]:-19.30 E[end]:-15.40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-9.2"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_samples = f'./dataset_100.csv'\n",
    "samples_df = pd.read_csv(filename_samples)\n",
    "\n",
    "sequence = ''\n",
    "s1 = ''\n",
    "s2 = ''\n",
    "\n",
    "for index, row in samples_df.iterrows():\n",
    "    if index != 5:\n",
    "        continue\n",
    "    sequence = row.sequence\n",
    "    s1 = row.s1\n",
    "    s2 = row.s2\n",
    "\n",
    "print (sequence)\n",
    "print (s1)\n",
    "print (s2)\n",
    "\n",
    "fc = RNA.fold_compound(sequence)\n",
    "en1 = fc.eval_structure(s1)\n",
    "en2 = fc.eval_structure(s2)\n",
    "\n",
    "def fp_call_sw(sequence, s1, s2, search_width):    \n",
    "    fp = findpath.findpath_single(sequence, s1, s2, search_width=search_width, mp=True)\n",
    "    result = fp.get_en()/100.0\n",
    "    path = fp.get_path()\n",
    "    # return result, path\n",
    "    return result, path\n",
    "\n",
    "\n",
    "sw = 1\n",
    "# sw = 100\n",
    "\n",
    "r, p = fp_call_sw(sequence, s1, s2, sw)\n",
    "p = [(i[0], i[1]) for i in p]\n",
    "\n",
    "print_moves(sequence, s1, s2, p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1, -1.9e+01,      0,    -77,    -84,    0.0,  69.76,     10\n",
      "   1, -1.9e+01,      1,     -3,    -53,   0.59,  41.19,    420\n",
      "   1, -1.9e+01,      2,    -33,    -50,   0.63,  39.67,    450\n",
      "   1, -1.9e+01,      3,     -1,    -55,   0.49,  32.66,    350\n",
      "   2, -1.7e+01,      0,     76,     84,   0.24,  42.64,    230\n",
      "   2, -1.7e+01,      1,    -34,    -49,   0.43,   16.4,    350\n",
      "   2, -1.7e+01,      2,    -33,    -50,   0.59,  15.25,    450\n",
      "   2, -1.7e+01,      3,     75,     85,   0.24,  13.58,    230\n",
      "   3, -1.7e+01,      0,     75,     85,    0.0,   35.1,   -370\n",
      "   3, -1.7e+01,      1,    -34,    -49,   0.67,  10.99,    350\n",
      "   3, -1.7e+01,      2,    -33,    -50,   0.76,  8.267,    450\n",
      "   3, -1.7e+01,      3,    -32,    -51,    0.7,  6.874,    390\n",
      "   4, -1.7e+01,      0,     74,     86,    0.0,  38.06,    -70\n",
      "   4, -1.7e+01,      1,    -34,    -49,   0.54,   16.6,    350\n",
      "   4, -1.7e+01,      2,    -33,    -50,   0.67,  12.84,    450\n",
      "   4, -1.7e+01,      3,    -32,    -51,   0.59,  10.57,    390\n",
      "   5, -1.7e+01,      0,    -34,    -49,   0.43,  19.77,    350\n",
      "   5, -1.7e+01,      1,    -33,    -50,   0.59,  19.06,    450\n",
      "   5, -1.7e+01,      2,    -32,    -51,   0.49,   13.0,    390\n",
      "   5, -1.7e+01,      3,    -15,    -30,    1.0,  10.05,    710\n",
      "   6, -1.6e+01,      0,    -33,    -50,   0.21,  20.18,    210\n",
      "   6, -1.6e+01,      1,    -16,    -29,    0.3,  10.72,    270\n",
      "   6, -1.6e+01,      2,    -32,    -51,   0.49,  7.277,    390\n",
      "   6, -1.6e+01,      3,     -7,    -12,   0.37,  5.522,    310\n",
      "   7, -1.6e+01,      0,    -32,    -51,    0.0,  22.79,   -210\n",
      "   7, -1.6e+01,      1,     -4,    -52,   0.32,  1.541,     80\n",
      "   7, -1.6e+01,      2,    -16,    -29,   0.52, -0.431,    270\n",
      "   7, -1.6e+01,      3,     -7,    -12,   0.57, -4.604,    310\n",
      "   8, -1.6e+01,      0,     -4,    -52,    0.0,   6.42,     80\n",
      "   8, -1.6e+01,      1,    -16,    -29,    0.3,  2.965,    270\n",
      "   8, -1.6e+01,      2,     -7,    -12,   0.37, 0.3743,    310\n",
      "   8, -1.6e+01,      3,     -6,    -13,   0.48, -3.321,    380\n",
      "   9, -1.6e+01,      0,     -3,    -53,    0.0,  24.43,    130\n",
      "   9, -1.6e+01,      1,     -7,    -12,   0.31,  18.56,    310\n",
      "   9, -1.6e+01,      2,     -6,    -13,   0.43,  17.24,    380\n",
      "   9, -1.6e+01,      3,    -16,    -29,   0.24,  8.916,    270\n",
      "  10, -1.2e+01,      0,     -6,    -13,   0.25,  25.28,    380\n",
      "  10, -1.2e+01,      1,     -7,    -12,  0.091,  24.79,    310\n",
      "  10, -1.2e+01,      2,     -2,    -54,   0.14,  23.94,    330\n",
      "  10, -1.2e+01,      3,     -1,    -55,   0.18,  19.02,    350\n",
      "  11, -8.6,      0,     -2,    -54,   0.59,  21.41,    330\n",
      "  11, -8.6,      1,     -1,    -55,   0.61,  13.86,    350\n",
      "  11, -8.6,      2,    -14,    -31,   0.65,   9.99,    380\n",
      "  11, -8.6,      3,     -7,    -12,    0.0,  9.937,   -220\n",
      "  12, -8.6,      0,     -1,    -55,    0.0,  30.03,   -340\n",
      "  12, -8.6,      1,     -7,    -12,   0.11,  15.58,   -220\n",
      "  12, -8.6,      2,    -14,    -31,   0.69,   2.74,    380\n",
      "  12, -8.6,      3,    -16,    -29,   0.58, -2.768,    270\n",
      "  13, -8.6,      0,     -7,    -12,    0.0,  10.62,   -310\n",
      "  13, -8.6,      1,      6,     56,   0.78,  6.799,    490\n",
      "  13, -8.6,      2,      4,     58,   0.82,  4.042,    530\n",
      "  13, -8.6,      3,    -16,    -29,   0.57, 0.03147,    270\n",
      "  14, -8.6,      0,      7,     55,   0.36,   22.7,    430\n",
      "  14, -8.6,      1,      5,     57,    0.7,  4.971,    580\n",
      "  14, -8.6,      2,    -14,    -31,   0.25,  2.667,    380\n",
      "  14, -8.6,      3,      6,     56,    0.7,  2.253,    580\n",
      "  15, -8.6,      0,      6,     56,    0.0,  39.22,   -140\n",
      "  15, -8.6,      1,      5,     57,   0.36,  23.04,    170\n",
      "  15, -8.6,      2,    -14,    -31,   0.61,   20.3,    380\n",
      "  15, -8.6,      3,      9,     54,   0.45,  19.04,    240\n",
      "  16, -8.6,      0,      5,     57,    0.0,  44.01,   -120\n",
      "  16, -8.6,      1,      9,     54,   0.43,   12.9,    240\n",
      "  16, -8.6,      2,      4,     58,   0.39,   10.7,    200\n",
      "  16, -8.6,      3,    -14,    -31,    0.6,  7.569,    380\n",
      "  17, -8.6,      0,      4,     58,    0.0,  37.46,    -60\n",
      "  17, -8.6,      1,      9,     54,   0.39,   8.88,    240\n",
      "  17, -8.6,      2,    -14,    -31,   0.57, -2.508,    380\n",
      "  17, -8.6,      3,     10,     53,   0.61, -3.185,    410\n",
      "  18, -8.6,      0,      9,     54,    0.0,  26.09,    240\n",
      "  18, -8.6,      1,     10,     53,   0.36,  4.711,    410\n",
      "  18, -8.6,      2,     11,     52,   0.45, -0.8227,    450\n",
      "  18, -8.6,      3,    -14,    -31,    0.3, -10.68,    380\n",
      "  19, -8.6,      0,     10,     53,    0.0,  32.32,    -60\n",
      "  19, -8.6,      1,     11,     52,   0.34,  4.464,    200\n",
      "  19, -8.6,      2,    -14,    -31,   0.57,  3.679,    380\n",
      "  19, -8.6,      3,    -16,    -29,   0.43, -10.21,    270\n",
      "  20, -8.6,      0,     11,     52,    0.0,  32.39,    -80\n",
      "  20, -8.6,      1,    -14,    -31,   0.58, -0.03756,    380\n",
      "  20, -8.6,      2,    -16,    -29,   0.44, -10.36,    270\n",
      "  20, -8.6,      3,    -15,    -30,    1.0, -30.82,    710\n",
      "  21, -8.6,      0,    -14,    -31,   0.25,  9.497,    380\n",
      "  21, -8.6,      1,    -16,    -29,    0.0,  2.366,    270\n",
      "  21, -8.6,      2,    -15,    -30,    1.0, -28.59,    710\n",
      "  22, -6.5,      0,    -16,    -29,    0.0,  17.06,    270\n",
      "  22, -6.5,      1,    -15,    -30,    1.0,  8.098,    330\n",
      "  23, -6.5,      0,    -15,    -30,    0.0,   29.1,   -260\n",
      "  24, -6.5,      0,     25,     31,  0.032,  27.11,    170\n",
      "  24, -6.5,      1,     24,     32,    0.0,  26.22,    160\n",
      "  24, -6.5,      2,     16,     51,   0.48,  11.74,    310\n",
      "  24, -6.5,      3,     17,     50,   0.77, -5.478,    400\n",
      "  25, -6.5,      0,     24,     32,    0.0,  38.38,   -200\n",
      "  25, -6.5,      1,     16,     51,   0.61, -1.879,    310\n",
      "  25, -6.5,      2,     18,     49,   0.72, -7.048,    400\n",
      "  25, -6.5,      3,     17,     50,   0.75, -7.484,    420\n",
      "  26, -6.5,      0,     23,     33,    0.0,  31.95,   -130\n",
      "  26, -6.5,      1,     16,     51,   0.58,  6.966,    310\n",
      "  26, -6.5,      2,     18,     49,    0.7,  -2.64,    400\n",
      "  26, -6.5,      3,     17,     50,   0.72, -3.135,    420\n",
      "  27, -6.5,      0,     16,     51,    0.0,  44.08,    310\n",
      "  27, -6.5,      1,     18,     49,   0.28,  35.22,    400\n",
      "  27, -6.5,      2,     17,     50,   0.34,  29.78,    420\n",
      "  27, -6.5,      3,     19,     48,    1.0, -7.007,    630\n",
      "  28, -6.5,      0,     17,     50,    0.0,  66.59,   -330\n",
      "  28, -6.5,      1,     18,     49,   0.55,  39.98,     10\n",
      "  28, -6.5,      2,     19,     48,    1.0,  23.08,    290\n",
      "  29, -6.5,      0,     18,     49,    0.0,  62.16,   -380\n",
      "  29, -6.5,      1,     19,     48,    1.0,  12.34,    210\n",
      "  30, -6.5,      0,     19,     48,    0.0,  64.89,    -70\n"
     ]
    }
   ],
   "source": [
    "s = s1\n",
    "fc = RNA.fold_compound(sequence)\n",
    "pt = list(RNA.ptable(s))\n",
    "pt2 = list(RNA.ptable(s2))\n",
    "\n",
    "lasts = s\n",
    "lasti = 0\n",
    "lastj = 0\n",
    "last_move = (0,0)\n",
    "\n",
    "en_max = fc.eval_structure(s)\n",
    "\n",
    "\n",
    "result, path = fp_call(sequence, s1, s2, 1)\n",
    "all_moves = [(abs(i[0]), abs(i[1])) for i in path]\n",
    "\n",
    "prev_move_dist = np.zeros((20)) \n",
    "\n",
    "found_moves = [(0, 0)]\n",
    "\n",
    "dist = 0\n",
    "\n",
    "while True:\n",
    "\n",
    "    dist += 1\n",
    "    # check where we can go, compare with our best move. \n",
    "    pt = list(RNA.ptable(s))\n",
    "\n",
    "    # check available moves, save them, sort them    \n",
    "    avail_moves = []\n",
    "    ij_moves = []\n",
    "    found_pos = None\n",
    "\n",
    "    # collect moves from the generator function, then sort them.\n",
    "    for pos, (i,j) in enumerate(find_moves(pt, pt2)):    \n",
    "        next_en = fc.eval_move_pt(pt, i, j)\n",
    "        # mark where we found our move\n",
    "        found = None\n",
    "        avail_moves.append((i, j, next_en, found))\n",
    "        ij_moves.append((abs(i),abs(j)))\n",
    "\n",
    "\n",
    "    avail_moves.sort(key=lambda x: x[2])\n",
    "    found_list = [x[3] for x in avail_moves]\n",
    "    en_list = np.array([[x[2] for x in avail_moves]])\n",
    "    en_list_scaled = min_max_scaler.fit_transform(en_list.T).T[0]\n",
    "\n",
    "    i_shift_moves, j_shift_moves = return_shift_moves(all_moves)\n",
    "\n",
    "    # print (s, pt, avail_moves)\n",
    "    # print (en_list_scaled)\n",
    "\n",
    "    candidates = []\n",
    "\n",
    "    for pos, (i,j, en, found) in enumerate(avail_moves):\n",
    "\n",
    "        this_move = (abs(i), abs(j))\n",
    "        last_move = (lasti, lastj)\n",
    "\n",
    "        if lasti:\n",
    "            # print (this_move, last_move, ij_moves)\n",
    "            ijd, thisclose, lastclose = ij_distance(last_move, this_move, ij_moves)\n",
    "        else:\n",
    "            ijd, thisclose, lastclose = 0, 0, 0\n",
    "\n",
    "        cd = config_distance(pt, this_move)\n",
    "\n",
    "        en_scaled = en_list_scaled[pos]\n",
    "\n",
    "\n",
    "        if abs(i) in i_shift_moves:\n",
    "            i_shift = 1\n",
    "        else:\n",
    "            i_shift = 0\n",
    "        if abs(j) in j_shift_moves:\n",
    "            j_shift = 1\n",
    "        else:\n",
    "            j_shift = 0\n",
    "\n",
    "        if i<0:\n",
    "            insert_or_delete = 0\n",
    "        else:\n",
    "            insert_or_delete = 1\n",
    "\n",
    "        if i > 0:\n",
    "            snew = s[:i-1] + \"(\" + s[i:j-1] + \")\" + s[j:]\n",
    "        if i < 0:\n",
    "            snew = s[:-i-1] + \".\" + s[-i:-j-1] + \".\" + s[-j:]\n",
    "        # ptnew = list(RNA.ptable(snew))\n",
    "        balance = balance_in_all_things(s1, s2, snew) / len(all_moves)\n",
    "\n",
    "\n",
    "        # run the prediction\n",
    "        sample = 1, ijd, thisclose, lastclose, cd, en_list_scaled[pos], 1\n",
    "        \n",
    "        sample = { \"target\": 0,\n",
    "           \"ijd\":       ijd,\n",
    "           \"thisclose\": thisclose,\n",
    "           \"lastclose\": lastclose,\n",
    "           \"cd\":        cd,\n",
    "           \"en_pos\":    en_list_scaled[pos],\n",
    "           \"i_shift\":   i_shift, \n",
    "           \"j_shift\":   j_shift,\n",
    "           \"insert_or_delete\": insert_or_delete,\n",
    "           \"balance\":   balance,\n",
    "            \"0\":  prev_move_dist[0],\n",
    "            \"1\":  prev_move_dist[1],\n",
    "            \"2\":  prev_move_dist[2],\n",
    "            \"3\":  prev_move_dist[3],\n",
    "            \"4\":  prev_move_dist[4],\n",
    "            \"5\":  prev_move_dist[5],\n",
    "            \"6\":  prev_move_dist[6],\n",
    "            \"7\":  prev_move_dist[7],\n",
    "            \"8\":  prev_move_dist[8],\n",
    "            \"9\":  prev_move_dist[9],\n",
    "            \"10\":  prev_move_dist[10],\n",
    "            \"11\":  prev_move_dist[11],\n",
    "            \"12\":  prev_move_dist[12],\n",
    "            \"13\":  prev_move_dist[13],\n",
    "            \"14\":  prev_move_dist[14],\n",
    "            \"15\":  prev_move_dist[15],\n",
    "            \"16\":  prev_move_dist[16],\n",
    "            \"17\":  prev_move_dist[17],\n",
    "            \"18\":  prev_move_dist[18],\n",
    "            \"19\":  prev_move_dist[19],\n",
    "        }       \n",
    "        \n",
    "        sample_test_ds = sample_to_dataset(sample, test)\n",
    "        result_predictor = model.predict(sample_test_ds)[0][0]\n",
    "\n",
    "        candidates.append((i, j, en_scaled, result_predictor, en))\n",
    "        \n",
    "        # print (ijd, thisclose, lastclose, cd, en_scaled, result_predictor)\n",
    "\n",
    "\n",
    "    # print (prev_move_dist)\n",
    "\n",
    "    candidates.sort(key=lambda x: -x[3])\n",
    "    # candidates.sort(key=lambda x: x[2])\n",
    "\n",
    "\n",
    "\n",
    "    # print (candidates)\n",
    "\n",
    "    # chosen move\n",
    "    i = candidates[0][0]\n",
    "    j = candidates[0][1]\n",
    "\n",
    "    lasts = s\n",
    "    lasti = abs(i)\n",
    "    lastj = abs(j)\n",
    "\n",
    "    prev_move_dist = new_move_dist(lasti, lastj, prev_move_dist)\n",
    "\n",
    "    if i > 0:\n",
    "        s = s[:i-1] + \"(\" + s[i:j-1] + \")\" + s[j:]\n",
    "    if i < 0:\n",
    "        s = s[:-i-1] + \".\" + s[-i:-j-1] + \".\" + s[-j:]\n",
    "\n",
    "    en_new = fc.eval_structure(s)\n",
    "    if en_new > en_max:\n",
    "        en_max = en_new\n",
    "\n",
    "    # print (s, i, j, en_new, en_max)\n",
    "\n",
    "    found_moves.append((i, j))\n",
    "\n",
    "    for i0, (i1, i2, i3, i4, i5) in enumerate(candidates):\n",
    "        print (f' {dist:3}, {en_max:2.2}, {i0:6}, {i1:6}, {i2:6}, {i3:6.2}, {i4:6.4}, {i5:6}')\n",
    "        if i0 == 3:\n",
    "            break\n",
    "\n",
    "    # break\n",
    "\n",
    "    if s==s2:\n",
    "        break\n",
    "\n",
    "\n",
    "# WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'ijd': <tf.Tensor 'ExpandDims_24:0' shape=(None, 1) dtype=float64>, 'thisclose': <tf.Tensor 'ExpandDims_28:0' shape=(None, 1) dtype=float64>, 'lastclose': <tf.Tensor 'ExpandDims_27:0' shape=(None, 1) dtype=float64>, 'cd': <tf.Tensor 'ExpandDims_21:0' shape=(None, 1) dtype=int64>, 'en_pos': <tf.Tensor 'ExpandDims_22:0' shape=(None, 1) dtype=float64>, 'i_shift': <tf.Tensor 'ExpandDims_23:0' shape=(None, 1) dtype=int64>, 'j_shift': <tf.Tensor 'ExpandDims_26:0' shape=(None, 1) dtype=int64>, 'insert_or_delete': <tf.Tensor 'ExpandDims_25:0' shape=(None, 1) dtype=int64>, 'balance': <tf.Tensor 'ExpandDims_20:0' shape=(None, 1) dtype=float64>, '0': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float64>, '1': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=float64>, '2': <tf.Tensor 'ExpandDims_12:0' shape=(None, 1) dtype=float64>, '3': <tf.Tensor 'ExpandDims_13:0' shape=(None, 1) dtype=float64>, '4': <tf.Tensor 'ExpandDims_14:0' shape=(None, 1) dtype=float64>, '5': <tf.Tensor 'ExpandDims_15:0' shape=(None, 1) dtype=float64>, '6': <tf.Tensor 'ExpandDims_16:0' shape=(None, 1) dtype=float64>, '7': <tf.Tensor 'ExpandDims_17:0' shape=(None, 1) dtype=float64>, '8': <tf.Tensor 'ExpandDims_18:0' shape=(None, 1) dtype=float64>, '9': <tf.Tensor 'ExpandDims_19:0' shape=(None, 1) dtype=float64>, '10': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=float64>, '11': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=float64>, '12': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=float64>, '13': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=float64>, '14': <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=float64>, '15': <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=float64>, '16': <tf.Tensor 'ExpandDims_8:0' shape=(None, 1) dtype=float64>, '17': <tf.Tensor 'ExpandDims_9:0' shape=(None, 1) dtype=float64>, '18': <tf.Tensor 'ExpandDims_10:0' shape=(None, 1) dtype=float64>, '19': <tf.Tensor 'ExpandDims_11:0' shape=(None, 1) dtype=float64>}\n",
    "# Consider rewriting this model with the Functional API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCUUUGGAUUUCCGGGCCAUUAACGCACCCCGUCUAGUAUCCCACUAUGGCAAGCUAACUUAAAGGUCUAGAAAGCCGAACCAGCUCCCGUCUAGACCGU\n",
      "((((.((....))(((............)))(((((((.....)))).))))))).........((((((((....(......)......)))))))).. \u001b[93m[   0,    0 ]\u001b[0m -19.30\n",
      "((((.((....))(((............)))(((((((.....)))).))))))).........((((((((....\u001b[93m\u001b[1m.\u001b[0m......\u001b[93m\u001b[1m.\u001b[0m......)))))))).. \u001b[93m[ -77,  -84 ]\u001b[0m -19.20\n",
      "((((.((....))(((............)))(((((((.....)))).))))))).........((((((((...\u001b[93m\u001b[1m(\u001b[0m.......\u001b[93m\u001b[1m)\u001b[0m......)))))))).. \u001b[93m[  76,   84 ]\u001b[0m -16.90\n",
      "((((.((....))(((............)))(((((((.....)))).))))))).........((((((((..\u001b[93m\u001b[1m(\u001b[0m(.......)\u001b[93m\u001b[1m)\u001b[0m.....)))))))).. \u001b[93m[  75,   85 ]\u001b[0m -20.60\n",
      "((((.((....))(((............)))(((((((.....)))).))))))).........((((((((.\u001b[93m\u001b[1m(\u001b[0m((.......))\u001b[93m\u001b[1m)\u001b[0m....)))))))).. \u001b[93m[  74,   86 ]\u001b[0m -21.30\n",
      "((((.((....))(((............)))((\u001b[93m\u001b[1m.\u001b[0m((((.....)))).\u001b[93m\u001b[1m.\u001b[0m)))))).........((((((((.(((.......)))....)))))))).. \u001b[93m[ -34,  -49 ]\u001b[0m -17.80\n",
      "((((.((....))(((............)))(\u001b[93m\u001b[1m.\u001b[0m.((((.....))))..\u001b[93m\u001b[1m.\u001b[0m))))).........((((((((.(((.......)))....)))))))).. \u001b[93m[ -33,  -50 ]\u001b[0m -15.70\n",
      "((((.((....))(((............)))\u001b[93m\u001b[1m.\u001b[0m..((((.....))))...\u001b[93m\u001b[1m.\u001b[0m)))).........((((((((.(((.......)))....)))))))).. \u001b[93m[ -32,  -51 ]\u001b[0m -17.80\n",
      "(((\u001b[93m\u001b[1m.\u001b[0m.((....))(((............)))...((((.....))))....\u001b[93m\u001b[1m.\u001b[0m))).........((((((((.(((.......)))....)))))))).. \u001b[93m[  -4,  -52 ]\u001b[0m -17.00\n",
      "((\u001b[93m\u001b[1m.\u001b[0m..((....))(((............)))...((((.....)))).....\u001b[93m\u001b[1m.\u001b[0m)).........((((((((.(((.......)))....)))))))).. \u001b[93m[  -3,  -53 ]\u001b[0m -15.70\n",
      "((...\u001b[93m\u001b[1m.\u001b[0m(....)\u001b[93m\u001b[1m.\u001b[0m(((............)))...((((.....))))......)).........((((((((.(((.......)))....)))))))).. \u001b[93m[  -6,  -13 ]\u001b[0m -11.90\n",
      "(\u001b[93m\u001b[1m.\u001b[0m....(....).(((............)))...((((.....))))......\u001b[93m\u001b[1m.\u001b[0m).........((((((((.(((.......)))....)))))))).. \u001b[93m[  -2,  -54 ]\u001b[0m  -8.60\n",
      "\u001b[93m\u001b[1m.\u001b[0m.....(....).(((............)))...((((.....)))).......\u001b[93m\u001b[1m.\u001b[0m.........((((((((.(((.......)))....)))))))).. \u001b[93m[  -1,  -55 ]\u001b[0m -12.00\n",
      "......\u001b[93m\u001b[1m.\u001b[0m....\u001b[93m\u001b[1m.\u001b[0m.(((............)))...((((.....)))).................((((((((.(((.......)))....)))))))).. \u001b[93m[  -7,  -12 ]\u001b[0m -15.10\n",
      "......\u001b[93m\u001b[1m(\u001b[0m......(((............)))...((((.....)))).......\u001b[93m\u001b[1m)\u001b[0m.........((((((((.(((.......)))....)))))))).. \u001b[93m[   7,   55 ]\u001b[0m -10.80\n",
      ".....\u001b[93m\u001b[1m(\u001b[0m(......(((............)))...((((.....)))).......)\u001b[93m\u001b[1m)\u001b[0m........((((((((.(((.......)))....)))))))).. \u001b[93m[   6,   56 ]\u001b[0m -12.20\n",
      "....\u001b[93m\u001b[1m(\u001b[0m((......(((............)))...((((.....)))).......))\u001b[93m\u001b[1m)\u001b[0m.......((((((((.(((.......)))....)))))))).. \u001b[93m[   5,   57 ]\u001b[0m -13.40\n",
      "...\u001b[93m\u001b[1m(\u001b[0m(((......(((............)))...((((.....)))).......)))\u001b[93m\u001b[1m)\u001b[0m......((((((((.(((.......)))....)))))))).. \u001b[93m[   4,   58 ]\u001b[0m -14.00\n",
      "...((((.\u001b[93m\u001b[1m(\u001b[0m....(((............)))...((((.....))))......\u001b[93m\u001b[1m)\u001b[0m))))......((((((((.(((.......)))....)))))))).. \u001b[93m[   9,   54 ]\u001b[0m -11.60\n",
      "...((((.(\u001b[93m\u001b[1m(\u001b[0m...(((............)))...((((.....)))).....\u001b[93m\u001b[1m)\u001b[0m)))))......((((((((.(((.......)))....)))))))).. \u001b[93m[  10,   53 ]\u001b[0m -12.20\n",
      "...((((.((\u001b[93m\u001b[1m(\u001b[0m..(((............)))...((((.....))))....\u001b[93m\u001b[1m)\u001b[0m))))))......((((((((.(((.......)))....)))))))).. \u001b[93m[  11,   52 ]\u001b[0m -13.00\n",
      "...((((.(((..\u001b[93m\u001b[1m.\u001b[0m((............))\u001b[93m\u001b[1m.\u001b[0m...((((.....))))....)))))))......((((((((.(((.......)))....)))))))).. \u001b[93m[ -14,  -31 ]\u001b[0m  -9.20\n",
      "...((((.(((...(\u001b[93m\u001b[1m.\u001b[0m............\u001b[93m\u001b[1m.\u001b[0m)....((((.....))))....)))))))......((((((((.(((.......)))....)))))))).. \u001b[93m[ -16,  -29 ]\u001b[0m \u001b[91m\u001b[1m -6.50\u001b[0m\n",
      "...((((.(((...\u001b[93m\u001b[1m.\u001b[0m..............\u001b[93m\u001b[1m.\u001b[0m....((((.....))))....)))))))......((((((((.(((.......)))....)))))))).. \u001b[93m[ -15,  -30 ]\u001b[0m  -9.10\n",
      "...((((.(((.............\u001b[93m\u001b[1m(\u001b[0m.....\u001b[93m\u001b[1m)\u001b[0m...((((.....))))....)))))))......((((((((.(((.......)))....)))))))).. \u001b[93m[  25,   31 ]\u001b[0m  -7.40\n",
      "...((((.(((............\u001b[93m\u001b[1m(\u001b[0m(.....)\u001b[93m\u001b[1m)\u001b[0m..((((.....))))....)))))))......((((((((.(((.......)))....)))))))).. \u001b[93m[  24,   32 ]\u001b[0m  -9.40\n",
      "...((((.(((...........\u001b[93m\u001b[1m(\u001b[0m((.....))\u001b[93m\u001b[1m)\u001b[0m.((((.....))))....)))))))......((((((((.(((.......)))....)))))))).. \u001b[93m[  23,   33 ]\u001b[0m -10.70\n",
      "...((((.(((....\u001b[93m\u001b[1m(\u001b[0m......(((.....))).((((.....))))...\u001b[93m\u001b[1m)\u001b[0m)))))))......((((((((.(((.......)))....)))))))).. \u001b[93m[  16,   51 ]\u001b[0m  -7.60\n",
      "...((((.(((....(\u001b[93m\u001b[1m(\u001b[0m.....(((.....))).((((.....))))..\u001b[93m\u001b[1m)\u001b[0m))))))))......((((((((.(((.......)))....)))))))).. \u001b[93m[  17,   50 ]\u001b[0m -10.90\n",
      "...((((.(((....((\u001b[93m\u001b[1m(\u001b[0m....(((.....))).((((.....)))).\u001b[93m\u001b[1m)\u001b[0m)))))))))......((((((((.(((.......)))....)))))))).. \u001b[93m[  18,   49 ]\u001b[0m -14.70\n",
      "...((((.(((....(((\u001b[93m\u001b[1m(\u001b[0m...(((.....))).((((.....))))\u001b[93m\u001b[1m)\u001b[0m))))))))))......((((((((.(((.......)))....)))))))).. \u001b[93m[  19,   48 ]\u001b[0m -15.40\n",
      "S:  -6.50 kcal/mol | B:  12.80 kcal/mol | E[start]:-19.30 E[end]:-15.40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-6.5"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_moves(sequence, s1, s2, found_moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: optimal path, compare with greedy & ML choice\n",
    "\n",
    "# check positions: at which position did we find the best move in our move set? "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
