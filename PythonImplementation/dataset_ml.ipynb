{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import RNA\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import SVG, display\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "import difflib\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from helper import print_moves\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('../')\n",
    "from pretty_print_path import print_moves\n",
    "import findpath_librna\n",
    "import findpath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49492</th>\n",
       "      <td>49492</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49493</th>\n",
       "      <td>49493</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49494</th>\n",
       "      <td>49494</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49495</th>\n",
       "      <td>49495</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49496</th>\n",
       "      <td>49496</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49497 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0         4         5         6  7         8  target\n",
       "0               0  0.363636  0.285714  0.142857  1  0.000000       1\n",
       "1               1  0.181818  0.333333  0.142857  1  0.034483       1\n",
       "2               2  0.818182  0.238095  0.142857  1  0.034483       0\n",
       "3               3  0.954545  0.047619  0.142857  1  0.103448       0\n",
       "4               4  0.636364  0.333333  0.142857  1  0.206897       1\n",
       "...           ...       ...       ...       ... ..       ...     ...\n",
       "49492       49492  0.666667  1.000000  0.000000  0  1.000000       1\n",
       "49493       49493  0.200000  1.000000  1.000000  1  0.000000       1\n",
       "49494       49494  0.800000  1.000000  1.000000  0  0.760000       1\n",
       "49495       49495  0.400000  1.000000  1.000000  0  0.920000       1\n",
       "49496       49496  0.600000  1.000000  1.000000  0  1.000000       1\n",
       "\n",
       "[49497 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file = \"dataset_102_train.csv\"\n",
    "\n",
    "dataframe = pd.read_csv(input_file)\n",
    "\n",
    "dataframe['target'] = np.where(dataframe[\"3\"]==1, 1, 0)\n",
    "\n",
    "dataframe = dataframe.drop(labels=\"0\", axis=1)\n",
    "dataframe = dataframe.drop(labels=\"1\", axis=1)\n",
    "dataframe = dataframe.drop(labels=\"2\", axis=1)\n",
    "dataframe = dataframe.drop(labels=\"9\", axis=1)\n",
    "dataframe = dataframe.drop(labels=\"3\", axis=1)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31677 train examples\n",
      "7920 validation examples\n",
      "9900 test examples\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(dataframe, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.2)\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every feature: ['Unnamed: 0', '4', '5', '6', '7', '8']\n",
      "A batch of ages: tf.Tensor([0.80769231 0.08333333 0.2173913  0.65217391 0.375     ], shape=(5,), dtype=float64)\n",
      "A batch of targets: tf.Tensor([0 1 1 0 1], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop('target')\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds\n",
    "\n",
    "batch_size = 5 # A small batch sized is used for demonstration purposes\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "## Understand the input pipeline\n",
    "# Now that we have created the input pipeline, let's call it to see the format of the data it returns. We have used a small batch size to keep the output readable.\n",
    "\n",
    "for feature_batch, label_batch in train_ds.take(1):\n",
    "  print('Every feature:', list(feature_batch.keys()))\n",
    "  print('A batch of ages:', feature_batch['4'])\n",
    "  print('A batch of targets:', label_batch )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09090909]\n",
      " [0.3       ]\n",
      " [0.75      ]\n",
      " [0.21052632]\n",
      " [0.11111111]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# We will use this batch to demonstrate several types of feature columns\n",
    "example_batch = next(iter(train_ds))[0]\n",
    "\n",
    "# A utility method to create a feature column\n",
    "# and to transform a batch of data\n",
    "def demo(feature_column):\n",
    "  feature_layer = layers.DenseFeatures(feature_column)\n",
    "  print(feature_layer(example_batch).numpy())\n",
    "\n",
    "# numeric columns\n",
    "photo_count = feature_column.numeric_column('4')\n",
    "demo(photo_count)\n",
    "\n",
    "# column 7 is a catigorical column\n",
    "animal_type = feature_column.categorical_column_with_vocabulary_list(\n",
    "      '7', [0, 1])\n",
    "animal_type_one_hot = feature_column.indicator_column(animal_type)\n",
    "demo(animal_type_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose which columns to use\n",
    "We have seen how to use several types of feature columns. Now we will use them to train a model. The goal of this tutorial is to show you the complete code (e.g. mechanics) needed to work with feature columns. We have selected a few columns to train our model below arbitrarily.\n",
    "\n",
    "Key point: If your aim is to build an accurate model, try a larger dataset of your own, and think carefully about which features are the most meaningful to include, and how they should be represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "\n",
    "# numeric cols\n",
    "for header in ['4', '5', '6', '8']:\n",
    "  feature_columns.append(feature_column.numeric_column(header))\n",
    "\n",
    "# categoriacal indicator_columns\n",
    "indicator_column_names = ['7']\n",
    "for col_name in indicator_column_names:\n",
    "  categorical_column = feature_column.categorical_column_with_vocabulary_list(\n",
    "      col_name, dataframe[col_name].unique())\n",
    "  indicator_column = feature_column.indicator_column(categorical_column)\n",
    "  feature_columns.append(indicator_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use lowest energy as indicator\n",
    "\n",
    "feature_columns = []\n",
    "\n",
    "# numeric cols\n",
    "for header in ['8']:\n",
    "  feature_columns.append(feature_column.numeric_column(header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NumericColumn(key='4', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='5', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='6', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='8', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='7', vocabulary_list=(1, 0), dtype=tf.int64, default_value=-1, num_oov_buckets=0))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a feature layer\n",
    "Now that we have defined our feature columns, we will use a [DenseFeatures](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/DenseFeatures) layer to input them to our Keras model.\n",
    "\n",
    "Earlier, we used a small batch size to demonstrate how feature columns worked. We create a new input pipeline with a larger batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "\n",
    "batch_size = 32\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'Unnamed: 0': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=int64>, '4': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float64>, '5': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=float64>, '6': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=float64>, '7': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=int64>, '8': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=float64>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'Unnamed: 0': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=int64>, '4': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float64>, '5': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=float64>, '6': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=float64>, '7': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=int64>, '8': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=float64>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "971/990 [============================>.] - ETA: 0s - loss: 0.5485 - accuracy: 0.7248WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'Unnamed: 0': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=int64>, '4': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float64>, '5': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=float64>, '6': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=float64>, '7': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=int64>, '8': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=float64>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5487 - accuracy: 0.7243 - val_loss: 0.5388 - val_accuracy: 0.7439\n",
      "Epoch 2/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5360 - accuracy: 0.7317 - val_loss: 0.5263 - val_accuracy: 0.7412\n",
      "Epoch 3/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5331 - accuracy: 0.7322 - val_loss: 0.5277 - val_accuracy: 0.7420\n",
      "Epoch 4/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5313 - accuracy: 0.7342 - val_loss: 0.5224 - val_accuracy: 0.7398\n",
      "Epoch 5/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5303 - accuracy: 0.7326 - val_loss: 0.5245 - val_accuracy: 0.7396\n",
      "Epoch 6/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5293 - accuracy: 0.7329 - val_loss: 0.5198 - val_accuracy: 0.7362\n",
      "Epoch 7/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5288 - accuracy: 0.7329 - val_loss: 0.5212 - val_accuracy: 0.7390\n",
      "Epoch 8/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5273 - accuracy: 0.7328 - val_loss: 0.5199 - val_accuracy: 0.7394\n",
      "Epoch 9/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5267 - accuracy: 0.7333 - val_loss: 0.5254 - val_accuracy: 0.7428\n",
      "Epoch 10/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5267 - accuracy: 0.7318 - val_loss: 0.5213 - val_accuracy: 0.7347\n",
      "Epoch 11/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.7331 - val_loss: 0.5246 - val_accuracy: 0.7360\n",
      "Epoch 12/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5255 - accuracy: 0.7331 - val_loss: 0.5211 - val_accuracy: 0.7423\n",
      "Epoch 13/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5251 - accuracy: 0.7325 - val_loss: 0.5193 - val_accuracy: 0.7413\n",
      "Epoch 14/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5248 - accuracy: 0.7338 - val_loss: 0.5216 - val_accuracy: 0.7419\n",
      "Epoch 15/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5253 - accuracy: 0.7333 - val_loss: 0.5194 - val_accuracy: 0.7393\n",
      "Epoch 16/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5244 - accuracy: 0.7331 - val_loss: 0.5200 - val_accuracy: 0.7442\n",
      "Epoch 17/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5239 - accuracy: 0.7349 - val_loss: 0.5214 - val_accuracy: 0.7456\n",
      "Epoch 18/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5236 - accuracy: 0.7339 - val_loss: 0.5207 - val_accuracy: 0.7420\n",
      "Epoch 19/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5235 - accuracy: 0.7345 - val_loss: 0.5215 - val_accuracy: 0.7402\n",
      "Epoch 20/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5237 - accuracy: 0.7344 - val_loss: 0.5207 - val_accuracy: 0.7426\n",
      "Epoch 21/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5225 - accuracy: 0.7351 - val_loss: 0.5187 - val_accuracy: 0.7385\n",
      "Epoch 22/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5218 - accuracy: 0.7344 - val_loss: 0.5206 - val_accuracy: 0.7404\n",
      "Epoch 23/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5226 - accuracy: 0.7347 - val_loss: 0.5190 - val_accuracy: 0.7438\n",
      "Epoch 24/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5222 - accuracy: 0.7349 - val_loss: 0.5191 - val_accuracy: 0.7429\n",
      "Epoch 25/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5217 - accuracy: 0.7353 - val_loss: 0.5178 - val_accuracy: 0.7384\n",
      "Epoch 26/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5217 - accuracy: 0.7363 - val_loss: 0.5198 - val_accuracy: 0.7426\n",
      "Epoch 27/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5217 - accuracy: 0.7363 - val_loss: 0.5184 - val_accuracy: 0.7446\n",
      "Epoch 28/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5214 - accuracy: 0.7349 - val_loss: 0.5175 - val_accuracy: 0.7432\n",
      "Epoch 29/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5213 - accuracy: 0.7348 - val_loss: 0.5216 - val_accuracy: 0.7461\n",
      "Epoch 30/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5210 - accuracy: 0.7360 - val_loss: 0.5193 - val_accuracy: 0.7394\n",
      "Epoch 31/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5203 - accuracy: 0.7361 - val_loss: 0.5176 - val_accuracy: 0.7378\n",
      "Epoch 32/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5199 - accuracy: 0.7369 - val_loss: 0.5184 - val_accuracy: 0.7381\n",
      "Epoch 33/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5203 - accuracy: 0.7353 - val_loss: 0.5181 - val_accuracy: 0.7413\n",
      "Epoch 34/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5203 - accuracy: 0.7359 - val_loss: 0.5179 - val_accuracy: 0.7412\n",
      "Epoch 35/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5193 - accuracy: 0.7366 - val_loss: 0.5181 - val_accuracy: 0.7448\n",
      "Epoch 36/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5205 - accuracy: 0.7362 - val_loss: 0.5169 - val_accuracy: 0.7413\n",
      "Epoch 37/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5199 - accuracy: 0.7368 - val_loss: 0.5172 - val_accuracy: 0.7436\n",
      "Epoch 38/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5195 - accuracy: 0.7383 - val_loss: 0.5190 - val_accuracy: 0.7423\n",
      "Epoch 39/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5194 - accuracy: 0.7367 - val_loss: 0.5184 - val_accuracy: 0.7399\n",
      "Epoch 40/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5193 - accuracy: 0.7372 - val_loss: 0.5176 - val_accuracy: 0.7412\n",
      "Epoch 41/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5196 - accuracy: 0.7376 - val_loss: 0.5184 - val_accuracy: 0.7403\n",
      "Epoch 42/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5188 - accuracy: 0.7380 - val_loss: 0.5182 - val_accuracy: 0.7419\n",
      "Epoch 43/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5189 - accuracy: 0.7378 - val_loss: 0.5184 - val_accuracy: 0.7423\n",
      "Epoch 44/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5190 - accuracy: 0.7360 - val_loss: 0.5165 - val_accuracy: 0.7436\n",
      "Epoch 45/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5183 - accuracy: 0.7392 - val_loss: 0.5182 - val_accuracy: 0.7470\n",
      "Epoch 46/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5184 - accuracy: 0.7394 - val_loss: 0.5180 - val_accuracy: 0.7370\n",
      "Epoch 47/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5178 - accuracy: 0.7383 - val_loss: 0.5158 - val_accuracy: 0.7441\n",
      "Epoch 48/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5176 - accuracy: 0.7393 - val_loss: 0.5182 - val_accuracy: 0.7375\n",
      "Epoch 49/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5180 - accuracy: 0.7379 - val_loss: 0.5174 - val_accuracy: 0.7419\n",
      "Epoch 50/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5174 - accuracy: 0.7389 - val_loss: 0.5174 - val_accuracy: 0.7455\n",
      "Epoch 51/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5173 - accuracy: 0.7379 - val_loss: 0.5173 - val_accuracy: 0.7427\n",
      "Epoch 52/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5167 - accuracy: 0.7394 - val_loss: 0.5191 - val_accuracy: 0.7467\n",
      "Epoch 53/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5170 - accuracy: 0.7390 - val_loss: 0.5200 - val_accuracy: 0.7460\n",
      "Epoch 54/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5168 - accuracy: 0.7376 - val_loss: 0.5168 - val_accuracy: 0.7456\n",
      "Epoch 55/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5167 - accuracy: 0.7397 - val_loss: 0.5163 - val_accuracy: 0.7447\n",
      "Epoch 56/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5161 - accuracy: 0.7391 - val_loss: 0.5167 - val_accuracy: 0.7442\n",
      "Epoch 57/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5156 - accuracy: 0.7393 - val_loss: 0.5174 - val_accuracy: 0.7452\n",
      "Epoch 58/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5163 - accuracy: 0.7396 - val_loss: 0.5188 - val_accuracy: 0.7407\n",
      "Epoch 59/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5167 - accuracy: 0.7390 - val_loss: 0.5163 - val_accuracy: 0.7431\n",
      "Epoch 60/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5157 - accuracy: 0.7399 - val_loss: 0.5183 - val_accuracy: 0.7439\n",
      "Epoch 61/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5159 - accuracy: 0.7399 - val_loss: 0.5167 - val_accuracy: 0.7443\n",
      "Epoch 62/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5155 - accuracy: 0.7405 - val_loss: 0.5174 - val_accuracy: 0.7452\n",
      "Epoch 63/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5157 - accuracy: 0.7403 - val_loss: 0.5169 - val_accuracy: 0.7429\n",
      "Epoch 64/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5155 - accuracy: 0.7395 - val_loss: 0.5174 - val_accuracy: 0.7449\n",
      "Epoch 65/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5159 - accuracy: 0.7387 - val_loss: 0.5163 - val_accuracy: 0.7455\n",
      "Epoch 66/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5153 - accuracy: 0.7401 - val_loss: 0.5165 - val_accuracy: 0.7417\n",
      "Epoch 67/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5147 - accuracy: 0.7408 - val_loss: 0.5163 - val_accuracy: 0.7471\n",
      "Epoch 68/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5152 - accuracy: 0.7404 - val_loss: 0.5158 - val_accuracy: 0.7441\n",
      "Epoch 69/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5147 - accuracy: 0.7410 - val_loss: 0.5157 - val_accuracy: 0.7453\n",
      "Epoch 70/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5149 - accuracy: 0.7406 - val_loss: 0.5161 - val_accuracy: 0.7467\n",
      "Epoch 71/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5147 - accuracy: 0.7396 - val_loss: 0.5163 - val_accuracy: 0.7418\n",
      "Epoch 72/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5139 - accuracy: 0.7414 - val_loss: 0.5169 - val_accuracy: 0.7446\n",
      "Epoch 73/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5148 - accuracy: 0.7414 - val_loss: 0.5164 - val_accuracy: 0.7439\n",
      "Epoch 74/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5145 - accuracy: 0.7394 - val_loss: 0.5161 - val_accuracy: 0.7436\n",
      "Epoch 75/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5134 - accuracy: 0.7411 - val_loss: 0.5167 - val_accuracy: 0.7455\n",
      "Epoch 76/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5135 - accuracy: 0.7416 - val_loss: 0.5165 - val_accuracy: 0.7442\n",
      "Epoch 77/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5143 - accuracy: 0.7403 - val_loss: 0.5171 - val_accuracy: 0.7462\n",
      "Epoch 78/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5135 - accuracy: 0.7395 - val_loss: 0.5162 - val_accuracy: 0.7463\n",
      "Epoch 79/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5137 - accuracy: 0.7409 - val_loss: 0.5180 - val_accuracy: 0.7429\n",
      "Epoch 80/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5131 - accuracy: 0.7423 - val_loss: 0.5178 - val_accuracy: 0.7395\n",
      "Epoch 81/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5135 - accuracy: 0.7408 - val_loss: 0.5171 - val_accuracy: 0.7429\n",
      "Epoch 82/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5130 - accuracy: 0.7418 - val_loss: 0.5173 - val_accuracy: 0.7441\n",
      "Epoch 83/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5136 - accuracy: 0.7412 - val_loss: 0.5188 - val_accuracy: 0.7441\n",
      "Epoch 84/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5126 - accuracy: 0.7422 - val_loss: 0.5174 - val_accuracy: 0.7470\n",
      "Epoch 85/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5132 - accuracy: 0.7408 - val_loss: 0.5176 - val_accuracy: 0.7412\n",
      "Epoch 86/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5129 - accuracy: 0.7414 - val_loss: 0.5185 - val_accuracy: 0.7456\n",
      "Epoch 87/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5123 - accuracy: 0.7419 - val_loss: 0.5182 - val_accuracy: 0.7417\n",
      "Epoch 88/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5122 - accuracy: 0.7416 - val_loss: 0.5155 - val_accuracy: 0.7477\n",
      "Epoch 89/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5121 - accuracy: 0.7414 - val_loss: 0.5181 - val_accuracy: 0.7471\n",
      "Epoch 90/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5120 - accuracy: 0.7426 - val_loss: 0.5171 - val_accuracy: 0.7449\n",
      "Epoch 91/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5113 - accuracy: 0.7417 - val_loss: 0.5172 - val_accuracy: 0.7453\n",
      "Epoch 92/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5119 - accuracy: 0.7417 - val_loss: 0.5170 - val_accuracy: 0.7458\n",
      "Epoch 93/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5108 - accuracy: 0.7417 - val_loss: 0.5182 - val_accuracy: 0.7419\n",
      "Epoch 94/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5108 - accuracy: 0.7426 - val_loss: 0.5196 - val_accuracy: 0.7472\n",
      "Epoch 95/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5114 - accuracy: 0.7428 - val_loss: 0.5172 - val_accuracy: 0.7468\n",
      "Epoch 96/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5113 - accuracy: 0.7425 - val_loss: 0.5169 - val_accuracy: 0.7442\n",
      "Epoch 97/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5108 - accuracy: 0.7433 - val_loss: 0.5180 - val_accuracy: 0.7451\n",
      "Epoch 98/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5109 - accuracy: 0.7421 - val_loss: 0.5189 - val_accuracy: 0.7427\n",
      "Epoch 99/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5109 - accuracy: 0.7423 - val_loss: 0.5185 - val_accuracy: 0.7452\n",
      "Epoch 100/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5107 - accuracy: 0.7421 - val_loss: 0.5197 - val_accuracy: 0.7424\n",
      "Epoch 101/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5108 - accuracy: 0.7421 - val_loss: 0.5205 - val_accuracy: 0.7497\n",
      "Epoch 102/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5108 - accuracy: 0.7427 - val_loss: 0.5186 - val_accuracy: 0.7452\n",
      "Epoch 103/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5099 - accuracy: 0.7424 - val_loss: 0.5237 - val_accuracy: 0.7505\n",
      "Epoch 104/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5101 - accuracy: 0.7418 - val_loss: 0.5187 - val_accuracy: 0.7428\n",
      "Epoch 105/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5099 - accuracy: 0.7428 - val_loss: 0.5178 - val_accuracy: 0.7467\n",
      "Epoch 106/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5097 - accuracy: 0.7427 - val_loss: 0.5174 - val_accuracy: 0.7475\n",
      "Epoch 107/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5093 - accuracy: 0.7417 - val_loss: 0.5167 - val_accuracy: 0.7434\n",
      "Epoch 108/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5095 - accuracy: 0.7442 - val_loss: 0.5181 - val_accuracy: 0.7426\n",
      "Epoch 109/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5098 - accuracy: 0.7424 - val_loss: 0.5169 - val_accuracy: 0.7424\n",
      "Epoch 110/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5089 - accuracy: 0.7433 - val_loss: 0.5181 - val_accuracy: 0.7448\n",
      "Epoch 111/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5090 - accuracy: 0.7428 - val_loss: 0.5222 - val_accuracy: 0.7463\n",
      "Epoch 112/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5094 - accuracy: 0.7438 - val_loss: 0.5183 - val_accuracy: 0.7431\n",
      "Epoch 113/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5091 - accuracy: 0.7452 - val_loss: 0.5183 - val_accuracy: 0.7447\n",
      "Epoch 114/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5083 - accuracy: 0.7430 - val_loss: 0.5174 - val_accuracy: 0.7465\n",
      "Epoch 115/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5088 - accuracy: 0.7433 - val_loss: 0.5176 - val_accuracy: 0.7429\n",
      "Epoch 116/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5083 - accuracy: 0.7442 - val_loss: 0.5171 - val_accuracy: 0.7466\n",
      "Epoch 117/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5091 - accuracy: 0.7429 - val_loss: 0.5178 - val_accuracy: 0.7427\n",
      "Epoch 118/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5083 - accuracy: 0.7443 - val_loss: 0.5185 - val_accuracy: 0.7422\n",
      "Epoch 119/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5078 - accuracy: 0.7428 - val_loss: 0.5198 - val_accuracy: 0.7453\n",
      "Epoch 120/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5078 - accuracy: 0.7441 - val_loss: 0.5181 - val_accuracy: 0.7410\n",
      "Epoch 121/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5079 - accuracy: 0.7424 - val_loss: 0.5200 - val_accuracy: 0.7477\n",
      "Epoch 122/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5081 - accuracy: 0.7437 - val_loss: 0.5214 - val_accuracy: 0.7491\n",
      "Epoch 123/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5078 - accuracy: 0.7426 - val_loss: 0.5189 - val_accuracy: 0.7468\n",
      "Epoch 124/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5079 - accuracy: 0.7444 - val_loss: 0.5183 - val_accuracy: 0.7458\n",
      "Epoch 125/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5068 - accuracy: 0.7439 - val_loss: 0.5193 - val_accuracy: 0.7444\n",
      "Epoch 126/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5077 - accuracy: 0.7438 - val_loss: 0.5186 - val_accuracy: 0.7447\n",
      "Epoch 127/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5072 - accuracy: 0.7440 - val_loss: 0.5222 - val_accuracy: 0.7476\n",
      "Epoch 128/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5062 - accuracy: 0.7447 - val_loss: 0.5202 - val_accuracy: 0.7471\n",
      "Epoch 129/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5075 - accuracy: 0.7452 - val_loss: 0.5193 - val_accuracy: 0.7429\n",
      "Epoch 130/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5072 - accuracy: 0.7432 - val_loss: 0.5189 - val_accuracy: 0.7408\n",
      "Epoch 131/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5069 - accuracy: 0.7444 - val_loss: 0.5189 - val_accuracy: 0.7455\n",
      "Epoch 132/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5059 - accuracy: 0.7455 - val_loss: 0.5202 - val_accuracy: 0.7494\n",
      "Epoch 133/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5062 - accuracy: 0.7451 - val_loss: 0.5184 - val_accuracy: 0.7463\n",
      "Epoch 134/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5072 - accuracy: 0.7442 - val_loss: 0.5201 - val_accuracy: 0.7453\n",
      "Epoch 135/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5063 - accuracy: 0.7445 - val_loss: 0.5212 - val_accuracy: 0.7457\n",
      "Epoch 136/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5069 - accuracy: 0.7442 - val_loss: 0.5192 - val_accuracy: 0.7455\n",
      "Epoch 137/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5065 - accuracy: 0.7444 - val_loss: 0.5192 - val_accuracy: 0.7442\n",
      "Epoch 138/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5056 - accuracy: 0.7442 - val_loss: 0.5180 - val_accuracy: 0.7456\n",
      "Epoch 139/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5065 - accuracy: 0.7446 - val_loss: 0.5179 - val_accuracy: 0.7434\n",
      "Epoch 140/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5063 - accuracy: 0.7432 - val_loss: 0.5209 - val_accuracy: 0.7463\n",
      "Epoch 141/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5059 - accuracy: 0.7440 - val_loss: 0.5195 - val_accuracy: 0.7458\n",
      "Epoch 142/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5061 - accuracy: 0.7459 - val_loss: 0.5189 - val_accuracy: 0.7419\n",
      "Epoch 143/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5057 - accuracy: 0.7443 - val_loss: 0.5203 - val_accuracy: 0.7428\n",
      "Epoch 144/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5054 - accuracy: 0.7449 - val_loss: 0.5181 - val_accuracy: 0.7426\n",
      "Epoch 145/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5057 - accuracy: 0.7445 - val_loss: 0.5189 - val_accuracy: 0.7442\n",
      "Epoch 146/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5050 - accuracy: 0.7448 - val_loss: 0.5195 - val_accuracy: 0.7451\n",
      "Epoch 147/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5058 - accuracy: 0.7447 - val_loss: 0.5195 - val_accuracy: 0.7431\n",
      "Epoch 148/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5054 - accuracy: 0.7442 - val_loss: 0.5183 - val_accuracy: 0.7443\n",
      "Epoch 149/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5058 - accuracy: 0.7455 - val_loss: 0.5187 - val_accuracy: 0.7444\n",
      "Epoch 150/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5049 - accuracy: 0.7447 - val_loss: 0.5197 - val_accuracy: 0.7452\n",
      "Epoch 151/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5055 - accuracy: 0.7453 - val_loss: 0.5193 - val_accuracy: 0.7441\n",
      "Epoch 152/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5052 - accuracy: 0.7462 - val_loss: 0.5186 - val_accuracy: 0.7473\n",
      "Epoch 153/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5040 - accuracy: 0.7454 - val_loss: 0.5217 - val_accuracy: 0.7428\n",
      "Epoch 154/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5042 - accuracy: 0.7460 - val_loss: 0.5189 - val_accuracy: 0.7424\n",
      "Epoch 155/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5042 - accuracy: 0.7462 - val_loss: 0.5219 - val_accuracy: 0.7485\n",
      "Epoch 156/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5041 - accuracy: 0.7460 - val_loss: 0.5197 - val_accuracy: 0.7453\n",
      "Epoch 157/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5045 - accuracy: 0.7452 - val_loss: 0.5200 - val_accuracy: 0.7446\n",
      "Epoch 158/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5043 - accuracy: 0.7454 - val_loss: 0.5229 - val_accuracy: 0.7444\n",
      "Epoch 159/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5039 - accuracy: 0.7464 - val_loss: 0.5198 - val_accuracy: 0.7441\n",
      "Epoch 160/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5037 - accuracy: 0.7462 - val_loss: 0.5204 - val_accuracy: 0.7443\n",
      "Epoch 161/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5039 - accuracy: 0.7455 - val_loss: 0.5191 - val_accuracy: 0.7451\n",
      "Epoch 162/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5034 - accuracy: 0.7467 - val_loss: 0.5181 - val_accuracy: 0.7420\n",
      "Epoch 163/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5038 - accuracy: 0.7459 - val_loss: 0.5202 - val_accuracy: 0.7417\n",
      "Epoch 164/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5039 - accuracy: 0.7444 - val_loss: 0.5184 - val_accuracy: 0.7461\n",
      "Epoch 165/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5032 - accuracy: 0.7481 - val_loss: 0.5197 - val_accuracy: 0.7429\n",
      "Epoch 166/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5031 - accuracy: 0.7457 - val_loss: 0.5200 - val_accuracy: 0.7407\n",
      "Epoch 167/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5034 - accuracy: 0.7449 - val_loss: 0.5215 - val_accuracy: 0.7442\n",
      "Epoch 168/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5034 - accuracy: 0.7462 - val_loss: 0.5203 - val_accuracy: 0.7429\n",
      "Epoch 169/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5033 - accuracy: 0.7463 - val_loss: 0.5206 - val_accuracy: 0.7446\n",
      "Epoch 170/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5022 - accuracy: 0.7471 - val_loss: 0.5220 - val_accuracy: 0.7420\n",
      "Epoch 171/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5029 - accuracy: 0.7463 - val_loss: 0.5219 - val_accuracy: 0.7455\n",
      "Epoch 172/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5028 - accuracy: 0.7467 - val_loss: 0.5199 - val_accuracy: 0.7444\n",
      "Epoch 173/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5035 - accuracy: 0.7460 - val_loss: 0.5187 - val_accuracy: 0.7453\n",
      "Epoch 174/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5022 - accuracy: 0.7470 - val_loss: 0.5213 - val_accuracy: 0.7455\n",
      "Epoch 175/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5027 - accuracy: 0.7460 - val_loss: 0.5197 - val_accuracy: 0.7439\n",
      "Epoch 176/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5031 - accuracy: 0.7467 - val_loss: 0.5187 - val_accuracy: 0.7443\n",
      "Epoch 177/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5020 - accuracy: 0.7476 - val_loss: 0.5211 - val_accuracy: 0.7432\n",
      "Epoch 178/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5026 - accuracy: 0.7464 - val_loss: 0.5213 - val_accuracy: 0.7453\n",
      "Epoch 179/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5023 - accuracy: 0.7478 - val_loss: 0.5234 - val_accuracy: 0.7434\n",
      "Epoch 180/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5018 - accuracy: 0.7467 - val_loss: 0.5226 - val_accuracy: 0.7432\n",
      "Epoch 181/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5026 - accuracy: 0.7463 - val_loss: 0.5216 - val_accuracy: 0.7431\n",
      "Epoch 182/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5013 - accuracy: 0.7471 - val_loss: 0.5218 - val_accuracy: 0.7458\n",
      "Epoch 183/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5025 - accuracy: 0.7463 - val_loss: 0.5227 - val_accuracy: 0.7390\n",
      "Epoch 184/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5013 - accuracy: 0.7469 - val_loss: 0.5215 - val_accuracy: 0.7449\n",
      "Epoch 185/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5025 - accuracy: 0.7470 - val_loss: 0.5204 - val_accuracy: 0.7470\n",
      "Epoch 186/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5011 - accuracy: 0.7476 - val_loss: 0.5214 - val_accuracy: 0.7458\n",
      "Epoch 187/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5013 - accuracy: 0.7476 - val_loss: 0.5233 - val_accuracy: 0.7473\n",
      "Epoch 188/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5013 - accuracy: 0.7475 - val_loss: 0.5206 - val_accuracy: 0.7453\n",
      "Epoch 189/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5009 - accuracy: 0.7475 - val_loss: 0.5221 - val_accuracy: 0.7436\n",
      "Epoch 190/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5013 - accuracy: 0.7477 - val_loss: 0.5228 - val_accuracy: 0.7453\n",
      "Epoch 191/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5020 - accuracy: 0.7475 - val_loss: 0.5208 - val_accuracy: 0.7441\n",
      "Epoch 192/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5018 - accuracy: 0.7469 - val_loss: 0.5216 - val_accuracy: 0.7444\n",
      "Epoch 193/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5005 - accuracy: 0.7473 - val_loss: 0.5219 - val_accuracy: 0.7453\n",
      "Epoch 194/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5004 - accuracy: 0.7471 - val_loss: 0.5248 - val_accuracy: 0.7466\n",
      "Epoch 195/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4999 - accuracy: 0.7486 - val_loss: 0.5259 - val_accuracy: 0.7449\n",
      "Epoch 196/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5008 - accuracy: 0.7484 - val_loss: 0.5223 - val_accuracy: 0.7462\n",
      "Epoch 197/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5013 - accuracy: 0.7470 - val_loss: 0.5220 - val_accuracy: 0.7426\n",
      "Epoch 198/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5006 - accuracy: 0.7486 - val_loss: 0.5224 - val_accuracy: 0.7444\n",
      "Epoch 199/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5002 - accuracy: 0.7467 - val_loss: 0.5233 - val_accuracy: 0.7434\n",
      "Epoch 200/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5002 - accuracy: 0.7495 - val_loss: 0.5229 - val_accuracy: 0.7433\n",
      "Epoch 201/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4997 - accuracy: 0.7476 - val_loss: 0.5222 - val_accuracy: 0.7444\n",
      "Epoch 202/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5004 - accuracy: 0.7486 - val_loss: 0.5227 - val_accuracy: 0.7460\n",
      "Epoch 203/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4994 - accuracy: 0.7472 - val_loss: 0.5220 - val_accuracy: 0.7426\n",
      "Epoch 204/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5006 - accuracy: 0.7478 - val_loss: 0.5215 - val_accuracy: 0.7453\n",
      "Epoch 205/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5001 - accuracy: 0.7477 - val_loss: 0.5236 - val_accuracy: 0.7463\n",
      "Epoch 206/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5003 - accuracy: 0.7482 - val_loss: 0.5229 - val_accuracy: 0.7439\n",
      "Epoch 207/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4991 - accuracy: 0.7485 - val_loss: 0.5219 - val_accuracy: 0.7441\n",
      "Epoch 208/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4994 - accuracy: 0.7496 - val_loss: 0.5216 - val_accuracy: 0.7428\n",
      "Epoch 209/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4992 - accuracy: 0.7500 - val_loss: 0.5233 - val_accuracy: 0.7418\n",
      "Epoch 210/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.5002 - accuracy: 0.7491 - val_loss: 0.5228 - val_accuracy: 0.7418\n",
      "Epoch 211/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4993 - accuracy: 0.7488 - val_loss: 0.5221 - val_accuracy: 0.7437\n",
      "Epoch 212/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4986 - accuracy: 0.7486 - val_loss: 0.5224 - val_accuracy: 0.7436\n",
      "Epoch 213/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4992 - accuracy: 0.7484 - val_loss: 0.5231 - val_accuracy: 0.7433\n",
      "Epoch 214/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4991 - accuracy: 0.7479 - val_loss: 0.5233 - val_accuracy: 0.7414\n",
      "Epoch 215/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4984 - accuracy: 0.7487 - val_loss: 0.5235 - val_accuracy: 0.7433\n",
      "Epoch 216/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4988 - accuracy: 0.7490 - val_loss: 0.5242 - val_accuracy: 0.7426\n",
      "Epoch 217/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4983 - accuracy: 0.7498 - val_loss: 0.5222 - val_accuracy: 0.7391\n",
      "Epoch 218/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4984 - accuracy: 0.7485 - val_loss: 0.5237 - val_accuracy: 0.7457\n",
      "Epoch 219/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4990 - accuracy: 0.7492 - val_loss: 0.5240 - val_accuracy: 0.7461\n",
      "Epoch 220/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4986 - accuracy: 0.7508 - val_loss: 0.5257 - val_accuracy: 0.7432\n",
      "Epoch 221/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4995 - accuracy: 0.7487 - val_loss: 0.5237 - val_accuracy: 0.7434\n",
      "Epoch 222/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4982 - accuracy: 0.7498 - val_loss: 0.5240 - val_accuracy: 0.7408\n",
      "Epoch 223/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4984 - accuracy: 0.7495 - val_loss: 0.5234 - val_accuracy: 0.7422\n",
      "Epoch 224/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4980 - accuracy: 0.7493 - val_loss: 0.5221 - val_accuracy: 0.7384\n",
      "Epoch 225/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4979 - accuracy: 0.7498 - val_loss: 0.5230 - val_accuracy: 0.7434\n",
      "Epoch 226/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4982 - accuracy: 0.7501 - val_loss: 0.5258 - val_accuracy: 0.7452\n",
      "Epoch 227/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4985 - accuracy: 0.7489 - val_loss: 0.5233 - val_accuracy: 0.7455\n",
      "Epoch 228/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4979 - accuracy: 0.7487 - val_loss: 0.5241 - val_accuracy: 0.7426\n",
      "Epoch 229/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4978 - accuracy: 0.7499 - val_loss: 0.5250 - val_accuracy: 0.7441\n",
      "Epoch 230/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4975 - accuracy: 0.7502 - val_loss: 0.5281 - val_accuracy: 0.7399\n",
      "Epoch 231/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4969 - accuracy: 0.7511 - val_loss: 0.5253 - val_accuracy: 0.7447\n",
      "Epoch 232/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4973 - accuracy: 0.7506 - val_loss: 0.5275 - val_accuracy: 0.7461\n",
      "Epoch 233/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4972 - accuracy: 0.7509 - val_loss: 0.5242 - val_accuracy: 0.7424\n",
      "Epoch 234/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4972 - accuracy: 0.7497 - val_loss: 0.5292 - val_accuracy: 0.7426\n",
      "Epoch 235/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4970 - accuracy: 0.7509 - val_loss: 0.5236 - val_accuracy: 0.7422\n",
      "Epoch 236/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4973 - accuracy: 0.7504 - val_loss: 0.5231 - val_accuracy: 0.7456\n",
      "Epoch 237/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4961 - accuracy: 0.7516 - val_loss: 0.5240 - val_accuracy: 0.7465\n",
      "Epoch 238/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4967 - accuracy: 0.7518 - val_loss: 0.5241 - val_accuracy: 0.7457\n",
      "Epoch 239/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4968 - accuracy: 0.7503 - val_loss: 0.5239 - val_accuracy: 0.7451\n",
      "Epoch 240/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4961 - accuracy: 0.7502 - val_loss: 0.5249 - val_accuracy: 0.7438\n",
      "Epoch 241/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4963 - accuracy: 0.7500 - val_loss: 0.5276 - val_accuracy: 0.7415\n",
      "Epoch 242/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4961 - accuracy: 0.7514 - val_loss: 0.5252 - val_accuracy: 0.7428\n",
      "Epoch 243/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4953 - accuracy: 0.7507 - val_loss: 0.5266 - val_accuracy: 0.7458\n",
      "Epoch 244/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4965 - accuracy: 0.7499 - val_loss: 0.5233 - val_accuracy: 0.7446\n",
      "Epoch 245/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4972 - accuracy: 0.7502 - val_loss: 0.5238 - val_accuracy: 0.7462\n",
      "Epoch 246/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4959 - accuracy: 0.7512 - val_loss: 0.5258 - val_accuracy: 0.7456\n",
      "Epoch 247/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4970 - accuracy: 0.7506 - val_loss: 0.5241 - val_accuracy: 0.7412\n",
      "Epoch 248/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4962 - accuracy: 0.7508 - val_loss: 0.5251 - val_accuracy: 0.7436\n",
      "Epoch 249/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4957 - accuracy: 0.7514 - val_loss: 0.5241 - val_accuracy: 0.7438\n",
      "Epoch 250/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4966 - accuracy: 0.7504 - val_loss: 0.5254 - val_accuracy: 0.7436\n",
      "Epoch 251/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4949 - accuracy: 0.7515 - val_loss: 0.5249 - val_accuracy: 0.7453\n",
      "Epoch 252/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4959 - accuracy: 0.7509 - val_loss: 0.5239 - val_accuracy: 0.7423\n",
      "Epoch 253/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4957 - accuracy: 0.7513 - val_loss: 0.5243 - val_accuracy: 0.7426\n",
      "Epoch 254/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4959 - accuracy: 0.7498 - val_loss: 0.5258 - val_accuracy: 0.7434\n",
      "Epoch 255/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4955 - accuracy: 0.7519 - val_loss: 0.5248 - val_accuracy: 0.7453\n",
      "Epoch 256/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4952 - accuracy: 0.7528 - val_loss: 0.5247 - val_accuracy: 0.7449\n",
      "Epoch 257/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4947 - accuracy: 0.7500 - val_loss: 0.5264 - val_accuracy: 0.7433\n",
      "Epoch 258/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4955 - accuracy: 0.7516 - val_loss: 0.5237 - val_accuracy: 0.7444\n",
      "Epoch 259/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4949 - accuracy: 0.7514 - val_loss: 0.5276 - val_accuracy: 0.7460\n",
      "Epoch 260/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4951 - accuracy: 0.7527 - val_loss: 0.5248 - val_accuracy: 0.7422\n",
      "Epoch 261/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4945 - accuracy: 0.7506 - val_loss: 0.5247 - val_accuracy: 0.7438\n",
      "Epoch 262/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4948 - accuracy: 0.7519 - val_loss: 0.5246 - val_accuracy: 0.7456\n",
      "Epoch 263/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4943 - accuracy: 0.7523 - val_loss: 0.5290 - val_accuracy: 0.7451\n",
      "Epoch 264/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4951 - accuracy: 0.7519 - val_loss: 0.5249 - val_accuracy: 0.7452\n",
      "Epoch 265/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4944 - accuracy: 0.7519 - val_loss: 0.5255 - val_accuracy: 0.7413\n",
      "Epoch 266/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4941 - accuracy: 0.7505 - val_loss: 0.5256 - val_accuracy: 0.7456\n",
      "Epoch 267/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4939 - accuracy: 0.7522 - val_loss: 0.5286 - val_accuracy: 0.7456\n",
      "Epoch 268/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4927 - accuracy: 0.7533 - val_loss: 0.5278 - val_accuracy: 0.7417\n",
      "Epoch 269/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4939 - accuracy: 0.7505 - val_loss: 0.5273 - val_accuracy: 0.7481\n",
      "Epoch 270/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4941 - accuracy: 0.7514 - val_loss: 0.5244 - val_accuracy: 0.7466\n",
      "Epoch 271/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4937 - accuracy: 0.7532 - val_loss: 0.5250 - val_accuracy: 0.7446\n",
      "Epoch 272/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4936 - accuracy: 0.7525 - val_loss: 0.5260 - val_accuracy: 0.7434\n",
      "Epoch 273/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4933 - accuracy: 0.7528 - val_loss: 0.5278 - val_accuracy: 0.7442\n",
      "Epoch 274/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4936 - accuracy: 0.7525 - val_loss: 0.5267 - val_accuracy: 0.7448\n",
      "Epoch 275/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4930 - accuracy: 0.7527 - val_loss: 0.5272 - val_accuracy: 0.7458\n",
      "Epoch 276/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4931 - accuracy: 0.7517 - val_loss: 0.5284 - val_accuracy: 0.7443\n",
      "Epoch 277/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4947 - accuracy: 0.7532 - val_loss: 0.5279 - val_accuracy: 0.7434\n",
      "Epoch 278/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4930 - accuracy: 0.7522 - val_loss: 0.5263 - val_accuracy: 0.7407\n",
      "Epoch 279/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4941 - accuracy: 0.7526 - val_loss: 0.5264 - val_accuracy: 0.7477\n",
      "Epoch 280/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4924 - accuracy: 0.7532 - val_loss: 0.5250 - val_accuracy: 0.7419\n",
      "Epoch 281/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4930 - accuracy: 0.7533 - val_loss: 0.5276 - val_accuracy: 0.7441\n",
      "Epoch 282/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4920 - accuracy: 0.7535 - val_loss: 0.5277 - val_accuracy: 0.7470\n",
      "Epoch 283/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4935 - accuracy: 0.7516 - val_loss: 0.5267 - val_accuracy: 0.7431\n",
      "Epoch 284/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4928 - accuracy: 0.7534 - val_loss: 0.5279 - val_accuracy: 0.7439\n",
      "Epoch 285/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4924 - accuracy: 0.7537 - val_loss: 0.5249 - val_accuracy: 0.7427\n",
      "Epoch 286/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4918 - accuracy: 0.7518 - val_loss: 0.5280 - val_accuracy: 0.7436\n",
      "Epoch 287/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4930 - accuracy: 0.7527 - val_loss: 0.5287 - val_accuracy: 0.7410\n",
      "Epoch 288/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4931 - accuracy: 0.7531 - val_loss: 0.5254 - val_accuracy: 0.7460\n",
      "Epoch 289/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4921 - accuracy: 0.7536 - val_loss: 0.5282 - val_accuracy: 0.7439\n",
      "Epoch 290/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4923 - accuracy: 0.7540 - val_loss: 0.5264 - val_accuracy: 0.7418\n",
      "Epoch 291/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4923 - accuracy: 0.7527 - val_loss: 0.5279 - val_accuracy: 0.7467\n",
      "Epoch 292/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4926 - accuracy: 0.7536 - val_loss: 0.5260 - val_accuracy: 0.7442\n",
      "Epoch 293/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4920 - accuracy: 0.7530 - val_loss: 0.5285 - val_accuracy: 0.7468\n",
      "Epoch 294/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4926 - accuracy: 0.7534 - val_loss: 0.5277 - val_accuracy: 0.7442\n",
      "Epoch 295/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4917 - accuracy: 0.7533 - val_loss: 0.5292 - val_accuracy: 0.7441\n",
      "Epoch 296/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4923 - accuracy: 0.7531 - val_loss: 0.5279 - val_accuracy: 0.7429\n",
      "Epoch 297/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4921 - accuracy: 0.7534 - val_loss: 0.5316 - val_accuracy: 0.7446\n",
      "Epoch 298/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4919 - accuracy: 0.7532 - val_loss: 0.5281 - val_accuracy: 0.7448\n",
      "Epoch 299/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4924 - accuracy: 0.7536 - val_loss: 0.5277 - val_accuracy: 0.7432\n",
      "Epoch 300/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4918 - accuracy: 0.7532 - val_loss: 0.5276 - val_accuracy: 0.7467\n",
      "Epoch 301/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4911 - accuracy: 0.7539 - val_loss: 0.5303 - val_accuracy: 0.7453\n",
      "Epoch 302/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4919 - accuracy: 0.7544 - val_loss: 0.5271 - val_accuracy: 0.7457\n",
      "Epoch 303/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4916 - accuracy: 0.7525 - val_loss: 0.5310 - val_accuracy: 0.7477\n",
      "Epoch 304/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4909 - accuracy: 0.7546 - val_loss: 0.5284 - val_accuracy: 0.7431\n",
      "Epoch 305/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4911 - accuracy: 0.7528 - val_loss: 0.5309 - val_accuracy: 0.7427\n",
      "Epoch 306/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4918 - accuracy: 0.7548 - val_loss: 0.5311 - val_accuracy: 0.7448\n",
      "Epoch 307/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4910 - accuracy: 0.7523 - val_loss: 0.5285 - val_accuracy: 0.7431\n",
      "Epoch 308/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4903 - accuracy: 0.7528 - val_loss: 0.5281 - val_accuracy: 0.7436\n",
      "Epoch 309/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4908 - accuracy: 0.7543 - val_loss: 0.5305 - val_accuracy: 0.7433\n",
      "Epoch 310/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4915 - accuracy: 0.7539 - val_loss: 0.5285 - val_accuracy: 0.7433\n",
      "Epoch 311/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4901 - accuracy: 0.7538 - val_loss: 0.5288 - val_accuracy: 0.7432\n",
      "Epoch 312/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4908 - accuracy: 0.7536 - val_loss: 0.5298 - val_accuracy: 0.7463\n",
      "Epoch 313/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4910 - accuracy: 0.7541 - val_loss: 0.5333 - val_accuracy: 0.7461\n",
      "Epoch 314/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4903 - accuracy: 0.7540 - val_loss: 0.5284 - val_accuracy: 0.7447\n",
      "Epoch 315/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4907 - accuracy: 0.7540 - val_loss: 0.5282 - val_accuracy: 0.7448\n",
      "Epoch 316/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4902 - accuracy: 0.7538 - val_loss: 0.5285 - val_accuracy: 0.7461\n",
      "Epoch 317/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4903 - accuracy: 0.7546 - val_loss: 0.5303 - val_accuracy: 0.7441\n",
      "Epoch 318/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4905 - accuracy: 0.7535 - val_loss: 0.5281 - val_accuracy: 0.7451\n",
      "Epoch 319/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4899 - accuracy: 0.7553 - val_loss: 0.5269 - val_accuracy: 0.7461\n",
      "Epoch 320/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4890 - accuracy: 0.7552 - val_loss: 0.5331 - val_accuracy: 0.7432\n",
      "Epoch 321/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4902 - accuracy: 0.7555 - val_loss: 0.5275 - val_accuracy: 0.7426\n",
      "Epoch 322/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4908 - accuracy: 0.7544 - val_loss: 0.5285 - val_accuracy: 0.7453\n",
      "Epoch 323/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4887 - accuracy: 0.7560 - val_loss: 0.5286 - val_accuracy: 0.7439\n",
      "Epoch 324/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4899 - accuracy: 0.7542 - val_loss: 0.5262 - val_accuracy: 0.7444\n",
      "Epoch 325/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4902 - accuracy: 0.7542 - val_loss: 0.5317 - val_accuracy: 0.7465\n",
      "Epoch 326/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4897 - accuracy: 0.7538 - val_loss: 0.5318 - val_accuracy: 0.7463\n",
      "Epoch 327/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4896 - accuracy: 0.7556 - val_loss: 0.5302 - val_accuracy: 0.7458\n",
      "Epoch 328/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4887 - accuracy: 0.7561 - val_loss: 0.5305 - val_accuracy: 0.7395\n",
      "Epoch 329/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4890 - accuracy: 0.7540 - val_loss: 0.5283 - val_accuracy: 0.7431\n",
      "Epoch 330/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4882 - accuracy: 0.7561 - val_loss: 0.5290 - val_accuracy: 0.7438\n",
      "Epoch 331/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4889 - accuracy: 0.7548 - val_loss: 0.5293 - val_accuracy: 0.7443\n",
      "Epoch 332/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4882 - accuracy: 0.7564 - val_loss: 0.5307 - val_accuracy: 0.7451\n",
      "Epoch 333/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4885 - accuracy: 0.7547 - val_loss: 0.5301 - val_accuracy: 0.7428\n",
      "Epoch 334/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4887 - accuracy: 0.7565 - val_loss: 0.5284 - val_accuracy: 0.7437\n",
      "Epoch 335/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4879 - accuracy: 0.7551 - val_loss: 0.5327 - val_accuracy: 0.7419\n",
      "Epoch 336/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4881 - accuracy: 0.7561 - val_loss: 0.5303 - val_accuracy: 0.7443\n",
      "Epoch 337/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4886 - accuracy: 0.7549 - val_loss: 0.5303 - val_accuracy: 0.7446\n",
      "Epoch 338/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4886 - accuracy: 0.7559 - val_loss: 0.5294 - val_accuracy: 0.7452\n",
      "Epoch 339/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4879 - accuracy: 0.7556 - val_loss: 0.5296 - val_accuracy: 0.7463\n",
      "Epoch 340/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4885 - accuracy: 0.7553 - val_loss: 0.5314 - val_accuracy: 0.7434\n",
      "Epoch 341/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4877 - accuracy: 0.7555 - val_loss: 0.5306 - val_accuracy: 0.7453\n",
      "Epoch 342/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4882 - accuracy: 0.7549 - val_loss: 0.5292 - val_accuracy: 0.7428\n",
      "Epoch 343/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4872 - accuracy: 0.7570 - val_loss: 0.5326 - val_accuracy: 0.7451\n",
      "Epoch 344/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4875 - accuracy: 0.7545 - val_loss: 0.5316 - val_accuracy: 0.7414\n",
      "Epoch 345/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4886 - accuracy: 0.7551 - val_loss: 0.5308 - val_accuracy: 0.7408\n",
      "Epoch 346/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4889 - accuracy: 0.7550 - val_loss: 0.5320 - val_accuracy: 0.7439\n",
      "Epoch 347/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4881 - accuracy: 0.7562 - val_loss: 0.5324 - val_accuracy: 0.7449\n",
      "Epoch 348/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4874 - accuracy: 0.7567 - val_loss: 0.5310 - val_accuracy: 0.7418\n",
      "Epoch 349/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4877 - accuracy: 0.7557 - val_loss: 0.5339 - val_accuracy: 0.7453\n",
      "Epoch 350/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4876 - accuracy: 0.7569 - val_loss: 0.5322 - val_accuracy: 0.7427\n",
      "Epoch 351/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4866 - accuracy: 0.7564 - val_loss: 0.5316 - val_accuracy: 0.7448\n",
      "Epoch 352/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4885 - accuracy: 0.7563 - val_loss: 0.5319 - val_accuracy: 0.7442\n",
      "Epoch 353/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4870 - accuracy: 0.7561 - val_loss: 0.5317 - val_accuracy: 0.7410\n",
      "Epoch 354/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4869 - accuracy: 0.7558 - val_loss: 0.5319 - val_accuracy: 0.7458\n",
      "Epoch 355/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4875 - accuracy: 0.7573 - val_loss: 0.5306 - val_accuracy: 0.7449\n",
      "Epoch 356/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4878 - accuracy: 0.7552 - val_loss: 0.5366 - val_accuracy: 0.7448\n",
      "Epoch 357/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4862 - accuracy: 0.7584 - val_loss: 0.5319 - val_accuracy: 0.7457\n",
      "Epoch 358/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4871 - accuracy: 0.7558 - val_loss: 0.5316 - val_accuracy: 0.7455\n",
      "Epoch 359/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4864 - accuracy: 0.7564 - val_loss: 0.5321 - val_accuracy: 0.7443\n",
      "Epoch 360/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4876 - accuracy: 0.7549 - val_loss: 0.5325 - val_accuracy: 0.7436\n",
      "Epoch 361/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4879 - accuracy: 0.7559 - val_loss: 0.5353 - val_accuracy: 0.7429\n",
      "Epoch 362/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4874 - accuracy: 0.7562 - val_loss: 0.5296 - val_accuracy: 0.7433\n",
      "Epoch 363/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4865 - accuracy: 0.7572 - val_loss: 0.5323 - val_accuracy: 0.7420\n",
      "Epoch 364/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4862 - accuracy: 0.7564 - val_loss: 0.5351 - val_accuracy: 0.7434\n",
      "Epoch 365/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4857 - accuracy: 0.7580 - val_loss: 0.5344 - val_accuracy: 0.7448\n",
      "Epoch 366/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4867 - accuracy: 0.7561 - val_loss: 0.5324 - val_accuracy: 0.7433\n",
      "Epoch 367/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4852 - accuracy: 0.7575 - val_loss: 0.5343 - val_accuracy: 0.7441\n",
      "Epoch 368/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4861 - accuracy: 0.7566 - val_loss: 0.5365 - val_accuracy: 0.7439\n",
      "Epoch 369/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4854 - accuracy: 0.7569 - val_loss: 0.5335 - val_accuracy: 0.7453\n",
      "Epoch 370/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4861 - accuracy: 0.7564 - val_loss: 0.5329 - val_accuracy: 0.7444\n",
      "Epoch 371/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4863 - accuracy: 0.7563 - val_loss: 0.5333 - val_accuracy: 0.7437\n",
      "Epoch 372/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4848 - accuracy: 0.7565 - val_loss: 0.5385 - val_accuracy: 0.7447\n",
      "Epoch 373/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4865 - accuracy: 0.7572 - val_loss: 0.5360 - val_accuracy: 0.7431\n",
      "Epoch 374/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4859 - accuracy: 0.7563 - val_loss: 0.5340 - val_accuracy: 0.7424\n",
      "Epoch 375/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4861 - accuracy: 0.7568 - val_loss: 0.5302 - val_accuracy: 0.7426\n",
      "Epoch 376/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4860 - accuracy: 0.7566 - val_loss: 0.5384 - val_accuracy: 0.7456\n",
      "Epoch 377/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4851 - accuracy: 0.7567 - val_loss: 0.5321 - val_accuracy: 0.7442\n",
      "Epoch 378/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4861 - accuracy: 0.7558 - val_loss: 0.5349 - val_accuracy: 0.7444\n",
      "Epoch 379/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4852 - accuracy: 0.7576 - val_loss: 0.5364 - val_accuracy: 0.7438\n",
      "Epoch 380/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4857 - accuracy: 0.7565 - val_loss: 0.5331 - val_accuracy: 0.7457\n",
      "Epoch 381/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4846 - accuracy: 0.7569 - val_loss: 0.5357 - val_accuracy: 0.7439\n",
      "Epoch 382/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4849 - accuracy: 0.7570 - val_loss: 0.5376 - val_accuracy: 0.7453\n",
      "Epoch 383/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4852 - accuracy: 0.7571 - val_loss: 0.5326 - val_accuracy: 0.7441\n",
      "Epoch 384/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4850 - accuracy: 0.7571 - val_loss: 0.5321 - val_accuracy: 0.7427\n",
      "Epoch 385/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4855 - accuracy: 0.7569 - val_loss: 0.5339 - val_accuracy: 0.7443\n",
      "Epoch 386/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4859 - accuracy: 0.7562 - val_loss: 0.5370 - val_accuracy: 0.7442\n",
      "Epoch 387/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4849 - accuracy: 0.7581 - val_loss: 0.5390 - val_accuracy: 0.7434\n",
      "Epoch 388/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4823 - accuracy: 0.7586 - val_loss: 0.5368 - val_accuracy: 0.7441\n",
      "Epoch 389/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4838 - accuracy: 0.7577 - val_loss: 0.5360 - val_accuracy: 0.7424\n",
      "Epoch 390/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4842 - accuracy: 0.7582 - val_loss: 0.5366 - val_accuracy: 0.7429\n",
      "Epoch 391/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4848 - accuracy: 0.7571 - val_loss: 0.5360 - val_accuracy: 0.7441\n",
      "Epoch 392/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4844 - accuracy: 0.7577 - val_loss: 0.5357 - val_accuracy: 0.7442\n",
      "Epoch 393/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4858 - accuracy: 0.7581 - val_loss: 0.5360 - val_accuracy: 0.7417\n",
      "Epoch 394/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4845 - accuracy: 0.7580 - val_loss: 0.5385 - val_accuracy: 0.7452\n",
      "Epoch 395/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4837 - accuracy: 0.7579 - val_loss: 0.5381 - val_accuracy: 0.7461\n",
      "Epoch 396/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4843 - accuracy: 0.7575 - val_loss: 0.5354 - val_accuracy: 0.7431\n",
      "Epoch 397/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4840 - accuracy: 0.7570 - val_loss: 0.5345 - val_accuracy: 0.7433\n",
      "Epoch 398/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4847 - accuracy: 0.7582 - val_loss: 0.5341 - val_accuracy: 0.7439\n",
      "Epoch 399/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4838 - accuracy: 0.7583 - val_loss: 0.5388 - val_accuracy: 0.7436\n",
      "Epoch 400/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4836 - accuracy: 0.7581 - val_loss: 0.5372 - val_accuracy: 0.7449\n",
      "Epoch 401/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4840 - accuracy: 0.7586 - val_loss: 0.5368 - val_accuracy: 0.7466\n",
      "Epoch 402/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4840 - accuracy: 0.7584 - val_loss: 0.5360 - val_accuracy: 0.7424\n",
      "Epoch 403/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4835 - accuracy: 0.7570 - val_loss: 0.5351 - val_accuracy: 0.7442\n",
      "Epoch 404/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4844 - accuracy: 0.7575 - val_loss: 0.5343 - val_accuracy: 0.7426\n",
      "Epoch 405/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4828 - accuracy: 0.7576 - val_loss: 0.5392 - val_accuracy: 0.7453\n",
      "Epoch 406/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4833 - accuracy: 0.7573 - val_loss: 0.5352 - val_accuracy: 0.7431\n",
      "Epoch 407/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4847 - accuracy: 0.7578 - val_loss: 0.5405 - val_accuracy: 0.7449\n",
      "Epoch 408/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4836 - accuracy: 0.7585 - val_loss: 0.5359 - val_accuracy: 0.7428\n",
      "Epoch 409/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4838 - accuracy: 0.7578 - val_loss: 0.5341 - val_accuracy: 0.7455\n",
      "Epoch 410/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4822 - accuracy: 0.7588 - val_loss: 0.5404 - val_accuracy: 0.7465\n",
      "Epoch 411/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4827 - accuracy: 0.7600 - val_loss: 0.5404 - val_accuracy: 0.7451\n",
      "Epoch 412/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4831 - accuracy: 0.7595 - val_loss: 0.5387 - val_accuracy: 0.7432\n",
      "Epoch 413/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4843 - accuracy: 0.7582 - val_loss: 0.5351 - val_accuracy: 0.7443\n",
      "Epoch 414/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4840 - accuracy: 0.7572 - val_loss: 0.5373 - val_accuracy: 0.7432\n",
      "Epoch 415/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4825 - accuracy: 0.7588 - val_loss: 0.5397 - val_accuracy: 0.7426\n",
      "Epoch 416/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4824 - accuracy: 0.7588 - val_loss: 0.5365 - val_accuracy: 0.7432\n",
      "Epoch 417/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4831 - accuracy: 0.7589 - val_loss: 0.5389 - val_accuracy: 0.7432\n",
      "Epoch 418/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4826 - accuracy: 0.7580 - val_loss: 0.5422 - val_accuracy: 0.7444\n",
      "Epoch 419/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4827 - accuracy: 0.7588 - val_loss: 0.5371 - val_accuracy: 0.7447\n",
      "Epoch 420/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4833 - accuracy: 0.7585 - val_loss: 0.5356 - val_accuracy: 0.7437\n",
      "Epoch 421/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4821 - accuracy: 0.7600 - val_loss: 0.5384 - val_accuracy: 0.7429\n",
      "Epoch 422/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4833 - accuracy: 0.7586 - val_loss: 0.5354 - val_accuracy: 0.7393\n",
      "Epoch 423/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4827 - accuracy: 0.7581 - val_loss: 0.5364 - val_accuracy: 0.7428\n",
      "Epoch 424/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4823 - accuracy: 0.7594 - val_loss: 0.5358 - val_accuracy: 0.7451\n",
      "Epoch 425/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4827 - accuracy: 0.7596 - val_loss: 0.5388 - val_accuracy: 0.7427\n",
      "Epoch 426/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4831 - accuracy: 0.7586 - val_loss: 0.5366 - val_accuracy: 0.7449\n",
      "Epoch 427/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4817 - accuracy: 0.7600 - val_loss: 0.5373 - val_accuracy: 0.7424\n",
      "Epoch 428/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4812 - accuracy: 0.7596 - val_loss: 0.5375 - val_accuracy: 0.7453\n",
      "Epoch 429/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4815 - accuracy: 0.7593 - val_loss: 0.5366 - val_accuracy: 0.7432\n",
      "Epoch 430/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4829 - accuracy: 0.7592 - val_loss: 0.5375 - val_accuracy: 0.7438\n",
      "Epoch 431/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4822 - accuracy: 0.7594 - val_loss: 0.5380 - val_accuracy: 0.7422\n",
      "Epoch 432/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4824 - accuracy: 0.7605 - val_loss: 0.5389 - val_accuracy: 0.7428\n",
      "Epoch 433/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4827 - accuracy: 0.7594 - val_loss: 0.5367 - val_accuracy: 0.7439\n",
      "Epoch 434/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4820 - accuracy: 0.7588 - val_loss: 0.5360 - val_accuracy: 0.7458\n",
      "Epoch 435/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4816 - accuracy: 0.7588 - val_loss: 0.5393 - val_accuracy: 0.7447\n",
      "Epoch 436/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4806 - accuracy: 0.7614 - val_loss: 0.5354 - val_accuracy: 0.7447\n",
      "Epoch 437/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4813 - accuracy: 0.7584 - val_loss: 0.5396 - val_accuracy: 0.7444\n",
      "Epoch 438/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4812 - accuracy: 0.7586 - val_loss: 0.5378 - val_accuracy: 0.7479\n",
      "Epoch 439/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4814 - accuracy: 0.7591 - val_loss: 0.5371 - val_accuracy: 0.7426\n",
      "Epoch 440/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4822 - accuracy: 0.7580 - val_loss: 0.5402 - val_accuracy: 0.7442\n",
      "Epoch 441/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4800 - accuracy: 0.7591 - val_loss: 0.5372 - val_accuracy: 0.7437\n",
      "Epoch 442/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4809 - accuracy: 0.7588 - val_loss: 0.5384 - val_accuracy: 0.7432\n",
      "Epoch 443/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4820 - accuracy: 0.7598 - val_loss: 0.5395 - val_accuracy: 0.7441\n",
      "Epoch 444/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4817 - accuracy: 0.7586 - val_loss: 0.5401 - val_accuracy: 0.7457\n",
      "Epoch 445/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4815 - accuracy: 0.7594 - val_loss: 0.5398 - val_accuracy: 0.7428\n",
      "Epoch 446/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4802 - accuracy: 0.7597 - val_loss: 0.5425 - val_accuracy: 0.7422\n",
      "Epoch 447/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4813 - accuracy: 0.7594 - val_loss: 0.5394 - val_accuracy: 0.7446\n",
      "Epoch 448/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4819 - accuracy: 0.7582 - val_loss: 0.5403 - val_accuracy: 0.7442\n",
      "Epoch 449/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4808 - accuracy: 0.7618 - val_loss: 0.5419 - val_accuracy: 0.7438\n",
      "Epoch 450/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4822 - accuracy: 0.7589 - val_loss: 0.5402 - val_accuracy: 0.7447\n",
      "Epoch 451/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4811 - accuracy: 0.7606 - val_loss: 0.5387 - val_accuracy: 0.7461\n",
      "Epoch 452/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4804 - accuracy: 0.7603 - val_loss: 0.5395 - val_accuracy: 0.7468\n",
      "Epoch 453/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4813 - accuracy: 0.7602 - val_loss: 0.5397 - val_accuracy: 0.7455\n",
      "Epoch 454/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4803 - accuracy: 0.7604 - val_loss: 0.5394 - val_accuracy: 0.7414\n",
      "Epoch 455/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4806 - accuracy: 0.7593 - val_loss: 0.5388 - val_accuracy: 0.7467\n",
      "Epoch 456/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4801 - accuracy: 0.7597 - val_loss: 0.5411 - val_accuracy: 0.7466\n",
      "Epoch 457/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4793 - accuracy: 0.7612 - val_loss: 0.5388 - val_accuracy: 0.7471\n",
      "Epoch 458/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4802 - accuracy: 0.7606 - val_loss: 0.5380 - val_accuracy: 0.7418\n",
      "Epoch 459/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4803 - accuracy: 0.7587 - val_loss: 0.5402 - val_accuracy: 0.7442\n",
      "Epoch 460/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4805 - accuracy: 0.7596 - val_loss: 0.5407 - val_accuracy: 0.7414\n",
      "Epoch 461/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4808 - accuracy: 0.7599 - val_loss: 0.5408 - val_accuracy: 0.7432\n",
      "Epoch 462/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4798 - accuracy: 0.7608 - val_loss: 0.5393 - val_accuracy: 0.7436\n",
      "Epoch 463/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4812 - accuracy: 0.7612 - val_loss: 0.5424 - val_accuracy: 0.7412\n",
      "Epoch 464/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4798 - accuracy: 0.7602 - val_loss: 0.5407 - val_accuracy: 0.7441\n",
      "Epoch 465/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4796 - accuracy: 0.7606 - val_loss: 0.5456 - val_accuracy: 0.7427\n",
      "Epoch 466/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4799 - accuracy: 0.7611 - val_loss: 0.5414 - val_accuracy: 0.7465\n",
      "Epoch 467/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4797 - accuracy: 0.7610 - val_loss: 0.5383 - val_accuracy: 0.7412\n",
      "Epoch 468/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4792 - accuracy: 0.7619 - val_loss: 0.5401 - val_accuracy: 0.7444\n",
      "Epoch 469/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4786 - accuracy: 0.7601 - val_loss: 0.5401 - val_accuracy: 0.7443\n",
      "Epoch 470/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4786 - accuracy: 0.7595 - val_loss: 0.5428 - val_accuracy: 0.7451\n",
      "Epoch 471/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4809 - accuracy: 0.7594 - val_loss: 0.5438 - val_accuracy: 0.7448\n",
      "Epoch 472/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4798 - accuracy: 0.7605 - val_loss: 0.5434 - val_accuracy: 0.7439\n",
      "Epoch 473/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4787 - accuracy: 0.7605 - val_loss: 0.5408 - val_accuracy: 0.7432\n",
      "Epoch 474/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4787 - accuracy: 0.7609 - val_loss: 0.5428 - val_accuracy: 0.7444\n",
      "Epoch 475/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4776 - accuracy: 0.7604 - val_loss: 0.5454 - val_accuracy: 0.7427\n",
      "Epoch 476/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4776 - accuracy: 0.7614 - val_loss: 0.5405 - val_accuracy: 0.7432\n",
      "Epoch 477/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4793 - accuracy: 0.7609 - val_loss: 0.5454 - val_accuracy: 0.7436\n",
      "Epoch 478/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4794 - accuracy: 0.7611 - val_loss: 0.5430 - val_accuracy: 0.7426\n",
      "Epoch 479/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4792 - accuracy: 0.7619 - val_loss: 0.5454 - val_accuracy: 0.7457\n",
      "Epoch 480/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4788 - accuracy: 0.7602 - val_loss: 0.5458 - val_accuracy: 0.7444\n",
      "Epoch 481/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4813 - accuracy: 0.7593 - val_loss: 0.5432 - val_accuracy: 0.7438\n",
      "Epoch 482/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4793 - accuracy: 0.7609 - val_loss: 0.5452 - val_accuracy: 0.7446\n",
      "Epoch 483/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4790 - accuracy: 0.7605 - val_loss: 0.5424 - val_accuracy: 0.7403\n",
      "Epoch 484/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4777 - accuracy: 0.7609 - val_loss: 0.5436 - val_accuracy: 0.7432\n",
      "Epoch 485/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4794 - accuracy: 0.7611 - val_loss: 0.5438 - val_accuracy: 0.7441\n",
      "Epoch 486/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4798 - accuracy: 0.7612 - val_loss: 0.5422 - val_accuracy: 0.7426\n",
      "Epoch 487/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4784 - accuracy: 0.7624 - val_loss: 0.5426 - val_accuracy: 0.7436\n",
      "Epoch 488/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4791 - accuracy: 0.7613 - val_loss: 0.5452 - val_accuracy: 0.7420\n",
      "Epoch 489/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4782 - accuracy: 0.7613 - val_loss: 0.5494 - val_accuracy: 0.7433\n",
      "Epoch 490/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4786 - accuracy: 0.7614 - val_loss: 0.5436 - val_accuracy: 0.7422\n",
      "Epoch 491/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4784 - accuracy: 0.7614 - val_loss: 0.5446 - val_accuracy: 0.7436\n",
      "Epoch 492/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4792 - accuracy: 0.7603 - val_loss: 0.5417 - val_accuracy: 0.7432\n",
      "Epoch 493/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4785 - accuracy: 0.7597 - val_loss: 0.5427 - val_accuracy: 0.7443\n",
      "Epoch 494/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4792 - accuracy: 0.7607 - val_loss: 0.5442 - val_accuracy: 0.7429\n",
      "Epoch 495/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4788 - accuracy: 0.7603 - val_loss: 0.5447 - val_accuracy: 0.7436\n",
      "Epoch 496/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4786 - accuracy: 0.7618 - val_loss: 0.5451 - val_accuracy: 0.7436\n",
      "Epoch 497/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4781 - accuracy: 0.7615 - val_loss: 0.5467 - val_accuracy: 0.7458\n",
      "Epoch 498/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4781 - accuracy: 0.7611 - val_loss: 0.5424 - val_accuracy: 0.7436\n",
      "Epoch 499/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4780 - accuracy: 0.7616 - val_loss: 0.5441 - val_accuracy: 0.7446\n",
      "Epoch 500/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4786 - accuracy: 0.7618 - val_loss: 0.5427 - val_accuracy: 0.7433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd64e4df550>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 500\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  feature_layer,\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dropout(.1),\n",
    "  layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_ds,\n",
    "          validation_data=val_ds,\n",
    "          epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310/310 [==============================] - 0s 951us/step - loss: 0.5596 - accuracy: 0.7365\n",
      "Accuracy 0.7364646196365356\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 31/31 [==============================] - 0s 1ms/step - loss: 0.6488 - accuracy: 0.7335\n",
    "# Accuracy 0.7335359454154968\n",
    "\n",
    "# 31/31 [==============================] - 0s 862us/step - loss: 0.5798 - accuracy: 0.6950\n",
    "# Accuracy 0.695035457611084"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model as a SavedModel.\n",
    "# !mkdir -p saved_model\n",
    "# model.save('saved_model/my_model')\n",
    "\n",
    "from tensorflow.keras.models import Sequential, save_model, load_model\n",
    "\n",
    "# filepath = './saved_model'\n",
    "# save_model(model, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morgan & Higgs: Findpath with SW 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ..((.((.(((...((((((((((.......))))).))..........)))..))).)))).(((.((((((..((((.....))))..)))))))))., -12, -54, -14.2/1: 0.36, 0.29, 0.14, 1, 90, 0.00 <-- taken\n",
      " 1, 0.36, 0.29, 0.14, 1, 0.00, 1\n",
      " ..((.(..((((..((((((((((.......))))).))..........))).))))..))).(((.((((((..((((.....))))..)))))))))., -7, -59, -14.2/1: 0.18, 0.33, 0.14, 1, 110, 0.03 \n",
      " 1, 0.18, 0.33, 0.14, 1, 0.03, 1\n",
      " ..((.((.((((..(((((((((.........)))).))..........))).)))).)))).(((.((((((..((((.....))))..)))))))))., -24, -32, -13.3/0: 0.82, 0.24, 0.14, 1, 110, 0.03 \n",
      " 1, 0.82, 0.24, 0.14, 1, 0.03, 1\n",
      " ..((.((.((((..((((((((((.......))))).))..........))).)))).))))..((.((((((..((((.....))))..)))))))).., -64, -99, -12.7/0: 0.95, 0.05, 0.14, 1, 150, 0.10 \n",
      " 1, 0.95, 0.05, 0.14, 1, 0.10, 1\n",
      " ..((.((.((((..(((((.((((.......))))..))..........))).)))).)))).(((.((((((..((((.....))))..)))))))))., -20, -36, -14.2/1: 0.64, 0.33, 0.14, 1, 210, 0.21 \n",
      " 1, 0.64, 0.33, 0.14, 1, 0.21, 1\n",
      " ..((.((.((((..((((.(((((.......)))))..)..........))).)))).)))).(((.((((((..((((.....))))..)))))))))., -19, -38, -14.2/1: 0.59, 0.24, 0.14, 1, 300, 0.36 \n",
      " 1, 0.59, 0.24, 0.14, 1, 0.36, 1\n",
      " ..((.((..(((..((((((((((.......))))).))..........))).)))..)))).(((.((((((..((((.....))))..)))))))))., -9, -57, -14.2/1: 0.23, 0.29, 0.14, 1, 310, 0.38 \n",
      " 1, 0.23, 0.29, 0.14, 1, 0.38, 1\n",
      " ..((.((.((((...(((((((((.......))))).))..........))..)))).)))).(((.((((((..((((.....))))..)))))))))., -15, -52, -14.2/1: 0.41, 0.24, 0.14, 1, 310, 0.38 \n",
      " 1, 0.41, 0.24, 0.14, 1, 0.38, 1\n",
      " ...(.((.((((..((((((((((.......))))).))..........))).)))).)))..(((.((((((..((((.....))))..)))))))))., -3, -62, -14.2/1: 0.05, 0.19, 0.14, 1, 330, 0.41 \n",
      " 1, 0.05, 0.19, 0.14, 1, 0.41, 1\n",
      " ..((.((.((((..((((((((((.......))))).))..(......)))).)))).)))).(((.((((((..((((.....))))..)))))))))., 42, 49, -14.2/1: 0.86, 0.10, 0.14, 0, 340, 0.43 \n",
      " 1, 0.86, 0.10, 0.14, 0, 0.43, 1\n",
      " ..((..(.((((..((((((((((.......))))).))..........))).)))).).)).(((.((((((..((((.....))))..)))))))))., -6, -60, -14.2/1: 0.14, 0.29, 0.14, 1, 350, 0.45 \n",
      " 1, 0.14, 0.29, 0.14, 1, 0.45, 1\n",
      " ..(..((.((((..((((((((((.......))))).))..........))).)))).)).).(((.((((((..((((.....))))..)))))))))., -4, -61, -14.2/1: 0.09, 0.19, 0.14, 1, 390, 0.52 \n",
      " 1, 0.09, 0.19, 0.14, 1, 0.52, 1\n",
      " ..((.((.((.(..((((((((((.......))))).))..........))).).)).)))).(((.((((((..((((.....))))..)))))))))., -11, -55, -14.2/1: 0.32, 0.29, 0.14, 0, 390, 0.52 \n",
      " 1, 0.32, 0.29, 0.14, 0, 0.52, 1\n",
      " ..((.((.((((..((.(((((((.......))))).))...........)).)))).)))).(((.((((((..((((.....))))..)))))))))., -17, -50, -14.2/1: 0.50, 0.14, 0.14, 1, 390, 0.52 \n",
      " 1, 0.50, 0.14, 0.14, 1, 0.52, 1\n",
      " ..((.((.((((..(((.((((((.......))))).)...........))).)))).)))).(((.((((((..((((.....))))..)))))))))., -18, -39, -14.2/1: 0.55, 0.19, 0.14, 1, 390, 0.52 \n",
      " 1, 0.55, 0.19, 0.14, 1, 0.52, 1\n",
      " ..((.((.((((..((((((((.(.......).))).))..........))).)))).)))).(((.((((((..((((.....))))..)))))))))., -23, -33, -13.3/0: 0.77, 0.24, 0.14, 0, 420, 0.57 \n",
      " 1, 0.77, 0.24, 0.14, 0, 0.57, 1\n",
      " ..((.((.((((..((((((((((.......))))).))...(....).))).)))).)))).(((.((((((..((((.....))))..)))))))))., 43, 48, -14.2/1: 0.91, 0.10, 0.14, 0, 430, 0.59 \n",
      " 1, 0.91, 0.10, 0.14, 0, 0.59, 1\n",
      " ..((.((.((((..(.((((((((.......))))).))..........).).)))).)))).(((.((((((..((((.....))))..)))))))))., -16, -51, -14.2/1: 0.45, 0.19, 0.14, 0, 500, 0.71 \n",
      " 1, 0.45, 0.19, 0.14, 0, 0.71, 1\n",
      " ..((.((.((((..((((((.(((.......))).).))..........))).)))).)))).(((.((((((..((((.....))))..)))))))))., -21, -35, -13.3/0: 0.68, 0.33, 0.14, 0, 600, 0.88 \n",
      " 1, 0.68, 0.33, 0.14, 0, 0.88, 1\n",
      " ..((.((.((((..(((((((.((.......)).)).))..........))).)))).)))).(((.((((((..((((.....))))..)))))))))., -22, -34, -13.3/0: 0.73, 0.29, 0.14, 0, 660, 0.98 \n",
      " 1, 0.73, 0.29, 0.14, 0, 0.98, 1\n",
      " ..((.((.(.((..((((((((((.......))))).))..........))).)).).)))).(((.((((((..((((.....))))..)))))))))., -10, -56, -14.2/1: 0.27, 0.29, 0.14, 0, 670, 1.00 \n",
      " 1, 0.27, 0.29, 0.14, 0, 1.00, 1\n"
     ]
    }
   ],
   "source": [
    "input_file = \"dataset_100.csv\"\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "sequence, s1, s2 = df.loc[0]\n",
    "\n",
    "\n",
    "search_width_multiplier = 4\n",
    "fp = findpath.findpath_single(sequence, s1, s2, search_width_multiplier=search_width_multiplier, mp=True)\n",
    "result = fp.get_en()/100.0\n",
    "path = fp.get_path()\n",
    "\n",
    "s = s1\n",
    "pt2 = list(RNA.ptable(s2))\n",
    "fc = RNA.fold_compound(sequence)\n",
    "\n",
    "\n",
    "def find_moves(s_ptable, t_ptable):\n",
    "    \"\"\"\n",
    "    generator function, yields possible structures 1 move away\n",
    "    from the original structure by finding fitting i and j with\n",
    "    RNA pair and loop tables\n",
    "    s_ptable: current ptable\n",
    "    t_ptable: s2 end ptable\n",
    "    \"\"\"\n",
    "    # loop table\n",
    "    ls = RNA.loopidx_from_ptable(s_ptable)\n",
    "    for i in range(len(s_ptable)):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if s_ptable[i] == 0 and t_ptable[i] > i:\n",
    "            j = t_ptable[i]\n",
    "            # found j has to be empty and currently on the same loop as i\n",
    "            if s_ptable[j] == 0 and ls[i] == ls[j]:\n",
    "                yield i, j\n",
    "        # test for bp removal: i has to be paired with a different j in s2\n",
    "        j = s_ptable[i]\n",
    "        # dont remove things which are present in s2\n",
    "        if s_ptable[i] > i and s_ptable[i] != s_ptable[j] and\\\n",
    "                s_ptable[i] != t_ptable[i] and s_ptable[j] != t_ptable[j]:\n",
    "            yield -i, -j\n",
    "\n",
    "\n",
    "def fp_call(sequence, s1, s2, search_width_multiplier = 20):    \n",
    "    fp = findpath.findpath_single(sequence, s1, s2, search_width_multiplier=search_width_multiplier, mp=True)\n",
    "    result = fp.get_en()/100.0\n",
    "    path = fp.get_path()\n",
    "    # return result, path\n",
    "    return result, path\n",
    "\n",
    "\n",
    "def ij_distance(last_move, this_move, ij_moves):\n",
    "    # how far is the last move away from the current move.\n",
    "    # it is likely, that the next move is close to the last one\n",
    "    # there are better distance metrices probably...\n",
    "\n",
    "    # ij move list is supposed to be sorted, find indices\n",
    "    \n",
    "    ijmoves = ij_moves + [last_move]\n",
    "    ijmoves.sort(key=lambda x: (x[0], x[1]))\n",
    "\n",
    "    pos_old = ijmoves.index(last_move)\n",
    "    pos_new = ijmoves.index(this_move)\n",
    "\n",
    "    distance = abs(pos_old-pos_new)/len(ijmoves)\n",
    "    \n",
    "    \n",
    "    # moves left in vicinity out of total moves\n",
    "    \n",
    "    thisi, thisj = this_move\n",
    "    lasti, lastj = last_move\n",
    "    thisclose = 0\n",
    "    lastclose = 0\n",
    "\n",
    "    for i, j in ij_moves:\n",
    "        if (abs(i-thisi) < 5) and (abs(j-thisj) < 5):\n",
    "            thisclose += 1\n",
    "        if (abs(i-lasti) < 5) and (abs(j-lastj) < 5):\n",
    "            lastclose += 1\n",
    "    \n",
    "\n",
    "    thisclose /= len(ij_moves)\n",
    "    lastclose /= len(ij_moves)\n",
    "\n",
    "    # print (\"thisclose\", thisclose, lastclose, distance)\n",
    "    return distance, thisclose, lastclose\n",
    "\n",
    "    print (distance)\n",
    "\n",
    "# sample call\n",
    "ij_moves = [(3, 62), (4, 61), (6, 60), (7, 59), (9, 57), (10, 56), (11, 55), (12, 54), (15, 52), (16, 51), (17, 50), (18, 39), (19, 38), (20, 36), (21, 35), (22, 34), (23, 33), (24, 32), (42, 49), (43, 48), (64, 99)]\n",
    "last_move = (2, 63)\n",
    "this_move = (6, 60)\n",
    "ij_distance(last_move, this_move, ij_moves)\n",
    "\n",
    "# \n",
    "\n",
    "def config_distance(pt, move):\n",
    "    \"\"\"\n",
    "    are we extending / removing the outside/inside layer of a loop or adding something in the middle?\n",
    "    \"\"\"\n",
    "    i = move[0]\n",
    "    j = move[1]\n",
    "    points = 0\n",
    "\n",
    "    # if we're extending from outside to inside, the position i+1 and j-1 should be ideally unpaired\n",
    "    # inside to outside: i-1 and j+1 should be ideally unpaired\n",
    "\n",
    "    if i>0:\n",
    "        # print (\"add\") \n",
    "        # outside/inside paired?        \n",
    "        if j+1 < pt[0] and i-1 > 0: # outside - boundary check\n",
    "            if pt[i-1] == j+1:\n",
    "                points += 1\n",
    "        if pt[i+1] == j-1:\n",
    "            points += 1\n",
    "    if i<0:\n",
    "        # print (\"del\")\n",
    "        i, j = -i, -j\n",
    "        # outside/inside paired?\n",
    "        if j+1 < pt[0] and i-1 > 0: # outside - boundary check\n",
    "            if pt[i-1] == j+1:\n",
    "                points += 1\n",
    "        if pt[i+1] == j-1:\n",
    "            points += 1\n",
    "        elif pt[i+1] == 0 and pt[j-1] == 0:\n",
    "            pass\n",
    "\n",
    "    if points == 2:\n",
    "        points = 0\n",
    "    return points\n",
    "\n",
    "\n",
    "s = s1\n",
    "lasts = s\n",
    "lasti = None\n",
    "lastj = None\n",
    "\n",
    "for e, (a,b, en) in enumerate(path):\n",
    "    if (a,b) == (0,0):\n",
    "        continue  \n",
    "\n",
    "    # check where we can go, compare with our best move. \n",
    "    pt = list(RNA.ptable(s))\n",
    "\n",
    "    # check available moves, save them, sort them    \n",
    "    avail_moves = []\n",
    "    ij_moves = []\n",
    "    found_pos = None\n",
    "\n",
    "    for pos, (i,j) in enumerate(find_moves(pt, pt2)):    \n",
    "        next_en = fc.eval_move_pt(pt, i, j)\n",
    "        # mark where we found our move\n",
    "        found = (i,j) == (a,b)\n",
    "        avail_moves.append((i, j, next_en, found))\n",
    "        ij_moves.append((abs(i),abs(j)))\n",
    "\n",
    "    # sort moves independent of delete insert moves\n",
    "\n",
    "    ij_moves.sort(key=lambda x: (x[0], x[1]))\n",
    "\n",
    "\n",
    "    avail_moves.sort(key=lambda x: x[2])\n",
    "    found_list = [x[3] for x in avail_moves]\n",
    "    en_list = np.array([[x[2] for x in avail_moves]])\n",
    "    en_list_scaled = min_max_scaler.fit_transform(en_list.T).T[0]\n",
    "    \n",
    "\n",
    "    # find where our move is after sorting\n",
    "    found_pos = found_list.index(True)\n",
    "    rel_pos = found_pos * 1.0 / len(found_list)\n",
    "\n",
    "    # print (e, a,b, 'found at pos:', found_pos, 'of', len(avail_moves), ':',  1-rel_pos)\n",
    "    # print (avail_moves, a, b)\n",
    "\n",
    "    # if s != lasts:\n",
    "        # print (\"---\") \n",
    "        # print (s)\n",
    "        # print (\"---\") \n",
    "\n",
    "    # for every move we take we have to run a new findpath, see if this move will yield the ideal result\n",
    "    \n",
    "    for pos, (i,j, en, found) in enumerate(avail_moves):\n",
    "        if i > 0:\n",
    "            snew = s[:i-1] + \"(\" + s[i:j-1] + \")\" + s[j:]\n",
    "        if i < 0:\n",
    "            snew = s[:-i-1] + \".\" + s[-i:-j-1] + \".\" + s[-j:]\n",
    "        ptnew = list(RNA.ptable(snew))\n",
    "\n",
    "        result_new, path = fp_call(sequence, snew, s2)\n",
    "\n",
    "        if result_new <= result:\n",
    "            pos_result = 1\n",
    "        else:\n",
    "            pos_result = 0\n",
    "\n",
    "        if found: found = \"<-- taken\"\n",
    "        else: found = \"\"\n",
    "\n",
    "        this_move = (abs(i), abs(j))\n",
    "        last_move = (lasti, lastj)\n",
    "\n",
    "        if lasti:\n",
    "            # print (this_move, last_move, ij_moves)\n",
    "            ijd, thisclose, lastclose = ij_distance(last_move, this_move, ij_moves)\n",
    "        else:\n",
    "            ijd, thisclose, lastclose = 0, 0, 0\n",
    "\n",
    "        cd = config_distance(pt, this_move)\n",
    "\n",
    "        if lasti:\n",
    "            print (f' {snew}, {i}, {j}, {result_new}/{pos_result}: {ijd:2.2f}, {thisclose:2.2f}, {lastclose:2.2f}, {cd}, {en}, {en_list_scaled[pos]:2.2f} {found}')\n",
    "            print (f' {1}, {ijd:2.2f}, {thisclose:2.2f}, {lastclose:2.2f}, {cd}, {en_list_scaled[pos]:2.2f}, 1')\n",
    "\n",
    "    # if e==63:\n",
    "    #     print (avail_moves)\n",
    "\n",
    "    # update s for the next iteration\n",
    "    \n",
    "    lasts = s\n",
    "    lasti = abs(a)\n",
    "    lastj = abs(b)\n",
    "    if a > 0:\n",
    "        s = s[:a-1] + \"(\" + s[a:b-1] + \")\" + s[b:]\n",
    "    if a < 0:\n",
    "        s = s[:-a-1] + \".\" + s[-a:-b-1] + \".\" + s[-b:]\n",
    "\n",
    "    if e>1: \n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39424</th>\n",
       "      <td>39424</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0</td>\n",
       "      <td>0.691358</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44489</th>\n",
       "      <td>44489</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>1</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30368</th>\n",
       "      <td>30368</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38959</th>\n",
       "      <td>38959</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24553</th>\n",
       "      <td>24553</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>1</td>\n",
       "      <td>0.705128</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24716</th>\n",
       "      <td>24716</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.937107</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12419</th>\n",
       "      <td>12419</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4130</th>\n",
       "      <td>4130</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.716981</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12647</th>\n",
       "      <td>12647</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>1</td>\n",
       "      <td>0.547270</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32613</th>\n",
       "      <td>32613</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.856459</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34891</th>\n",
       "      <td>34891</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.092593</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43918</th>\n",
       "      <td>43918</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.623717</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40498</th>\n",
       "      <td>40498</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17067</th>\n",
       "      <td>17067</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0</td>\n",
       "      <td>0.679487</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39223</th>\n",
       "      <td>39223</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39449</th>\n",
       "      <td>39449</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19877</th>\n",
       "      <td>19877</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47225</th>\n",
       "      <td>47225</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7854</th>\n",
       "      <td>7854</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36518</th>\n",
       "      <td>36518</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30058</th>\n",
       "      <td>30058</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23629</th>\n",
       "      <td>23629</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25674</th>\n",
       "      <td>25674</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28098</th>\n",
       "      <td>28098</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>1</td>\n",
       "      <td>0.701493</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39013</th>\n",
       "      <td>39013</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0</td>\n",
       "      <td>0.972727</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0         4         5         6  7         8  target\n",
       "39424       39424  0.227273  0.380952  0.095238  0  0.691358       0\n",
       "44489       44489  0.166667  0.130435  0.086957  1  0.750000       0\n",
       "30368       30368  0.769231  0.160000  0.040000  0  0.485294       0\n",
       "38959       38959  0.100000  0.210526  0.157895  0  0.846154       0\n",
       "24553       24553  0.333333  0.285714  0.071429  1  0.705128       0\n",
       "24716       24716  0.230769  0.250000  0.000000  0  0.937107       0\n",
       "12419       12419  0.500000  0.714286  0.142857  0  0.758621       0\n",
       "4130         4130  0.153846  0.250000  0.250000  1  0.716981       0\n",
       "12647       12647  0.583333  0.454545  0.090909  1  0.547270       0\n",
       "32613       32613  0.533333  0.428571  0.000000  0  0.856459       0\n",
       "34891       34891  0.250000  0.222222  0.000000  1  0.092593       0\n",
       "43918       43918  0.700000  0.555556  0.000000  0  0.623717       1\n",
       "40498       40498  0.192308  0.320000  0.000000  0  0.950000       0\n",
       "17067       17067  0.500000  0.555556  0.111111  0  0.679487       0\n",
       "39223       39223  0.363636  0.500000  0.500000  0  0.961538       0\n",
       "39449       39449  0.565217  0.227273  0.181818  1  0.666667       0\n",
       "19877       19877  0.791667  0.130435  0.043478  0  0.571429       0\n",
       "47225       47225  0.714286  0.500000  0.000000  0  1.000000       0\n",
       "7854         7854  0.346154  0.240000  0.160000  0  0.944444       0\n",
       "36518       36518  0.272727  0.100000  0.200000  1  0.000000       1\n",
       "30058       30058  0.071429  0.230769  0.153846  0  0.000000       1\n",
       "23629       23629  0.100000  0.111111  0.444444  0  1.000000       0\n",
       "25674       25674  0.107143  0.185185  0.111111  1  0.000000       1\n",
       "28098       28098  0.473684  0.388889  0.111111  1  0.701493       0\n",
       "39013       39013  0.055556  0.176471  0.058824  0  0.972727       0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A few random samples\n",
    "\n",
    "\n",
    "# Generate predictions for samples\n",
    "predictions = model.predict(test_ds)\n",
    "\n",
    "predictions\n",
    "sample_test\n",
    "# test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0     4     5     6  7     8  target\n",
      "39424           1  0.23  0.29  0.14  1  0.38       1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/.local/lib/python3.8/site-packages/pandas/core/indexing.py:1699: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, v, pi)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.13753884]], dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe['target']\n",
    "\n",
    "\n",
    "sample_test = test.iloc[0:1]\n",
    "# print (sample_test)\n",
    "# sample_test.loc[0]   = 0, 0, 0.5, 0,0,1,1 # last column = true value, does nothing\n",
    "\n",
    "sample_test.loc[39424] = 12, 0.36, 0.29, 0.14, 1, 0.00, 1 # should be 1\n",
    "sample_test.loc[39424] =  1, 0.82, 0.24, 0.14, 1, 0.03, 1\n",
    "sample_test.loc[39424] =  1, 0.23, 0.29, 0.14, 1, 0.38, 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (sample_test)\n",
    "# sample_test.loc[0] = 12647,\t0.583333,\t0.454545,\t0.090909,\t1,\t0.547270,\t0\n",
    "sample_test_ds = df_to_dataset(sample_test, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "predictions = model.predict(sample_test_ds)\n",
    "predictions"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
