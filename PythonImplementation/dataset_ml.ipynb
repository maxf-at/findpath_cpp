{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import RNA\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import SVG, display\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "import difflib\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from helper import print_moves\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('../')\n",
    "from pretty_print_path import print_moves\n",
    "import findpath_librna\n",
    "import findpath\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-11-03 10:25:03.718101: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-03 10:25:03.718135: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "input_file = \"dataset_102_train.csv\"\n",
    "\n",
    "dataframe = pd.read_csv(input_file)\n",
    "\n",
    "dataframe['target'] = np.where(dataframe[\"3\"]==1, 1, 0)\n",
    "\n",
    "dataframe = dataframe.drop(labels=\"0\", axis=1)\n",
    "dataframe = dataframe.drop(labels=\"1\", axis=1)\n",
    "dataframe = dataframe.drop(labels=\"2\", axis=1)\n",
    "dataframe = dataframe.drop(labels=\"9\", axis=1)\n",
    "dataframe = dataframe.drop(labels=\"3\", axis=1)\n",
    "dataframe"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       Unnamed: 0         4         5         6  7         8  target\n",
       "0               0  0.363636  0.285714  0.142857  1  0.000000       1\n",
       "1               1  0.181818  0.333333  0.142857  1  0.034483       1\n",
       "2               2  0.818182  0.238095  0.142857  1  0.034483       0\n",
       "3               3  0.954545  0.047619  0.142857  1  0.103448       0\n",
       "4               4  0.636364  0.333333  0.142857  1  0.206897       1\n",
       "...           ...       ...       ...       ... ..       ...     ...\n",
       "49492       49492  0.666667  1.000000  0.000000  0  1.000000       1\n",
       "49493       49493  0.200000  1.000000  1.000000  1  0.000000       1\n",
       "49494       49494  0.800000  1.000000  1.000000  0  0.760000       1\n",
       "49495       49495  0.400000  1.000000  1.000000  0  0.920000       1\n",
       "49496       49496  0.600000  1.000000  1.000000  0  1.000000       1\n",
       "\n",
       "[49497 rows x 7 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49492</th>\n",
       "      <td>49492</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49493</th>\n",
       "      <td>49493</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49494</th>\n",
       "      <td>49494</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49495</th>\n",
       "      <td>49495</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49496</th>\n",
       "      <td>49496</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49497 rows Ã— 7 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "train, test = train_test_split(dataframe, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.2)\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "31677 train examples\n",
      "7920 validation examples\n",
      "9900 test examples\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop('target')\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds\n",
    "\n",
    "batch_size = 5 # A small batch sized is used for demonstration purposes\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "## Understand the input pipeline\n",
    "# Now that we have created the input pipeline, let's call it to see the format of the data it returns. We have used a small batch size to keep the output readable.\n",
    "\n",
    "for feature_batch, label_batch in train_ds.take(1):\n",
    "  print('Every feature:', list(feature_batch.keys()))\n",
    "  print('A batch of ages:', feature_batch['4'])\n",
    "  print('A batch of targets:', label_batch )\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Every feature: ['Unnamed: 0', '4', '5', '6', '7', '8']\n",
      "A batch of ages: tf.Tensor([0.63636364 0.47368421 0.11764706 0.41666667 0.15384615], shape=(5,), dtype=float64)\n",
      "A batch of targets: tf.Tensor([0 1 0 0 1], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# We will use this batch to demonstrate several types of feature columns\n",
    "example_batch = next(iter(train_ds))[0]\n",
    "\n",
    "# A utility method to create a feature column\n",
    "# and to transform a batch of data\n",
    "def demo(feature_column):\n",
    "  feature_layer = layers.DenseFeatures(feature_column)\n",
    "  print(feature_layer(example_batch).numpy())\n",
    "\n",
    "# numeric columns\n",
    "photo_count = feature_column.numeric_column('4')\n",
    "demo(photo_count)\n",
    "\n",
    "# column 7 is a catigorical column\n",
    "animal_type = feature_column.categorical_column_with_vocabulary_list(\n",
    "      '7', [0, 1])\n",
    "animal_type_one_hot = feature_column.indicator_column(animal_type)\n",
    "demo(animal_type_one_hot)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.53333336]\n",
      " [0.07692308]\n",
      " [0.72727275]\n",
      " [0.84615386]\n",
      " [0.8888889 ]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Choose which columns to use\n",
    "We have seen how to use several types of feature columns. Now we will use them to train a model. The goal of this tutorial is to show you the complete code (e.g. mechanics) needed to work with feature columns. We have selected a few columns to train our model below arbitrarily.\n",
    "\n",
    "Key point: If your aim is to build an accurate model, try a larger dataset of your own, and think carefully about which features are the most meaningful to include, and how they should be represented."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "feature_columns = []\n",
    "\n",
    "# numeric cols\n",
    "for header in ['4', '5', '6', '8']:\n",
    "  feature_columns.append(feature_column.numeric_column(header))\n",
    "\n",
    "# categoriacal indicator_columns\n",
    "indicator_column_names = ['7']\n",
    "for col_name in indicator_column_names:\n",
    "  categorical_column = feature_column.categorical_column_with_vocabulary_list(\n",
    "      col_name, dataframe[col_name].unique())\n",
    "  indicator_column = feature_column.indicator_column(categorical_column)\n",
    "  feature_columns.append(indicator_column)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# only use lowest energy as indicator\n",
    "\n",
    "feature_columns = []\n",
    "\n",
    "# numeric cols\n",
    "for header in ['8']:\n",
    "  feature_columns.append(feature_column.numeric_column(header))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "feature_columns"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[NumericColumn(key='4', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='5', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='6', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='8', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='7', vocabulary_list=(1, 0), dtype=tf.int64, default_value=-1, num_oov_buckets=0))]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a feature layer\n",
    "Now that we have defined our feature columns, we will use a [DenseFeatures](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/DenseFeatures) layer to input them to our Keras model.\n",
    "\n",
    "Earlier, we used a small batch size to demonstrate how feature columns worked. We create a new input pipeline with a larger batch size."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "\n",
    "batch_size = 32\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "epochs = 500\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  feature_layer,\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dropout(.1),\n",
    "  layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_ds,\n",
    "          validation_data=val_ds,\n",
    "          epochs=epochs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/500\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'Unnamed: 0': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=int64>, '4': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float64>, '5': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=float64>, '6': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=float64>, '7': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=int64>, '8': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=float64>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'Unnamed: 0': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=int64>, '4': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float64>, '5': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=float64>, '6': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=float64>, '7': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=int64>, '8': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=float64>}\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-11-03 10:25:51.381933: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "953/990 [===========================>..] - ETA: 0s - loss: 0.5470 - accuracy: 0.7251WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'Unnamed: 0': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=int64>, '4': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float64>, '5': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=float64>, '6': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=float64>, '7': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=int64>, '8': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=float64>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "990/990 [==============================] - 2s 1ms/step - loss: 0.5462 - accuracy: 0.7261 - val_loss: 0.5289 - val_accuracy: 0.7299\n",
      "Epoch 2/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5355 - accuracy: 0.7336 - val_loss: 0.5223 - val_accuracy: 0.7359\n",
      "Epoch 3/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5330 - accuracy: 0.7332 - val_loss: 0.5217 - val_accuracy: 0.7347\n",
      "Epoch 4/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5305 - accuracy: 0.7349 - val_loss: 0.5201 - val_accuracy: 0.7341\n",
      "Epoch 5/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5297 - accuracy: 0.7345 - val_loss: 0.5201 - val_accuracy: 0.7400\n",
      "Epoch 6/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5282 - accuracy: 0.7343 - val_loss: 0.5175 - val_accuracy: 0.7348\n",
      "Epoch 7/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5270 - accuracy: 0.7344 - val_loss: 0.5273 - val_accuracy: 0.7384\n",
      "Epoch 8/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5269 - accuracy: 0.7345 - val_loss: 0.5188 - val_accuracy: 0.7403\n",
      "Epoch 9/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5263 - accuracy: 0.7342 - val_loss: 0.5174 - val_accuracy: 0.7333\n",
      "Epoch 10/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5255 - accuracy: 0.7339 - val_loss: 0.5276 - val_accuracy: 0.7264\n",
      "Epoch 11/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5248 - accuracy: 0.7356 - val_loss: 0.5170 - val_accuracy: 0.7307\n",
      "Epoch 12/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5249 - accuracy: 0.7342 - val_loss: 0.5193 - val_accuracy: 0.7330\n",
      "Epoch 13/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5245 - accuracy: 0.7345 - val_loss: 0.5177 - val_accuracy: 0.7398\n",
      "Epoch 14/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5236 - accuracy: 0.7349 - val_loss: 0.5145 - val_accuracy: 0.7426\n",
      "Epoch 15/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5238 - accuracy: 0.7358 - val_loss: 0.5193 - val_accuracy: 0.7324\n",
      "Epoch 16/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5232 - accuracy: 0.7356 - val_loss: 0.5174 - val_accuracy: 0.7322\n",
      "Epoch 17/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5237 - accuracy: 0.7344 - val_loss: 0.5144 - val_accuracy: 0.7366\n",
      "Epoch 18/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5227 - accuracy: 0.7355 - val_loss: 0.5153 - val_accuracy: 0.7418\n",
      "Epoch 19/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5220 - accuracy: 0.7369 - val_loss: 0.5175 - val_accuracy: 0.7298\n",
      "Epoch 20/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5223 - accuracy: 0.7358 - val_loss: 0.5167 - val_accuracy: 0.7452\n",
      "Epoch 21/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5218 - accuracy: 0.7363 - val_loss: 0.5152 - val_accuracy: 0.7314\n",
      "Epoch 22/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5215 - accuracy: 0.7362 - val_loss: 0.5149 - val_accuracy: 0.7436\n",
      "Epoch 23/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5214 - accuracy: 0.7365 - val_loss: 0.5144 - val_accuracy: 0.7395\n",
      "Epoch 24/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5211 - accuracy: 0.7363 - val_loss: 0.5187 - val_accuracy: 0.7330\n",
      "Epoch 25/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5208 - accuracy: 0.7368 - val_loss: 0.5147 - val_accuracy: 0.7396\n",
      "Epoch 26/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5208 - accuracy: 0.7365 - val_loss: 0.5139 - val_accuracy: 0.7398\n",
      "Epoch 27/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5204 - accuracy: 0.7379 - val_loss: 0.5155 - val_accuracy: 0.7348\n",
      "Epoch 28/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5204 - accuracy: 0.7368 - val_loss: 0.5152 - val_accuracy: 0.7338\n",
      "Epoch 29/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5196 - accuracy: 0.7380 - val_loss: 0.5168 - val_accuracy: 0.7465\n",
      "Epoch 30/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5201 - accuracy: 0.7371 - val_loss: 0.5142 - val_accuracy: 0.7354\n",
      "Epoch 31/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5196 - accuracy: 0.7372 - val_loss: 0.5145 - val_accuracy: 0.7381\n",
      "Epoch 32/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5191 - accuracy: 0.7383 - val_loss: 0.5152 - val_accuracy: 0.7376\n",
      "Epoch 33/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5195 - accuracy: 0.7390 - val_loss: 0.5146 - val_accuracy: 0.7357\n",
      "Epoch 34/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5190 - accuracy: 0.7368 - val_loss: 0.5143 - val_accuracy: 0.7410\n",
      "Epoch 35/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5184 - accuracy: 0.7379 - val_loss: 0.5142 - val_accuracy: 0.7408\n",
      "Epoch 36/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5185 - accuracy: 0.7379 - val_loss: 0.5157 - val_accuracy: 0.7352\n",
      "Epoch 37/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5188 - accuracy: 0.7382 - val_loss: 0.5156 - val_accuracy: 0.7367\n",
      "Epoch 38/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5178 - accuracy: 0.7388 - val_loss: 0.5149 - val_accuracy: 0.7364\n",
      "Epoch 39/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5183 - accuracy: 0.7380 - val_loss: 0.5166 - val_accuracy: 0.7404\n",
      "Epoch 40/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5181 - accuracy: 0.7386 - val_loss: 0.5141 - val_accuracy: 0.7337\n",
      "Epoch 41/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5181 - accuracy: 0.7392 - val_loss: 0.5142 - val_accuracy: 0.7370\n",
      "Epoch 42/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5173 - accuracy: 0.7397 - val_loss: 0.5138 - val_accuracy: 0.7365\n",
      "Epoch 43/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5173 - accuracy: 0.7397 - val_loss: 0.5135 - val_accuracy: 0.7412\n",
      "Epoch 44/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5176 - accuracy: 0.7394 - val_loss: 0.5168 - val_accuracy: 0.7407\n",
      "Epoch 45/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5170 - accuracy: 0.7393 - val_loss: 0.5131 - val_accuracy: 0.7389\n",
      "Epoch 46/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5164 - accuracy: 0.7394 - val_loss: 0.5132 - val_accuracy: 0.7357\n",
      "Epoch 47/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5168 - accuracy: 0.7392 - val_loss: 0.5126 - val_accuracy: 0.7381\n",
      "Epoch 48/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5167 - accuracy: 0.7400 - val_loss: 0.5119 - val_accuracy: 0.7427\n",
      "Epoch 49/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5161 - accuracy: 0.7404 - val_loss: 0.5147 - val_accuracy: 0.7355\n",
      "Epoch 50/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5165 - accuracy: 0.7387 - val_loss: 0.5118 - val_accuracy: 0.7426\n",
      "Epoch 51/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5155 - accuracy: 0.7398 - val_loss: 0.5137 - val_accuracy: 0.7431\n",
      "Epoch 52/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5159 - accuracy: 0.7399 - val_loss: 0.5132 - val_accuracy: 0.7389\n",
      "Epoch 53/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5154 - accuracy: 0.7409 - val_loss: 0.5143 - val_accuracy: 0.7442\n",
      "Epoch 54/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5150 - accuracy: 0.7420 - val_loss: 0.5136 - val_accuracy: 0.7376\n",
      "Epoch 55/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5155 - accuracy: 0.7405 - val_loss: 0.5138 - val_accuracy: 0.7359\n",
      "Epoch 56/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5150 - accuracy: 0.7406 - val_loss: 0.5121 - val_accuracy: 0.7371\n",
      "Epoch 57/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5153 - accuracy: 0.7407 - val_loss: 0.5126 - val_accuracy: 0.7361\n",
      "Epoch 58/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5146 - accuracy: 0.7401 - val_loss: 0.5141 - val_accuracy: 0.7380\n",
      "Epoch 59/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5146 - accuracy: 0.7402 - val_loss: 0.5134 - val_accuracy: 0.7359\n",
      "Epoch 60/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5143 - accuracy: 0.7424 - val_loss: 0.5137 - val_accuracy: 0.7365\n",
      "Epoch 61/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5138 - accuracy: 0.7409 - val_loss: 0.5122 - val_accuracy: 0.7404\n",
      "Epoch 62/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5137 - accuracy: 0.7409 - val_loss: 0.5128 - val_accuracy: 0.7431\n",
      "Epoch 63/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5146 - accuracy: 0.7410 - val_loss: 0.5143 - val_accuracy: 0.7378\n",
      "Epoch 64/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5139 - accuracy: 0.7409 - val_loss: 0.5143 - val_accuracy: 0.7415\n",
      "Epoch 65/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5134 - accuracy: 0.7423 - val_loss: 0.5117 - val_accuracy: 0.7415\n",
      "Epoch 66/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5136 - accuracy: 0.7415 - val_loss: 0.5131 - val_accuracy: 0.7347\n",
      "Epoch 67/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5134 - accuracy: 0.7411 - val_loss: 0.5131 - val_accuracy: 0.7405\n",
      "Epoch 68/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5136 - accuracy: 0.7404 - val_loss: 0.5136 - val_accuracy: 0.7451\n",
      "Epoch 69/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5134 - accuracy: 0.7406 - val_loss: 0.5124 - val_accuracy: 0.7431\n",
      "Epoch 70/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5131 - accuracy: 0.7428 - val_loss: 0.5143 - val_accuracy: 0.7407\n",
      "Epoch 71/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5126 - accuracy: 0.7425 - val_loss: 0.5136 - val_accuracy: 0.7399\n",
      "Epoch 72/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5121 - accuracy: 0.7418 - val_loss: 0.5137 - val_accuracy: 0.7393\n",
      "Epoch 73/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5126 - accuracy: 0.7420 - val_loss: 0.5131 - val_accuracy: 0.7391\n",
      "Epoch 74/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5125 - accuracy: 0.7415 - val_loss: 0.5134 - val_accuracy: 0.7378\n",
      "Epoch 75/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5121 - accuracy: 0.7415 - val_loss: 0.5119 - val_accuracy: 0.7419\n",
      "Epoch 76/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5121 - accuracy: 0.7419 - val_loss: 0.5125 - val_accuracy: 0.7423\n",
      "Epoch 77/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5117 - accuracy: 0.7416 - val_loss: 0.5139 - val_accuracy: 0.7449\n",
      "Epoch 78/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5117 - accuracy: 0.7424 - val_loss: 0.5117 - val_accuracy: 0.7434\n",
      "Epoch 79/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5123 - accuracy: 0.7413 - val_loss: 0.5120 - val_accuracy: 0.7423\n",
      "Epoch 80/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5121 - accuracy: 0.7426 - val_loss: 0.5120 - val_accuracy: 0.7429\n",
      "Epoch 81/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5118 - accuracy: 0.7418 - val_loss: 0.5154 - val_accuracy: 0.7402\n",
      "Epoch 82/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5108 - accuracy: 0.7424 - val_loss: 0.5127 - val_accuracy: 0.7391\n",
      "Epoch 83/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5111 - accuracy: 0.7418 - val_loss: 0.5126 - val_accuracy: 0.7436\n",
      "Epoch 84/500\n",
      "990/990 [==============================] - 2s 1ms/step - loss: 0.5106 - accuracy: 0.7421 - val_loss: 0.5132 - val_accuracy: 0.7453\n",
      "Epoch 85/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5112 - accuracy: 0.7415 - val_loss: 0.5138 - val_accuracy: 0.7391\n",
      "Epoch 86/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5109 - accuracy: 0.7422 - val_loss: 0.5113 - val_accuracy: 0.7391\n",
      "Epoch 87/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5103 - accuracy: 0.7413 - val_loss: 0.5133 - val_accuracy: 0.7398\n",
      "Epoch 88/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5110 - accuracy: 0.7428 - val_loss: 0.5144 - val_accuracy: 0.7410\n",
      "Epoch 89/500\n",
      "990/990 [==============================] - 2s 1ms/step - loss: 0.5102 - accuracy: 0.7428 - val_loss: 0.5130 - val_accuracy: 0.7399\n",
      "Epoch 90/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5101 - accuracy: 0.7422 - val_loss: 0.5134 - val_accuracy: 0.7393\n",
      "Epoch 91/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5104 - accuracy: 0.7430 - val_loss: 0.5123 - val_accuracy: 0.7398\n",
      "Epoch 92/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5096 - accuracy: 0.7417 - val_loss: 0.5128 - val_accuracy: 0.7398\n",
      "Epoch 93/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5100 - accuracy: 0.7435 - val_loss: 0.5131 - val_accuracy: 0.7414\n",
      "Epoch 94/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5092 - accuracy: 0.7430 - val_loss: 0.5134 - val_accuracy: 0.7431\n",
      "Epoch 95/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5097 - accuracy: 0.7439 - val_loss: 0.5134 - val_accuracy: 0.7415\n",
      "Epoch 96/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5101 - accuracy: 0.7443 - val_loss: 0.5147 - val_accuracy: 0.7396\n",
      "Epoch 97/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5091 - accuracy: 0.7436 - val_loss: 0.5149 - val_accuracy: 0.7376\n",
      "Epoch 98/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5095 - accuracy: 0.7449 - val_loss: 0.5142 - val_accuracy: 0.7388\n",
      "Epoch 99/500\n",
      "990/990 [==============================] - 2s 1ms/step - loss: 0.5087 - accuracy: 0.7451 - val_loss: 0.5131 - val_accuracy: 0.7402\n",
      "Epoch 100/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5091 - accuracy: 0.7435 - val_loss: 0.5138 - val_accuracy: 0.7396\n",
      "Epoch 101/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5088 - accuracy: 0.7440 - val_loss: 0.5159 - val_accuracy: 0.7364\n",
      "Epoch 102/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5083 - accuracy: 0.7436 - val_loss: 0.5138 - val_accuracy: 0.7403\n",
      "Epoch 103/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5083 - accuracy: 0.7436 - val_loss: 0.5129 - val_accuracy: 0.7413\n",
      "Epoch 104/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5077 - accuracy: 0.7443 - val_loss: 0.5144 - val_accuracy: 0.7451\n",
      "Epoch 105/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5088 - accuracy: 0.7445 - val_loss: 0.5139 - val_accuracy: 0.7385\n",
      "Epoch 106/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5077 - accuracy: 0.7439 - val_loss: 0.5142 - val_accuracy: 0.7413\n",
      "Epoch 107/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5084 - accuracy: 0.7443 - val_loss: 0.5130 - val_accuracy: 0.7399\n",
      "Epoch 108/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5074 - accuracy: 0.7455 - val_loss: 0.5135 - val_accuracy: 0.7383\n",
      "Epoch 109/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5077 - accuracy: 0.7435 - val_loss: 0.5136 - val_accuracy: 0.7408\n",
      "Epoch 110/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5078 - accuracy: 0.7443 - val_loss: 0.5143 - val_accuracy: 0.7431\n",
      "Epoch 111/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5079 - accuracy: 0.7439 - val_loss: 0.5134 - val_accuracy: 0.7433\n",
      "Epoch 112/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5072 - accuracy: 0.7442 - val_loss: 0.5157 - val_accuracy: 0.7433\n",
      "Epoch 113/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5073 - accuracy: 0.7434 - val_loss: 0.5131 - val_accuracy: 0.7424\n",
      "Epoch 114/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5077 - accuracy: 0.7438 - val_loss: 0.5122 - val_accuracy: 0.7418\n",
      "Epoch 115/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5074 - accuracy: 0.7441 - val_loss: 0.5122 - val_accuracy: 0.7429\n",
      "Epoch 116/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5071 - accuracy: 0.7449 - val_loss: 0.5129 - val_accuracy: 0.7414\n",
      "Epoch 117/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5076 - accuracy: 0.7454 - val_loss: 0.5152 - val_accuracy: 0.7424\n",
      "Epoch 118/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5061 - accuracy: 0.7448 - val_loss: 0.5137 - val_accuracy: 0.7402\n",
      "Epoch 119/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5065 - accuracy: 0.7446 - val_loss: 0.5148 - val_accuracy: 0.7396\n",
      "Epoch 120/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5065 - accuracy: 0.7448 - val_loss: 0.5131 - val_accuracy: 0.7417\n",
      "Epoch 121/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5063 - accuracy: 0.7445 - val_loss: 0.5142 - val_accuracy: 0.7414\n",
      "Epoch 122/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5064 - accuracy: 0.7440 - val_loss: 0.5140 - val_accuracy: 0.7399\n",
      "Epoch 123/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5057 - accuracy: 0.7462 - val_loss: 0.5144 - val_accuracy: 0.7402\n",
      "Epoch 124/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5062 - accuracy: 0.7441 - val_loss: 0.5140 - val_accuracy: 0.7436\n",
      "Epoch 125/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5061 - accuracy: 0.7455 - val_loss: 0.5152 - val_accuracy: 0.7447\n",
      "Epoch 126/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5057 - accuracy: 0.7470 - val_loss: 0.5153 - val_accuracy: 0.7361\n",
      "Epoch 127/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5054 - accuracy: 0.7451 - val_loss: 0.5134 - val_accuracy: 0.7441\n",
      "Epoch 128/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5053 - accuracy: 0.7462 - val_loss: 0.5170 - val_accuracy: 0.7436\n",
      "Epoch 129/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5047 - accuracy: 0.7458 - val_loss: 0.5139 - val_accuracy: 0.7367\n",
      "Epoch 130/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5049 - accuracy: 0.7460 - val_loss: 0.5149 - val_accuracy: 0.7402\n",
      "Epoch 131/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5047 - accuracy: 0.7459 - val_loss: 0.5132 - val_accuracy: 0.7442\n",
      "Epoch 132/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5043 - accuracy: 0.7466 - val_loss: 0.5160 - val_accuracy: 0.7448\n",
      "Epoch 133/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5048 - accuracy: 0.7453 - val_loss: 0.5196 - val_accuracy: 0.7438\n",
      "Epoch 134/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5048 - accuracy: 0.7477 - val_loss: 0.5155 - val_accuracy: 0.7393\n",
      "Epoch 135/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5038 - accuracy: 0.7467 - val_loss: 0.5142 - val_accuracy: 0.7407\n",
      "Epoch 136/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5047 - accuracy: 0.7459 - val_loss: 0.5152 - val_accuracy: 0.7467\n",
      "Epoch 137/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5040 - accuracy: 0.7458 - val_loss: 0.5142 - val_accuracy: 0.7436\n",
      "Epoch 138/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5036 - accuracy: 0.7469 - val_loss: 0.5168 - val_accuracy: 0.7385\n",
      "Epoch 139/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5045 - accuracy: 0.7463 - val_loss: 0.5161 - val_accuracy: 0.7400\n",
      "Epoch 140/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5043 - accuracy: 0.7464 - val_loss: 0.5133 - val_accuracy: 0.7410\n",
      "Epoch 141/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5037 - accuracy: 0.7466 - val_loss: 0.5161 - val_accuracy: 0.7422\n",
      "Epoch 142/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5040 - accuracy: 0.7472 - val_loss: 0.5151 - val_accuracy: 0.7408\n",
      "Epoch 143/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5032 - accuracy: 0.7474 - val_loss: 0.5153 - val_accuracy: 0.7384\n",
      "Epoch 144/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5042 - accuracy: 0.7461 - val_loss: 0.5143 - val_accuracy: 0.7420\n",
      "Epoch 145/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5032 - accuracy: 0.7468 - val_loss: 0.5142 - val_accuracy: 0.7385\n",
      "Epoch 146/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5031 - accuracy: 0.7460 - val_loss: 0.5149 - val_accuracy: 0.7407\n",
      "Epoch 147/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5019 - accuracy: 0.7487 - val_loss: 0.5141 - val_accuracy: 0.7412\n",
      "Epoch 148/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5028 - accuracy: 0.7472 - val_loss: 0.5155 - val_accuracy: 0.7407\n",
      "Epoch 149/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5028 - accuracy: 0.7485 - val_loss: 0.5144 - val_accuracy: 0.7396\n",
      "Epoch 150/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5031 - accuracy: 0.7468 - val_loss: 0.5155 - val_accuracy: 0.7394\n",
      "Epoch 151/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5022 - accuracy: 0.7476 - val_loss: 0.5159 - val_accuracy: 0.7376\n",
      "Epoch 152/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5023 - accuracy: 0.7470 - val_loss: 0.5149 - val_accuracy: 0.7396\n",
      "Epoch 153/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5027 - accuracy: 0.7483 - val_loss: 0.5151 - val_accuracy: 0.7356\n",
      "Epoch 154/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5024 - accuracy: 0.7468 - val_loss: 0.5162 - val_accuracy: 0.7415\n",
      "Epoch 155/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5024 - accuracy: 0.7483 - val_loss: 0.5146 - val_accuracy: 0.7388\n",
      "Epoch 156/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5023 - accuracy: 0.7476 - val_loss: 0.5158 - val_accuracy: 0.7408\n",
      "Epoch 157/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5025 - accuracy: 0.7460 - val_loss: 0.5151 - val_accuracy: 0.7428\n",
      "Epoch 158/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5022 - accuracy: 0.7491 - val_loss: 0.5159 - val_accuracy: 0.7420\n",
      "Epoch 159/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5019 - accuracy: 0.7481 - val_loss: 0.5144 - val_accuracy: 0.7391\n",
      "Epoch 160/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5006 - accuracy: 0.7473 - val_loss: 0.5177 - val_accuracy: 0.7413\n",
      "Epoch 161/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5006 - accuracy: 0.7490 - val_loss: 0.5169 - val_accuracy: 0.7426\n",
      "Epoch 162/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5011 - accuracy: 0.7472 - val_loss: 0.5178 - val_accuracy: 0.7446\n",
      "Epoch 163/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5018 - accuracy: 0.7466 - val_loss: 0.5184 - val_accuracy: 0.7417\n",
      "Epoch 164/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5016 - accuracy: 0.7486 - val_loss: 0.5169 - val_accuracy: 0.7388\n",
      "Epoch 165/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5004 - accuracy: 0.7489 - val_loss: 0.5163 - val_accuracy: 0.7371\n",
      "Epoch 166/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5005 - accuracy: 0.7486 - val_loss: 0.5179 - val_accuracy: 0.7376\n",
      "Epoch 167/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5002 - accuracy: 0.7486 - val_loss: 0.5173 - val_accuracy: 0.7381\n",
      "Epoch 168/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5004 - accuracy: 0.7485 - val_loss: 0.5190 - val_accuracy: 0.7433\n",
      "Epoch 169/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5013 - accuracy: 0.7494 - val_loss: 0.5166 - val_accuracy: 0.7400\n",
      "Epoch 170/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5001 - accuracy: 0.7491 - val_loss: 0.5185 - val_accuracy: 0.7431\n",
      "Epoch 171/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5004 - accuracy: 0.7487 - val_loss: 0.5174 - val_accuracy: 0.7400\n",
      "Epoch 172/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5004 - accuracy: 0.7490 - val_loss: 0.5184 - val_accuracy: 0.7446\n",
      "Epoch 173/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5004 - accuracy: 0.7489 - val_loss: 0.5179 - val_accuracy: 0.7386\n",
      "Epoch 174/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4998 - accuracy: 0.7495 - val_loss: 0.5187 - val_accuracy: 0.7355\n",
      "Epoch 175/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4997 - accuracy: 0.7482 - val_loss: 0.5156 - val_accuracy: 0.7376\n",
      "Epoch 176/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5002 - accuracy: 0.7492 - val_loss: 0.5185 - val_accuracy: 0.7415\n",
      "Epoch 177/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4990 - accuracy: 0.7489 - val_loss: 0.5171 - val_accuracy: 0.7437\n",
      "Epoch 178/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.5005 - accuracy: 0.7493 - val_loss: 0.5155 - val_accuracy: 0.7383\n",
      "Epoch 179/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4999 - accuracy: 0.7482 - val_loss: 0.5209 - val_accuracy: 0.7412\n",
      "Epoch 180/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4986 - accuracy: 0.7493 - val_loss: 0.5165 - val_accuracy: 0.7424\n",
      "Epoch 181/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4993 - accuracy: 0.7492 - val_loss: 0.5199 - val_accuracy: 0.7426\n",
      "Epoch 182/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4992 - accuracy: 0.7493 - val_loss: 0.5184 - val_accuracy: 0.7415\n",
      "Epoch 183/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4992 - accuracy: 0.7498 - val_loss: 0.5178 - val_accuracy: 0.7378\n",
      "Epoch 184/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4986 - accuracy: 0.7491 - val_loss: 0.5195 - val_accuracy: 0.7396\n",
      "Epoch 185/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4994 - accuracy: 0.7505 - val_loss: 0.5168 - val_accuracy: 0.7408\n",
      "Epoch 186/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4980 - accuracy: 0.7495 - val_loss: 0.5178 - val_accuracy: 0.7426\n",
      "Epoch 187/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4985 - accuracy: 0.7515 - val_loss: 0.5172 - val_accuracy: 0.7381\n",
      "Epoch 188/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4988 - accuracy: 0.7496 - val_loss: 0.5184 - val_accuracy: 0.7399\n",
      "Epoch 189/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4987 - accuracy: 0.7491 - val_loss: 0.5179 - val_accuracy: 0.7403\n",
      "Epoch 190/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4982 - accuracy: 0.7498 - val_loss: 0.5172 - val_accuracy: 0.7405\n",
      "Epoch 191/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4980 - accuracy: 0.7505 - val_loss: 0.5176 - val_accuracy: 0.7418\n",
      "Epoch 192/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4987 - accuracy: 0.7488 - val_loss: 0.5183 - val_accuracy: 0.7434\n",
      "Epoch 193/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4984 - accuracy: 0.7497 - val_loss: 0.5185 - val_accuracy: 0.7376\n",
      "Epoch 194/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4979 - accuracy: 0.7486 - val_loss: 0.5191 - val_accuracy: 0.7412\n",
      "Epoch 195/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4975 - accuracy: 0.7499 - val_loss: 0.5221 - val_accuracy: 0.7432\n",
      "Epoch 196/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4986 - accuracy: 0.7502 - val_loss: 0.5184 - val_accuracy: 0.7351\n",
      "Epoch 197/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4979 - accuracy: 0.7507 - val_loss: 0.5184 - val_accuracy: 0.7424\n",
      "Epoch 198/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4976 - accuracy: 0.7506 - val_loss: 0.5189 - val_accuracy: 0.7432\n",
      "Epoch 199/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4977 - accuracy: 0.7495 - val_loss: 0.5180 - val_accuracy: 0.7399\n",
      "Epoch 200/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4971 - accuracy: 0.7510 - val_loss: 0.5180 - val_accuracy: 0.7370\n",
      "Epoch 201/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4972 - accuracy: 0.7509 - val_loss: 0.5193 - val_accuracy: 0.7434\n",
      "Epoch 202/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4974 - accuracy: 0.7502 - val_loss: 0.5179 - val_accuracy: 0.7410\n",
      "Epoch 203/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4974 - accuracy: 0.7514 - val_loss: 0.5190 - val_accuracy: 0.7378\n",
      "Epoch 204/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4964 - accuracy: 0.7509 - val_loss: 0.5196 - val_accuracy: 0.7417\n",
      "Epoch 205/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4962 - accuracy: 0.7513 - val_loss: 0.5205 - val_accuracy: 0.7394\n",
      "Epoch 206/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4967 - accuracy: 0.7518 - val_loss: 0.5177 - val_accuracy: 0.7394\n",
      "Epoch 207/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4964 - accuracy: 0.7516 - val_loss: 0.5197 - val_accuracy: 0.7374\n",
      "Epoch 208/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4961 - accuracy: 0.7522 - val_loss: 0.5179 - val_accuracy: 0.7369\n",
      "Epoch 209/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4954 - accuracy: 0.7504 - val_loss: 0.5191 - val_accuracy: 0.7439\n",
      "Epoch 210/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4960 - accuracy: 0.7516 - val_loss: 0.5195 - val_accuracy: 0.7407\n",
      "Epoch 211/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4962 - accuracy: 0.7519 - val_loss: 0.5184 - val_accuracy: 0.7394\n",
      "Epoch 212/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4963 - accuracy: 0.7510 - val_loss: 0.5173 - val_accuracy: 0.7398\n",
      "Epoch 213/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4955 - accuracy: 0.7514 - val_loss: 0.5194 - val_accuracy: 0.7451\n",
      "Epoch 214/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4962 - accuracy: 0.7520 - val_loss: 0.5167 - val_accuracy: 0.7386\n",
      "Epoch 215/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4947 - accuracy: 0.7516 - val_loss: 0.5192 - val_accuracy: 0.7436\n",
      "Epoch 216/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4953 - accuracy: 0.7532 - val_loss: 0.5200 - val_accuracy: 0.7395\n",
      "Epoch 217/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4958 - accuracy: 0.7522 - val_loss: 0.5206 - val_accuracy: 0.7385\n",
      "Epoch 218/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4956 - accuracy: 0.7518 - val_loss: 0.5192 - val_accuracy: 0.7429\n",
      "Epoch 219/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4954 - accuracy: 0.7517 - val_loss: 0.5228 - val_accuracy: 0.7407\n",
      "Epoch 220/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4957 - accuracy: 0.7520 - val_loss: 0.5220 - val_accuracy: 0.7391\n",
      "Epoch 221/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4945 - accuracy: 0.7531 - val_loss: 0.5193 - val_accuracy: 0.7404\n",
      "Epoch 222/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4953 - accuracy: 0.7519 - val_loss: 0.5216 - val_accuracy: 0.7372\n",
      "Epoch 223/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4953 - accuracy: 0.7506 - val_loss: 0.5205 - val_accuracy: 0.7371\n",
      "Epoch 224/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4946 - accuracy: 0.7521 - val_loss: 0.5197 - val_accuracy: 0.7360\n",
      "Epoch 225/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4948 - accuracy: 0.7499 - val_loss: 0.5213 - val_accuracy: 0.7398\n",
      "Epoch 226/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4956 - accuracy: 0.7510 - val_loss: 0.5227 - val_accuracy: 0.7372\n",
      "Epoch 227/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4942 - accuracy: 0.7520 - val_loss: 0.5189 - val_accuracy: 0.7405\n",
      "Epoch 228/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4938 - accuracy: 0.7524 - val_loss: 0.5192 - val_accuracy: 0.7427\n",
      "Epoch 229/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4946 - accuracy: 0.7517 - val_loss: 0.5208 - val_accuracy: 0.7376\n",
      "Epoch 230/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4947 - accuracy: 0.7518 - val_loss: 0.5201 - val_accuracy: 0.7413\n",
      "Epoch 231/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4941 - accuracy: 0.7521 - val_loss: 0.5230 - val_accuracy: 0.7424\n",
      "Epoch 232/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4944 - accuracy: 0.7531 - val_loss: 0.5240 - val_accuracy: 0.7408\n",
      "Epoch 233/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4938 - accuracy: 0.7522 - val_loss: 0.5201 - val_accuracy: 0.7393\n",
      "Epoch 234/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4948 - accuracy: 0.7523 - val_loss: 0.5208 - val_accuracy: 0.7408\n",
      "Epoch 235/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4938 - accuracy: 0.7532 - val_loss: 0.5206 - val_accuracy: 0.7414\n",
      "Epoch 236/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4939 - accuracy: 0.7514 - val_loss: 0.5229 - val_accuracy: 0.7434\n",
      "Epoch 237/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4944 - accuracy: 0.7516 - val_loss: 0.5196 - val_accuracy: 0.7379\n",
      "Epoch 238/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4941 - accuracy: 0.7536 - val_loss: 0.5222 - val_accuracy: 0.7403\n",
      "Epoch 239/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4935 - accuracy: 0.7539 - val_loss: 0.5206 - val_accuracy: 0.7410\n",
      "Epoch 240/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4939 - accuracy: 0.7522 - val_loss: 0.5215 - val_accuracy: 0.7393\n",
      "Epoch 241/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4942 - accuracy: 0.7525 - val_loss: 0.5211 - val_accuracy: 0.7398\n",
      "Epoch 242/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4932 - accuracy: 0.7532 - val_loss: 0.5214 - val_accuracy: 0.7380\n",
      "Epoch 243/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4931 - accuracy: 0.7529 - val_loss: 0.5212 - val_accuracy: 0.7426\n",
      "Epoch 244/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4931 - accuracy: 0.7532 - val_loss: 0.5209 - val_accuracy: 0.7426\n",
      "Epoch 245/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4921 - accuracy: 0.7541 - val_loss: 0.5241 - val_accuracy: 0.7413\n",
      "Epoch 246/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4926 - accuracy: 0.7528 - val_loss: 0.5211 - val_accuracy: 0.7402\n",
      "Epoch 247/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4922 - accuracy: 0.7532 - val_loss: 0.5214 - val_accuracy: 0.7399\n",
      "Epoch 248/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4914 - accuracy: 0.7534 - val_loss: 0.5228 - val_accuracy: 0.7427\n",
      "Epoch 249/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4931 - accuracy: 0.7524 - val_loss: 0.5248 - val_accuracy: 0.7366\n",
      "Epoch 250/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4930 - accuracy: 0.7532 - val_loss: 0.5215 - val_accuracy: 0.7415\n",
      "Epoch 251/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4923 - accuracy: 0.7534 - val_loss: 0.5233 - val_accuracy: 0.7433\n",
      "Epoch 252/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4929 - accuracy: 0.7518 - val_loss: 0.5217 - val_accuracy: 0.7439\n",
      "Epoch 253/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4918 - accuracy: 0.7541 - val_loss: 0.5206 - val_accuracy: 0.7380\n",
      "Epoch 254/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4920 - accuracy: 0.7534 - val_loss: 0.5221 - val_accuracy: 0.7414\n",
      "Epoch 255/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4923 - accuracy: 0.7520 - val_loss: 0.5248 - val_accuracy: 0.7422\n",
      "Epoch 256/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4918 - accuracy: 0.7549 - val_loss: 0.5242 - val_accuracy: 0.7441\n",
      "Epoch 257/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4924 - accuracy: 0.7530 - val_loss: 0.5225 - val_accuracy: 0.7442\n",
      "Epoch 258/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4928 - accuracy: 0.7537 - val_loss: 0.5236 - val_accuracy: 0.7400\n",
      "Epoch 259/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4904 - accuracy: 0.7545 - val_loss: 0.5242 - val_accuracy: 0.7436\n",
      "Epoch 260/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4923 - accuracy: 0.7524 - val_loss: 0.5241 - val_accuracy: 0.7398\n",
      "Epoch 261/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4912 - accuracy: 0.7544 - val_loss: 0.5231 - val_accuracy: 0.7423\n",
      "Epoch 262/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4918 - accuracy: 0.7556 - val_loss: 0.5232 - val_accuracy: 0.7383\n",
      "Epoch 263/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4919 - accuracy: 0.7533 - val_loss: 0.5217 - val_accuracy: 0.7405\n",
      "Epoch 264/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4918 - accuracy: 0.7536 - val_loss: 0.5271 - val_accuracy: 0.7424\n",
      "Epoch 265/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4914 - accuracy: 0.7534 - val_loss: 0.5246 - val_accuracy: 0.7451\n",
      "Epoch 266/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4913 - accuracy: 0.7539 - val_loss: 0.5224 - val_accuracy: 0.7414\n",
      "Epoch 267/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4912 - accuracy: 0.7542 - val_loss: 0.5237 - val_accuracy: 0.7403\n",
      "Epoch 268/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4916 - accuracy: 0.7540 - val_loss: 0.5259 - val_accuracy: 0.7355\n",
      "Epoch 269/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4912 - accuracy: 0.7539 - val_loss: 0.5223 - val_accuracy: 0.7388\n",
      "Epoch 270/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4916 - accuracy: 0.7542 - val_loss: 0.5259 - val_accuracy: 0.7417\n",
      "Epoch 271/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4918 - accuracy: 0.7544 - val_loss: 0.5264 - val_accuracy: 0.7427\n",
      "Epoch 272/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4901 - accuracy: 0.7542 - val_loss: 0.5255 - val_accuracy: 0.7383\n",
      "Epoch 273/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4908 - accuracy: 0.7543 - val_loss: 0.5240 - val_accuracy: 0.7404\n",
      "Epoch 274/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4914 - accuracy: 0.7526 - val_loss: 0.5236 - val_accuracy: 0.7441\n",
      "Epoch 275/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4904 - accuracy: 0.7549 - val_loss: 0.5252 - val_accuracy: 0.7443\n",
      "Epoch 276/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4907 - accuracy: 0.7536 - val_loss: 0.5246 - val_accuracy: 0.7380\n",
      "Epoch 277/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4914 - accuracy: 0.7535 - val_loss: 0.5242 - val_accuracy: 0.7356\n",
      "Epoch 278/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4901 - accuracy: 0.7538 - val_loss: 0.5240 - val_accuracy: 0.7417\n",
      "Epoch 279/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4896 - accuracy: 0.7544 - val_loss: 0.5261 - val_accuracy: 0.7378\n",
      "Epoch 280/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4895 - accuracy: 0.7552 - val_loss: 0.5261 - val_accuracy: 0.7400\n",
      "Epoch 281/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4894 - accuracy: 0.7552 - val_loss: 0.5251 - val_accuracy: 0.7394\n",
      "Epoch 282/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4893 - accuracy: 0.7545 - val_loss: 0.5253 - val_accuracy: 0.7391\n",
      "Epoch 283/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4901 - accuracy: 0.7538 - val_loss: 0.5264 - val_accuracy: 0.7439\n",
      "Epoch 284/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4906 - accuracy: 0.7557 - val_loss: 0.5223 - val_accuracy: 0.7379\n",
      "Epoch 285/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4900 - accuracy: 0.7542 - val_loss: 0.5261 - val_accuracy: 0.7376\n",
      "Epoch 286/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4890 - accuracy: 0.7543 - val_loss: 0.5253 - val_accuracy: 0.7467\n",
      "Epoch 287/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4895 - accuracy: 0.7555 - val_loss: 0.5242 - val_accuracy: 0.7390\n",
      "Epoch 288/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4890 - accuracy: 0.7552 - val_loss: 0.5253 - val_accuracy: 0.7433\n",
      "Epoch 289/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4899 - accuracy: 0.7547 - val_loss: 0.5274 - val_accuracy: 0.7407\n",
      "Epoch 290/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4890 - accuracy: 0.7557 - val_loss: 0.5285 - val_accuracy: 0.7384\n",
      "Epoch 291/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4888 - accuracy: 0.7560 - val_loss: 0.5251 - val_accuracy: 0.7372\n",
      "Epoch 292/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4881 - accuracy: 0.7568 - val_loss: 0.5273 - val_accuracy: 0.7385\n",
      "Epoch 293/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4885 - accuracy: 0.7550 - val_loss: 0.5256 - val_accuracy: 0.7431\n",
      "Epoch 294/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4883 - accuracy: 0.7555 - val_loss: 0.5252 - val_accuracy: 0.7398\n",
      "Epoch 295/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4886 - accuracy: 0.7559 - val_loss: 0.5254 - val_accuracy: 0.7419\n",
      "Epoch 296/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4884 - accuracy: 0.7548 - val_loss: 0.5267 - val_accuracy: 0.7395\n",
      "Epoch 297/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4887 - accuracy: 0.7544 - val_loss: 0.5264 - val_accuracy: 0.7403\n",
      "Epoch 298/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4885 - accuracy: 0.7553 - val_loss: 0.5274 - val_accuracy: 0.7405\n",
      "Epoch 299/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4888 - accuracy: 0.7556 - val_loss: 0.5262 - val_accuracy: 0.7412\n",
      "Epoch 300/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4888 - accuracy: 0.7543 - val_loss: 0.5248 - val_accuracy: 0.7402\n",
      "Epoch 301/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4881 - accuracy: 0.7574 - val_loss: 0.5241 - val_accuracy: 0.7404\n",
      "Epoch 302/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4882 - accuracy: 0.7571 - val_loss: 0.5253 - val_accuracy: 0.7381\n",
      "Epoch 303/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4888 - accuracy: 0.7561 - val_loss: 0.5241 - val_accuracy: 0.7396\n",
      "Epoch 304/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4888 - accuracy: 0.7559 - val_loss: 0.5247 - val_accuracy: 0.7383\n",
      "Epoch 305/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4876 - accuracy: 0.7568 - val_loss: 0.5279 - val_accuracy: 0.7364\n",
      "Epoch 306/500\n",
      "990/990 [==============================] - 2s 1ms/step - loss: 0.4890 - accuracy: 0.7542 - val_loss: 0.5279 - val_accuracy: 0.7407\n",
      "Epoch 307/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4879 - accuracy: 0.7572 - val_loss: 0.5251 - val_accuracy: 0.7434\n",
      "Epoch 308/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4878 - accuracy: 0.7555 - val_loss: 0.5277 - val_accuracy: 0.7375\n",
      "Epoch 309/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4866 - accuracy: 0.7570 - val_loss: 0.5304 - val_accuracy: 0.7424\n",
      "Epoch 310/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4878 - accuracy: 0.7557 - val_loss: 0.5287 - val_accuracy: 0.7361\n",
      "Epoch 311/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4882 - accuracy: 0.7543 - val_loss: 0.5274 - val_accuracy: 0.7371\n",
      "Epoch 312/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4869 - accuracy: 0.7562 - val_loss: 0.5251 - val_accuracy: 0.7438\n",
      "Epoch 313/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4879 - accuracy: 0.7564 - val_loss: 0.5237 - val_accuracy: 0.7409\n",
      "Epoch 314/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4872 - accuracy: 0.7562 - val_loss: 0.5289 - val_accuracy: 0.7384\n",
      "Epoch 315/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4879 - accuracy: 0.7564 - val_loss: 0.5255 - val_accuracy: 0.7407\n",
      "Epoch 316/500\n",
      "990/990 [==============================] - 2s 1ms/step - loss: 0.4877 - accuracy: 0.7558 - val_loss: 0.5304 - val_accuracy: 0.7452\n",
      "Epoch 317/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4869 - accuracy: 0.7563 - val_loss: 0.5258 - val_accuracy: 0.7414\n",
      "Epoch 318/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4872 - accuracy: 0.7569 - val_loss: 0.5284 - val_accuracy: 0.7393\n",
      "Epoch 319/500\n",
      "990/990 [==============================] - 2s 1ms/step - loss: 0.4868 - accuracy: 0.7563 - val_loss: 0.5264 - val_accuracy: 0.7426\n",
      "Epoch 320/500\n",
      "990/990 [==============================] - 2s 1ms/step - loss: 0.4860 - accuracy: 0.7590 - val_loss: 0.5268 - val_accuracy: 0.7410\n",
      "Epoch 321/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4863 - accuracy: 0.7558 - val_loss: 0.5268 - val_accuracy: 0.7396\n",
      "Epoch 322/500\n",
      "990/990 [==============================] - 2s 1ms/step - loss: 0.4872 - accuracy: 0.7573 - val_loss: 0.5262 - val_accuracy: 0.7417\n",
      "Epoch 323/500\n",
      "990/990 [==============================] - 2s 1ms/step - loss: 0.4859 - accuracy: 0.7573 - val_loss: 0.5294 - val_accuracy: 0.7403\n",
      "Epoch 324/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4861 - accuracy: 0.7580 - val_loss: 0.5278 - val_accuracy: 0.7412\n",
      "Epoch 325/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4867 - accuracy: 0.7565 - val_loss: 0.5277 - val_accuracy: 0.7408\n",
      "Epoch 326/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4861 - accuracy: 0.7575 - val_loss: 0.5266 - val_accuracy: 0.7408\n",
      "Epoch 327/500\n",
      "990/990 [==============================] - 2s 1ms/step - loss: 0.4862 - accuracy: 0.7566 - val_loss: 0.5306 - val_accuracy: 0.7402\n",
      "Epoch 328/500\n",
      "990/990 [==============================] - 2s 1ms/step - loss: 0.4855 - accuracy: 0.7586 - val_loss: 0.5279 - val_accuracy: 0.7404\n",
      "Epoch 329/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4861 - accuracy: 0.7564 - val_loss: 0.5264 - val_accuracy: 0.7396\n",
      "Epoch 330/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4859 - accuracy: 0.7582 - val_loss: 0.5278 - val_accuracy: 0.7327\n",
      "Epoch 331/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4855 - accuracy: 0.7566 - val_loss: 0.5269 - val_accuracy: 0.7390\n",
      "Epoch 332/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4855 - accuracy: 0.7585 - val_loss: 0.5297 - val_accuracy: 0.7413\n",
      "Epoch 333/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4860 - accuracy: 0.7580 - val_loss: 0.5312 - val_accuracy: 0.7407\n",
      "Epoch 334/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4860 - accuracy: 0.7578 - val_loss: 0.5272 - val_accuracy: 0.7374\n",
      "Epoch 335/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4859 - accuracy: 0.7571 - val_loss: 0.5306 - val_accuracy: 0.7399\n",
      "Epoch 336/500\n",
      "990/990 [==============================] - 2s 1ms/step - loss: 0.4859 - accuracy: 0.7574 - val_loss: 0.5281 - val_accuracy: 0.7403\n",
      "Epoch 337/500\n",
      "990/990 [==============================] - 2s 1ms/step - loss: 0.4856 - accuracy: 0.7571 - val_loss: 0.5275 - val_accuracy: 0.7394\n",
      "Epoch 338/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4861 - accuracy: 0.7571 - val_loss: 0.5305 - val_accuracy: 0.7381\n",
      "Epoch 339/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4838 - accuracy: 0.7566 - val_loss: 0.5315 - val_accuracy: 0.7381\n",
      "Epoch 340/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4844 - accuracy: 0.7591 - val_loss: 0.5282 - val_accuracy: 0.7405\n",
      "Epoch 341/500\n",
      "990/990 [==============================] - 2s 1ms/step - loss: 0.4861 - accuracy: 0.7586 - val_loss: 0.5303 - val_accuracy: 0.7399\n",
      "Epoch 342/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4849 - accuracy: 0.7588 - val_loss: 0.5290 - val_accuracy: 0.7424\n",
      "Epoch 343/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4849 - accuracy: 0.7573 - val_loss: 0.5311 - val_accuracy: 0.7431\n",
      "Epoch 344/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4859 - accuracy: 0.7583 - val_loss: 0.5289 - val_accuracy: 0.7384\n",
      "Epoch 345/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4855 - accuracy: 0.7581 - val_loss: 0.5289 - val_accuracy: 0.7381\n",
      "Epoch 346/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4845 - accuracy: 0.7587 - val_loss: 0.5298 - val_accuracy: 0.7404\n",
      "Epoch 347/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4847 - accuracy: 0.7585 - val_loss: 0.5302 - val_accuracy: 0.7405\n",
      "Epoch 348/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4849 - accuracy: 0.7577 - val_loss: 0.5280 - val_accuracy: 0.7399\n",
      "Epoch 349/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4854 - accuracy: 0.7572 - val_loss: 0.5315 - val_accuracy: 0.7372\n",
      "Epoch 350/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4845 - accuracy: 0.7569 - val_loss: 0.5310 - val_accuracy: 0.7354\n",
      "Epoch 351/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4844 - accuracy: 0.7592 - val_loss: 0.5281 - val_accuracy: 0.7420\n",
      "Epoch 352/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4842 - accuracy: 0.7593 - val_loss: 0.5284 - val_accuracy: 0.7360\n",
      "Epoch 353/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4844 - accuracy: 0.7578 - val_loss: 0.5294 - val_accuracy: 0.7385\n",
      "Epoch 354/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4835 - accuracy: 0.7586 - val_loss: 0.5301 - val_accuracy: 0.7367\n",
      "Epoch 355/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4835 - accuracy: 0.7584 - val_loss: 0.5320 - val_accuracy: 0.7396\n",
      "Epoch 356/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4839 - accuracy: 0.7590 - val_loss: 0.5301 - val_accuracy: 0.7402\n",
      "Epoch 357/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4832 - accuracy: 0.7586 - val_loss: 0.5300 - val_accuracy: 0.7379\n",
      "Epoch 358/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4844 - accuracy: 0.7584 - val_loss: 0.5317 - val_accuracy: 0.7360\n",
      "Epoch 359/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4838 - accuracy: 0.7579 - val_loss: 0.5347 - val_accuracy: 0.7415\n",
      "Epoch 360/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4840 - accuracy: 0.7584 - val_loss: 0.5305 - val_accuracy: 0.7412\n",
      "Epoch 361/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4846 - accuracy: 0.7575 - val_loss: 0.5281 - val_accuracy: 0.7410\n",
      "Epoch 362/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4839 - accuracy: 0.7567 - val_loss: 0.5327 - val_accuracy: 0.7419\n",
      "Epoch 363/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4833 - accuracy: 0.7580 - val_loss: 0.5322 - val_accuracy: 0.7405\n",
      "Epoch 364/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4837 - accuracy: 0.7599 - val_loss: 0.5293 - val_accuracy: 0.7383\n",
      "Epoch 365/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4826 - accuracy: 0.7588 - val_loss: 0.5292 - val_accuracy: 0.7409\n",
      "Epoch 366/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4841 - accuracy: 0.7604 - val_loss: 0.5335 - val_accuracy: 0.7399\n",
      "Epoch 367/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4849 - accuracy: 0.7572 - val_loss: 0.5338 - val_accuracy: 0.7410\n",
      "Epoch 368/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4837 - accuracy: 0.7574 - val_loss: 0.5305 - val_accuracy: 0.7389\n",
      "Epoch 369/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4831 - accuracy: 0.7578 - val_loss: 0.5327 - val_accuracy: 0.7385\n",
      "Epoch 370/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4833 - accuracy: 0.7600 - val_loss: 0.5321 - val_accuracy: 0.7398\n",
      "Epoch 371/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4835 - accuracy: 0.7584 - val_loss: 0.5320 - val_accuracy: 0.7378\n",
      "Epoch 372/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4837 - accuracy: 0.7603 - val_loss: 0.5334 - val_accuracy: 0.7383\n",
      "Epoch 373/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4841 - accuracy: 0.7591 - val_loss: 0.5289 - val_accuracy: 0.7365\n",
      "Epoch 374/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4837 - accuracy: 0.7593 - val_loss: 0.5315 - val_accuracy: 0.7413\n",
      "Epoch 375/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4831 - accuracy: 0.7590 - val_loss: 0.5339 - val_accuracy: 0.7394\n",
      "Epoch 376/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4817 - accuracy: 0.7590 - val_loss: 0.5313 - val_accuracy: 0.7385\n",
      "Epoch 377/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4825 - accuracy: 0.7576 - val_loss: 0.5349 - val_accuracy: 0.7402\n",
      "Epoch 378/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4832 - accuracy: 0.7588 - val_loss: 0.5313 - val_accuracy: 0.7413\n",
      "Epoch 379/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4825 - accuracy: 0.7606 - val_loss: 0.5304 - val_accuracy: 0.7359\n",
      "Epoch 380/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4838 - accuracy: 0.7596 - val_loss: 0.5331 - val_accuracy: 0.7359\n",
      "Epoch 381/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4825 - accuracy: 0.7603 - val_loss: 0.5332 - val_accuracy: 0.7390\n",
      "Epoch 382/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4816 - accuracy: 0.7598 - val_loss: 0.5363 - val_accuracy: 0.7381\n",
      "Epoch 383/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4819 - accuracy: 0.7591 - val_loss: 0.5311 - val_accuracy: 0.7385\n",
      "Epoch 384/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4832 - accuracy: 0.7584 - val_loss: 0.5331 - val_accuracy: 0.7357\n",
      "Epoch 385/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4826 - accuracy: 0.7580 - val_loss: 0.5365 - val_accuracy: 0.7383\n",
      "Epoch 386/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4826 - accuracy: 0.7592 - val_loss: 0.5357 - val_accuracy: 0.7375\n",
      "Epoch 387/500\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 0.4815 - accuracy: 0.7597 - val_loss: 0.5326 - val_accuracy: 0.7385\n",
      "Epoch 388/500\n",
      "990/990 [==============================] - 2s 1ms/step - loss: 0.4818 - accuracy: 0.7592 - val_loss: 0.5367 - val_accuracy: 0.7376\n",
      "Epoch 389/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4824 - accuracy: 0.7595 - val_loss: 0.5335 - val_accuracy: 0.7346\n",
      "Epoch 390/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4809 - accuracy: 0.7595 - val_loss: 0.5382 - val_accuracy: 0.7380\n",
      "Epoch 391/500\n",
      "990/990 [==============================] - 2s 1ms/step - loss: 0.4813 - accuracy: 0.7598 - val_loss: 0.5342 - val_accuracy: 0.7372\n",
      "Epoch 392/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4812 - accuracy: 0.7581 - val_loss: 0.5353 - val_accuracy: 0.7393\n",
      "Epoch 393/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4820 - accuracy: 0.7582 - val_loss: 0.5366 - val_accuracy: 0.7408\n",
      "Epoch 394/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4808 - accuracy: 0.7606 - val_loss: 0.5311 - val_accuracy: 0.7379\n",
      "Epoch 395/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4818 - accuracy: 0.7596 - val_loss: 0.5320 - val_accuracy: 0.7383\n",
      "Epoch 396/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4819 - accuracy: 0.7590 - val_loss: 0.5328 - val_accuracy: 0.7371\n",
      "Epoch 397/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4806 - accuracy: 0.7598 - val_loss: 0.5329 - val_accuracy: 0.7413\n",
      "Epoch 398/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4803 - accuracy: 0.7595 - val_loss: 0.5336 - val_accuracy: 0.7402\n",
      "Epoch 399/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4812 - accuracy: 0.7609 - val_loss: 0.5363 - val_accuracy: 0.7370\n",
      "Epoch 400/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4811 - accuracy: 0.7596 - val_loss: 0.5340 - val_accuracy: 0.7414\n",
      "Epoch 401/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4807 - accuracy: 0.7605 - val_loss: 0.5375 - val_accuracy: 0.7380\n",
      "Epoch 402/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4801 - accuracy: 0.7610 - val_loss: 0.5367 - val_accuracy: 0.7352\n",
      "Epoch 403/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4808 - accuracy: 0.7598 - val_loss: 0.5366 - val_accuracy: 0.7385\n",
      "Epoch 404/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4802 - accuracy: 0.7595 - val_loss: 0.5313 - val_accuracy: 0.7434\n",
      "Epoch 405/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4797 - accuracy: 0.7602 - val_loss: 0.5395 - val_accuracy: 0.7388\n",
      "Epoch 406/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4816 - accuracy: 0.7603 - val_loss: 0.5363 - val_accuracy: 0.7402\n",
      "Epoch 407/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4811 - accuracy: 0.7601 - val_loss: 0.5380 - val_accuracy: 0.7388\n",
      "Epoch 408/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4806 - accuracy: 0.7598 - val_loss: 0.5367 - val_accuracy: 0.7399\n",
      "Epoch 409/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4800 - accuracy: 0.7599 - val_loss: 0.5350 - val_accuracy: 0.7400\n",
      "Epoch 410/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4811 - accuracy: 0.7599 - val_loss: 0.5355 - val_accuracy: 0.7359\n",
      "Epoch 411/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4811 - accuracy: 0.7601 - val_loss: 0.5356 - val_accuracy: 0.7352\n",
      "Epoch 412/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4801 - accuracy: 0.7607 - val_loss: 0.5373 - val_accuracy: 0.7386\n",
      "Epoch 413/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4798 - accuracy: 0.7601 - val_loss: 0.5378 - val_accuracy: 0.7378\n",
      "Epoch 414/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4805 - accuracy: 0.7606 - val_loss: 0.5359 - val_accuracy: 0.7415\n",
      "Epoch 415/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4789 - accuracy: 0.7613 - val_loss: 0.5349 - val_accuracy: 0.7366\n",
      "Epoch 416/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4803 - accuracy: 0.7600 - val_loss: 0.5362 - val_accuracy: 0.7396\n",
      "Epoch 417/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4794 - accuracy: 0.7606 - val_loss: 0.5388 - val_accuracy: 0.7384\n",
      "Epoch 418/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4805 - accuracy: 0.7612 - val_loss: 0.5412 - val_accuracy: 0.7429\n",
      "Epoch 419/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4800 - accuracy: 0.7597 - val_loss: 0.5391 - val_accuracy: 0.7412\n",
      "Epoch 420/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4791 - accuracy: 0.7612 - val_loss: 0.5356 - val_accuracy: 0.7404\n",
      "Epoch 421/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4800 - accuracy: 0.7606 - val_loss: 0.5371 - val_accuracy: 0.7388\n",
      "Epoch 422/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4784 - accuracy: 0.7619 - val_loss: 0.5377 - val_accuracy: 0.7350\n",
      "Epoch 423/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4798 - accuracy: 0.7590 - val_loss: 0.5363 - val_accuracy: 0.7359\n",
      "Epoch 424/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4804 - accuracy: 0.7605 - val_loss: 0.5352 - val_accuracy: 0.7386\n",
      "Epoch 425/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4798 - accuracy: 0.7611 - val_loss: 0.5382 - val_accuracy: 0.7394\n",
      "Epoch 426/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4793 - accuracy: 0.7610 - val_loss: 0.5353 - val_accuracy: 0.7381\n",
      "Epoch 427/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4798 - accuracy: 0.7610 - val_loss: 0.5363 - val_accuracy: 0.7355\n",
      "Epoch 428/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4794 - accuracy: 0.7618 - val_loss: 0.5389 - val_accuracy: 0.7386\n",
      "Epoch 429/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4794 - accuracy: 0.7620 - val_loss: 0.5376 - val_accuracy: 0.7402\n",
      "Epoch 430/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4791 - accuracy: 0.7615 - val_loss: 0.5384 - val_accuracy: 0.7388\n",
      "Epoch 431/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4789 - accuracy: 0.7598 - val_loss: 0.5366 - val_accuracy: 0.7384\n",
      "Epoch 432/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4798 - accuracy: 0.7601 - val_loss: 0.5403 - val_accuracy: 0.7395\n",
      "Epoch 433/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4782 - accuracy: 0.7619 - val_loss: 0.5431 - val_accuracy: 0.7403\n",
      "Epoch 434/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4797 - accuracy: 0.7601 - val_loss: 0.5403 - val_accuracy: 0.7383\n",
      "Epoch 435/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4797 - accuracy: 0.7608 - val_loss: 0.5374 - val_accuracy: 0.7400\n",
      "Epoch 436/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4788 - accuracy: 0.7623 - val_loss: 0.5404 - val_accuracy: 0.7378\n",
      "Epoch 437/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4790 - accuracy: 0.7596 - val_loss: 0.5417 - val_accuracy: 0.7418\n",
      "Epoch 438/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4791 - accuracy: 0.7618 - val_loss: 0.5374 - val_accuracy: 0.7375\n",
      "Epoch 439/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4782 - accuracy: 0.7626 - val_loss: 0.5356 - val_accuracy: 0.7361\n",
      "Epoch 440/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4776 - accuracy: 0.7621 - val_loss: 0.5403 - val_accuracy: 0.7383\n",
      "Epoch 441/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4783 - accuracy: 0.7630 - val_loss: 0.5379 - val_accuracy: 0.7375\n",
      "Epoch 442/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4784 - accuracy: 0.7616 - val_loss: 0.5423 - val_accuracy: 0.7395\n",
      "Epoch 443/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4778 - accuracy: 0.7620 - val_loss: 0.5391 - val_accuracy: 0.7380\n",
      "Epoch 444/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4783 - accuracy: 0.7615 - val_loss: 0.5408 - val_accuracy: 0.7369\n",
      "Epoch 445/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4788 - accuracy: 0.7618 - val_loss: 0.5389 - val_accuracy: 0.7364\n",
      "Epoch 446/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4779 - accuracy: 0.7608 - val_loss: 0.5421 - val_accuracy: 0.7375\n",
      "Epoch 447/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4799 - accuracy: 0.7594 - val_loss: 0.5445 - val_accuracy: 0.7389\n",
      "Epoch 448/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4784 - accuracy: 0.7619 - val_loss: 0.5404 - val_accuracy: 0.7398\n",
      "Epoch 449/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4788 - accuracy: 0.7614 - val_loss: 0.5376 - val_accuracy: 0.7395\n",
      "Epoch 450/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4777 - accuracy: 0.7626 - val_loss: 0.5392 - val_accuracy: 0.7371\n",
      "Epoch 451/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4786 - accuracy: 0.7623 - val_loss: 0.5400 - val_accuracy: 0.7419\n",
      "Epoch 452/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4775 - accuracy: 0.7637 - val_loss: 0.5438 - val_accuracy: 0.7374\n",
      "Epoch 453/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4789 - accuracy: 0.7614 - val_loss: 0.5375 - val_accuracy: 0.7396\n",
      "Epoch 454/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4772 - accuracy: 0.7616 - val_loss: 0.5434 - val_accuracy: 0.7370\n",
      "Epoch 455/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4776 - accuracy: 0.7622 - val_loss: 0.5391 - val_accuracy: 0.7395\n",
      "Epoch 456/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4781 - accuracy: 0.7614 - val_loss: 0.5402 - val_accuracy: 0.7369\n",
      "Epoch 457/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4760 - accuracy: 0.7624 - val_loss: 0.5406 - val_accuracy: 0.7386\n",
      "Epoch 458/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4771 - accuracy: 0.7637 - val_loss: 0.5391 - val_accuracy: 0.7375\n",
      "Epoch 459/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4779 - accuracy: 0.7611 - val_loss: 0.5426 - val_accuracy: 0.7402\n",
      "Epoch 460/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4764 - accuracy: 0.7627 - val_loss: 0.5402 - val_accuracy: 0.7386\n",
      "Epoch 461/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4761 - accuracy: 0.7635 - val_loss: 0.5444 - val_accuracy: 0.7417\n",
      "Epoch 462/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4763 - accuracy: 0.7636 - val_loss: 0.5419 - val_accuracy: 0.7390\n",
      "Epoch 463/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4756 - accuracy: 0.7620 - val_loss: 0.5428 - val_accuracy: 0.7379\n",
      "Epoch 464/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4771 - accuracy: 0.7636 - val_loss: 0.5412 - val_accuracy: 0.7402\n",
      "Epoch 465/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4768 - accuracy: 0.7621 - val_loss: 0.5405 - val_accuracy: 0.7388\n",
      "Epoch 466/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4777 - accuracy: 0.7615 - val_loss: 0.5447 - val_accuracy: 0.7361\n",
      "Epoch 467/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4775 - accuracy: 0.7630 - val_loss: 0.5411 - val_accuracy: 0.7393\n",
      "Epoch 468/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4763 - accuracy: 0.7616 - val_loss: 0.5379 - val_accuracy: 0.7419\n",
      "Epoch 469/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4771 - accuracy: 0.7626 - val_loss: 0.5437 - val_accuracy: 0.7352\n",
      "Epoch 470/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4762 - accuracy: 0.7615 - val_loss: 0.5430 - val_accuracy: 0.7355\n",
      "Epoch 471/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4770 - accuracy: 0.7629 - val_loss: 0.5392 - val_accuracy: 0.7348\n",
      "Epoch 472/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4758 - accuracy: 0.7636 - val_loss: 0.5399 - val_accuracy: 0.7393\n",
      "Epoch 473/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4752 - accuracy: 0.7635 - val_loss: 0.5408 - val_accuracy: 0.7409\n",
      "Epoch 474/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4753 - accuracy: 0.7630 - val_loss: 0.5455 - val_accuracy: 0.7400\n",
      "Epoch 475/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4764 - accuracy: 0.7637 - val_loss: 0.5395 - val_accuracy: 0.7372\n",
      "Epoch 476/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4764 - accuracy: 0.7630 - val_loss: 0.5424 - val_accuracy: 0.7400\n",
      "Epoch 477/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4764 - accuracy: 0.7610 - val_loss: 0.5419 - val_accuracy: 0.7371\n",
      "Epoch 478/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4761 - accuracy: 0.7615 - val_loss: 0.5385 - val_accuracy: 0.7419\n",
      "Epoch 479/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4763 - accuracy: 0.7626 - val_loss: 0.5442 - val_accuracy: 0.7384\n",
      "Epoch 480/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4748 - accuracy: 0.7634 - val_loss: 0.5423 - val_accuracy: 0.7379\n",
      "Epoch 481/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4757 - accuracy: 0.7618 - val_loss: 0.5396 - val_accuracy: 0.7357\n",
      "Epoch 482/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4759 - accuracy: 0.7636 - val_loss: 0.5442 - val_accuracy: 0.7350\n",
      "Epoch 483/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4771 - accuracy: 0.7626 - val_loss: 0.5523 - val_accuracy: 0.7399\n",
      "Epoch 484/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4753 - accuracy: 0.7641 - val_loss: 0.5431 - val_accuracy: 0.7395\n",
      "Epoch 485/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4760 - accuracy: 0.7633 - val_loss: 0.5415 - val_accuracy: 0.7403\n",
      "Epoch 486/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4751 - accuracy: 0.7641 - val_loss: 0.5446 - val_accuracy: 0.7342\n",
      "Epoch 487/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4761 - accuracy: 0.7626 - val_loss: 0.5453 - val_accuracy: 0.7389\n",
      "Epoch 488/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4758 - accuracy: 0.7631 - val_loss: 0.5452 - val_accuracy: 0.7375\n",
      "Epoch 489/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4755 - accuracy: 0.7630 - val_loss: 0.5427 - val_accuracy: 0.7376\n",
      "Epoch 490/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4761 - accuracy: 0.7615 - val_loss: 0.5403 - val_accuracy: 0.7355\n",
      "Epoch 491/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4756 - accuracy: 0.7643 - val_loss: 0.5428 - val_accuracy: 0.7405\n",
      "Epoch 492/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4746 - accuracy: 0.7631 - val_loss: 0.5422 - val_accuracy: 0.7391\n",
      "Epoch 493/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4755 - accuracy: 0.7634 - val_loss: 0.5396 - val_accuracy: 0.7346\n",
      "Epoch 494/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4741 - accuracy: 0.7644 - val_loss: 0.5450 - val_accuracy: 0.7388\n",
      "Epoch 495/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4762 - accuracy: 0.7638 - val_loss: 0.5390 - val_accuracy: 0.7371\n",
      "Epoch 496/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4742 - accuracy: 0.7649 - val_loss: 0.5453 - val_accuracy: 0.7408\n",
      "Epoch 497/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4752 - accuracy: 0.7644 - val_loss: 0.5439 - val_accuracy: 0.7337\n",
      "Epoch 498/500\n",
      "990/990 [==============================] - 2s 1ms/step - loss: 0.4742 - accuracy: 0.7636 - val_loss: 0.5468 - val_accuracy: 0.7390\n",
      "Epoch 499/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4740 - accuracy: 0.7644 - val_loss: 0.5492 - val_accuracy: 0.7441\n",
      "Epoch 500/500\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 0.4754 - accuracy: 0.7638 - val_loss: 0.5427 - val_accuracy: 0.7391\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f36a222ba60>"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(\"Accuracy\", accuracy)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "310/310 [==============================] - 0s 824us/step - loss: 0.5579 - accuracy: 0.7416\n",
      "Accuracy 0.7416161894798279\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 31/31 [==============================] - 0s 1ms/step - loss: 0.6488 - accuracy: 0.7335\n",
    "# Accuracy 0.7335359454154968\n",
    "\n",
    "# 31/31 [==============================] - 0s 862us/step - loss: 0.5798 - accuracy: 0.6950\n",
    "# Accuracy 0.695035457611084"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Save the entire model as a SavedModel.\n",
    "# !mkdir -p saved_model\n",
    "# model.save('saved_model/my_model')\n",
    "\n",
    "# from tensorflow.keras.models import Sequential, save_model, load_model\n",
    "\n",
    "# filepath = './saved_model'\n",
    "# save_model(model, filepath)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Morgan & Higgs: Findpath with SW 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "input_file = \"dataset_100.csv\"\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "sequence, s1, s2 = df.loc[0]\n",
    "\n",
    "\n",
    "search_width_multiplier = 4\n",
    "fp = findpath.findpath_single(sequence, s1, s2, search_width_multiplier=search_width_multiplier, mp=True)\n",
    "result = fp.get_en()/100.0\n",
    "path = fp.get_path()\n",
    "\n",
    "s = s1\n",
    "pt2 = list(RNA.ptable(s2))\n",
    "fc = RNA.fold_compound(sequence)\n",
    "\n",
    "\n",
    "def find_moves(s_ptable, t_ptable):\n",
    "    \"\"\"\n",
    "    generator function, yields possible structures 1 move away\n",
    "    from the original structure by finding fitting i and j with\n",
    "    RNA pair and loop tables\n",
    "    s_ptable: current ptable\n",
    "    t_ptable: s2 end ptable\n",
    "    \"\"\"\n",
    "    # loop table\n",
    "    ls = RNA.loopidx_from_ptable(s_ptable)\n",
    "    for i in range(len(s_ptable)):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if s_ptable[i] == 0 and t_ptable[i] > i:\n",
    "            j = t_ptable[i]\n",
    "            # found j has to be empty and currently on the same loop as i\n",
    "            if s_ptable[j] == 0 and ls[i] == ls[j]:\n",
    "                yield i, j\n",
    "        # test for bp removal: i has to be paired with a different j in s2\n",
    "        j = s_ptable[i]\n",
    "        # dont remove things which are present in s2\n",
    "        if s_ptable[i] > i and s_ptable[i] != s_ptable[j] and\\\n",
    "                s_ptable[i] != t_ptable[i] and s_ptable[j] != t_ptable[j]:\n",
    "            yield -i, -j\n",
    "\n",
    "\n",
    "def fp_call(sequence, s1, s2, search_width_multiplier = 20):    \n",
    "    fp = findpath.findpath_single(sequence, s1, s2, search_width_multiplier=search_width_multiplier, mp=True)\n",
    "    result = fp.get_en()/100.0\n",
    "    path = fp.get_path()\n",
    "    # return result, path\n",
    "    return result, path\n",
    "\n",
    "\n",
    "def ij_distance(last_move, this_move, ij_moves):\n",
    "    # how far is the last move away from the current move.\n",
    "    # it is likely, that the next move is close to the last one\n",
    "    # there are better distance metrices probably...\n",
    "\n",
    "    # ij move list is supposed to be sorted, find indices\n",
    "    \n",
    "    ijmoves = ij_moves + [last_move]\n",
    "    ijmoves.sort(key=lambda x: (x[0], x[1]))\n",
    "\n",
    "    pos_old = ijmoves.index(last_move)\n",
    "    pos_new = ijmoves.index(this_move)\n",
    "\n",
    "    distance = abs(pos_old-pos_new)/len(ijmoves)\n",
    "    \n",
    "    \n",
    "    # moves left in vicinity out of total moves\n",
    "    \n",
    "    thisi, thisj = this_move\n",
    "    lasti, lastj = last_move\n",
    "    thisclose = 0\n",
    "    lastclose = 0\n",
    "\n",
    "    for i, j in ij_moves:\n",
    "        if (abs(i-thisi) < 5) and (abs(j-thisj) < 5):\n",
    "            thisclose += 1\n",
    "        if (abs(i-lasti) < 5) and (abs(j-lastj) < 5):\n",
    "            lastclose += 1\n",
    "    \n",
    "\n",
    "    thisclose /= len(ij_moves)\n",
    "    lastclose /= len(ij_moves)\n",
    "\n",
    "    # print (\"thisclose\", thisclose, lastclose, distance)\n",
    "    return distance, thisclose, lastclose\n",
    "\n",
    "    print (distance)\n",
    "\n",
    "# sample call\n",
    "ij_moves = [(3, 62), (4, 61), (6, 60), (7, 59), (9, 57), (10, 56), (11, 55), (12, 54), (15, 52), (16, 51), (17, 50), (18, 39), (19, 38), (20, 36), (21, 35), (22, 34), (23, 33), (24, 32), (42, 49), (43, 48), (64, 99)]\n",
    "last_move = (2, 63)\n",
    "this_move = (6, 60)\n",
    "ij_distance(last_move, this_move, ij_moves)\n",
    "\n",
    "# \n",
    "\n",
    "def config_distance(pt, move):\n",
    "    \"\"\"\n",
    "    are we extending / removing the outside/inside layer of a loop or adding something in the middle?\n",
    "    \"\"\"\n",
    "    i = move[0]\n",
    "    j = move[1]\n",
    "    points = 0\n",
    "\n",
    "    # if we're extending from outside to inside, the position i+1 and j-1 should be ideally unpaired\n",
    "    # inside to outside: i-1 and j+1 should be ideally unpaired\n",
    "\n",
    "    if i>0:\n",
    "        # print (\"add\") \n",
    "        # outside/inside paired?        \n",
    "        if j+1 < pt[0] and i-1 > 0: # outside - boundary check\n",
    "            if pt[i-1] == j+1:\n",
    "                points += 1\n",
    "        if pt[i+1] == j-1:\n",
    "            points += 1\n",
    "    if i<0:\n",
    "        # print (\"del\")\n",
    "        i, j = -i, -j\n",
    "        # outside/inside paired?\n",
    "        if j+1 < pt[0] and i-1 > 0: # outside - boundary check\n",
    "            if pt[i-1] == j+1:\n",
    "                points += 1\n",
    "        if pt[i+1] == j-1:\n",
    "            points += 1\n",
    "        elif pt[i+1] == 0 and pt[j-1] == 0:\n",
    "            pass\n",
    "\n",
    "    if points == 2:\n",
    "        points = 0\n",
    "    return points\n",
    "\n",
    "\n",
    "s = s1\n",
    "lasts = s\n",
    "lasti = None\n",
    "lastj = None\n",
    "\n",
    "for e, (a,b, en) in enumerate(path):\n",
    "    if (a,b) == (0,0):\n",
    "        continue  \n",
    "\n",
    "    # check where we can go, compare with our best move. \n",
    "    pt = list(RNA.ptable(s))\n",
    "\n",
    "    # check available moves, save them, sort them    \n",
    "    avail_moves = []\n",
    "    ij_moves = []\n",
    "    found_pos = None\n",
    "\n",
    "    for pos, (i,j) in enumerate(find_moves(pt, pt2)):    \n",
    "        next_en = fc.eval_move_pt(pt, i, j)\n",
    "        # mark where we found our move\n",
    "        found = (i,j) == (a,b)\n",
    "        avail_moves.append((i, j, next_en, found))\n",
    "        ij_moves.append((abs(i),abs(j)))\n",
    "\n",
    "    # sort moves independent of delete insert moves\n",
    "\n",
    "    ij_moves.sort(key=lambda x: (x[0], x[1]))\n",
    "\n",
    "\n",
    "    avail_moves.sort(key=lambda x: x[2])\n",
    "    found_list = [x[3] for x in avail_moves]\n",
    "    en_list = np.array([[x[2] for x in avail_moves]])\n",
    "    en_list_scaled = min_max_scaler.fit_transform(en_list.T).T[0]\n",
    "    \n",
    "\n",
    "    # find where our move is after sorting\n",
    "    found_pos = found_list.index(True)\n",
    "    rel_pos = found_pos * 1.0 / len(found_list)\n",
    "\n",
    "    # print (e, a,b, 'found at pos:', found_pos, 'of', len(avail_moves), ':',  1-rel_pos)\n",
    "    # print (avail_moves, a, b)\n",
    "\n",
    "    # if s != lasts:\n",
    "        # print (\"---\") \n",
    "        # print (s)\n",
    "        # print (\"---\") \n",
    "\n",
    "    # for every move we take we have to run a new findpath, see if this move will yield the ideal result\n",
    "    \n",
    "    for pos, (i,j, en, found) in enumerate(avail_moves):\n",
    "        if i > 0:\n",
    "            snew = s[:i-1] + \"(\" + s[i:j-1] + \")\" + s[j:]\n",
    "        if i < 0:\n",
    "            snew = s[:-i-1] + \".\" + s[-i:-j-1] + \".\" + s[-j:]\n",
    "        ptnew = list(RNA.ptable(snew))\n",
    "\n",
    "        result_new, path = fp_call(sequence, snew, s2)\n",
    "\n",
    "        if result_new <= result:\n",
    "            pos_result = 1\n",
    "        else:\n",
    "            pos_result = 0\n",
    "\n",
    "        if found: found = \"<-- taken\"\n",
    "        else: found = \"\"\n",
    "\n",
    "        this_move = (abs(i), abs(j))\n",
    "        last_move = (lasti, lastj)\n",
    "\n",
    "        if lasti:\n",
    "            # print (this_move, last_move, ij_moves)\n",
    "            ijd, thisclose, lastclose = ij_distance(last_move, this_move, ij_moves)\n",
    "        else:\n",
    "            ijd, thisclose, lastclose = 0, 0, 0\n",
    "\n",
    "        cd = config_distance(pt, this_move)\n",
    "\n",
    "        if lasti:\n",
    "            print (f' {snew[0:20]}, {i}, {j}, {result_new}/{pos_result}: {ijd:2.2f}, {thisclose:2.2f}, {lastclose:2.2f}, {cd}, {en}, {en_list_scaled[pos]:2.2f} {found}')\n",
    "\n",
    "            sample = 1, ijd, thisclose, lastclose, cd, en_list_scaled[pos], 1\n",
    "            sample_test_ds = sample_to_dataset(sample, test)\n",
    "            result_predictor = model.predict(sample_test_ds)[0][0]\n",
    "\n",
    "            print (f' {1}, {ijd:2.2f}, {thisclose:2.2f}, {lastclose:2.2f}, {cd}, {en_list_scaled[pos]:2.2f}, 1: prediction: {result_predictor}')\n",
    "\n",
    "\n",
    "\n",
    "    # if e==63:\n",
    "    #     print (avail_moves)\n",
    "\n",
    "    # update s for the next iteration\n",
    "    \n",
    "    lasts = s\n",
    "    lasti = abs(a)\n",
    "    lastj = abs(b)\n",
    "    if a > 0:\n",
    "        s = s[:a-1] + \"(\" + s[a:b-1] + \")\" + s[b:]\n",
    "    if a < 0:\n",
    "        s = s[:-a-1] + \".\" + s[-a:-b-1] + \".\" + s[-b:]\n",
    "\n",
    "    if e>1: \n",
    "        break\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " ..((.((.(((...((((((, -12, -54, -14.2/1: 0.36, 0.29, 0.14, 1, 90, 0.00 <-- taken\n",
      " 1, 0.36, 0.29, 0.14, 1, 0.00, 1: prediction: 0.6828221082687378\n",
      " ..((.(..((((..((((((, -7, -59, -14.2/1: 0.18, 0.33, 0.14, 1, 110, 0.03 \n",
      " 1, 0.18, 0.33, 0.14, 1, 0.03, 1: prediction: -0.31268346309661865\n",
      " ..((.((.((((..((((((, -24, -32, -13.3/0: 0.82, 0.24, 0.14, 1, 110, 0.03 \n",
      " 1, 0.82, 0.24, 0.14, 1, 0.03, 1: prediction: -2.4331817626953125\n",
      " ..((.((.((((..((((((, -64, -99, -12.7/0: 0.95, 0.05, 0.14, 1, 150, 0.10 \n",
      " 1, 0.95, 0.05, 0.14, 1, 0.10, 1: prediction: -8.228426933288574\n",
      " ..((.((.((((..(((((., -20, -36, -14.2/1: 0.64, 0.33, 0.14, 1, 210, 0.21 \n",
      " 1, 0.64, 0.33, 0.14, 1, 0.21, 1: prediction: -1.486687183380127\n",
      " ..((.((.((((..((((.(, -19, -38, -14.2/1: 0.59, 0.24, 0.14, 1, 300, 0.36 \n",
      " 1, 0.59, 0.24, 0.14, 1, 0.36, 1: prediction: -1.3077186346054077\n",
      " ..((.((..(((..((((((, -9, -57, -14.2/1: 0.23, 0.29, 0.14, 1, 310, 0.38 \n",
      " 1, 0.23, 0.29, 0.14, 1, 0.38, 1: prediction: -0.09669250249862671\n",
      " ..((.((.((((...(((((, -15, -52, -14.2/1: 0.41, 0.24, 0.14, 1, 310, 0.38 \n",
      " 1, 0.41, 0.24, 0.14, 1, 0.38, 1: prediction: -1.4521899223327637\n",
      " ...(.((.((((..((((((, -3, -62, -14.2/1: 0.05, 0.19, 0.14, 1, 330, 0.41 \n",
      " 1, 0.05, 0.19, 0.14, 1, 0.41, 1: prediction: 0.7475156784057617\n",
      " ..((.((.((((..((((((, 42, 49, -14.2/1: 0.86, 0.10, 0.14, 0, 340, 0.43 \n",
      " 1, 0.86, 0.10, 0.14, 0, 0.43, 1: prediction: -1.5531705617904663\n",
      " ..((..(.((((..((((((, -6, -60, -14.2/1: 0.14, 0.29, 0.14, 1, 350, 0.45 \n",
      " 1, 0.14, 0.29, 0.14, 1, 0.45, 1: prediction: -0.18936875462532043\n",
      " ..(..((.((((..((((((, -4, -61, -14.2/1: 0.09, 0.19, 0.14, 1, 390, 0.52 \n",
      " 1, 0.09, 0.19, 0.14, 1, 0.52, 1: prediction: 0.6279747486114502\n",
      " ..((.((.((.(..((((((, -11, -55, -14.2/1: 0.32, 0.29, 0.14, 0, 390, 0.52 \n",
      " 1, 0.32, 0.29, 0.14, 0, 0.52, 1: prediction: -0.9867163896560669\n",
      " ..((.((.((((..((.(((, -17, -50, -14.2/1: 0.50, 0.14, 0.14, 1, 390, 0.52 \n",
      " 1, 0.50, 0.14, 0.14, 1, 0.52, 1: prediction: -1.5610805749893188\n",
      " ..((.((.((((..(((.((, -18, -39, -14.2/1: 0.55, 0.19, 0.14, 1, 390, 0.52 \n",
      " 1, 0.55, 0.19, 0.14, 1, 0.52, 1: prediction: -1.482547402381897\n",
      " ..((.((.((((..((((((, -23, -33, -13.3/0: 0.77, 0.24, 0.14, 0, 420, 0.57 \n",
      " 1, 0.77, 0.24, 0.14, 0, 0.57, 1: prediction: -2.5425515174865723\n",
      " ..((.((.((((..((((((, 43, 48, -14.2/1: 0.91, 0.10, 0.14, 0, 430, 0.59 \n",
      " 1, 0.91, 0.10, 0.14, 0, 0.59, 1: prediction: 0.5762128829956055\n",
      " ..((.((.((((..(.((((, -16, -51, -14.2/1: 0.45, 0.19, 0.14, 0, 500, 0.71 \n",
      " 1, 0.45, 0.19, 0.14, 0, 0.71, 1: prediction: -2.3422980308532715\n",
      " ..((.((.((((..((((((, -21, -35, -13.3/0: 0.68, 0.33, 0.14, 0, 600, 0.88 \n",
      " 1, 0.68, 0.33, 0.14, 0, 0.88, 1: prediction: -3.1054506301879883\n",
      " ..((.((.((((..((((((, -22, -34, -13.3/0: 0.73, 0.29, 0.14, 0, 660, 0.98 \n",
      " 1, 0.73, 0.29, 0.14, 0, 0.98, 1: prediction: -3.650803565979004\n",
      " ..((.((.(.((..((((((, -10, -56, -14.2/1: 0.27, 0.29, 0.14, 0, 670, 1.00 \n",
      " 1, 0.27, 0.29, 0.14, 0, 1.00, 1: prediction: -2.473306179046631\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# A few random samples\n",
    "\n",
    "\n",
    "# Generate predictions for samples\n",
    "predictions = model.predict(test_ds)\n",
    "\n",
    "predictions\n",
    "# sample_test\n",
    "# test_ds"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-1.3807142],\n",
       "       [-0.6300998],\n",
       "       [-2.0548708],\n",
       "       ...,\n",
       "       [-1.4828502],\n",
       "       [-1.810354 ],\n",
       "       [-1.9349219]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "# A utility method to create a tf.data from a dictionary\n",
    "def dict_to_dataset(inputdict, labels, shuffle=True, batch_size=32):\n",
    "  # dataframe = dataframe.copy()\n",
    "  # labels = dataframe.pop('target')\n",
    "  ds = tf.data.Dataset.from_tensor_slices((inputdict, labels))\n",
    "  # ds = tf.data.Dataset.from_tensor_slices([45240,  0.5,  0.142857,  0.047619,  0,  0.930233,       0])\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))  \n",
    "  print ('ds', ds)\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds\n",
    "\n",
    "def sample_to_dataset(sample, test):\n",
    "\n",
    "  sample_test = test.iloc[0:1].copy()\n",
    "  row0 = sample_test.index[0]\n",
    "  sample_test.at[row0 ,'Unnamed: 0'] = 2 # irrelevant\n",
    "  sample_test.at[row0 ,'4'] = sample[1]\n",
    "  sample_test.at[row0 ,'5'] = sample[2]\n",
    "  sample_test.at[row0 ,'6'] = sample[3]\n",
    "  sample_test.at[row0 ,'7'] = sample[4]\n",
    "  sample_test.at[row0 ,'8'] = sample[5]\n",
    "  sample_test.at[row0 ,'target'] = 2 # irrelevant\n",
    "\n",
    "  sample_test_ds = df_to_dataset(sample_test, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "  return sample_test_ds\n",
    "\n",
    "\n",
    "\n",
    "sample = [1, 0.82, 0.44, 0.24, 1, 0.05, 1]\n",
    "sample = 1, 0.36, 0.29, 0.14, 1, 0.00, 1\n",
    "sample_test_ds = sample_to_dataset(sample, test)\n",
    "result_predictor = model.predict(sample_test_ds)[0][0]\n",
    "print (result_predictor)\n",
    "\n",
    "\n",
    "predictions = model.predict(sample_test_ds)\n",
    "predictions"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.7363248\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.7363248]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "source": [
    "filename_samples = f'./dataset_100.csv'\n",
    "samples_df = pd.read_csv(filename_samples)\n",
    "\n",
    "sequence = ''\n",
    "s1 = ''\n",
    "s2 = ''\n",
    "\n",
    "for index, row in samples_df.iterrows():\n",
    "    if index != 56:\n",
    "        continue\n",
    "    sequence = row.sequence\n",
    "    s1 = row.s1\n",
    "    s2 = row.s2\n",
    "\n",
    "\n",
    "print (sequence)\n",
    "print (s1)\n",
    "print (s2)\n",
    "\n",
    "fc = RNA.fold_compound(sequence)\n",
    "en1 = fc.eval_structure(s1)\n",
    "en2 = fc.eval_structure(s2)\n",
    "\n",
    "def fp_call_sw(sequence, s1, s2, search_width):    \n",
    "    fp = findpath.findpath_single(sequence, s1, s2, search_width=search_width, mp=True)\n",
    "    result = fp.get_en()/100.0\n",
    "    path = fp.get_path()\n",
    "    # return result, path\n",
    "    return result, path\n",
    "\n",
    "\n",
    "sw = 1\n",
    "sw = 100\n",
    "\n",
    "r, p = fp_call_sw(sequence, s1, s2, sw)\n",
    "p = [(i[0], i[1]) for i in p]\n",
    "\n",
    "print_moves(sequence, s1, s2, p)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GGCUCAGUCGACGCUGCACCGCCUAGACCCGAUCGCUAAGAAGGAAAACAGUCAGAUGACCGAGUAUUCGAACGCUGCCGUAUCUCUCGCAUAUAACACA\n",
      "(((.((((....))))....))).....((..((.....)).))......(((....)))((((....((........)).....))))...........\n",
      "((..((((....))))..))..............((..(((.((...((.(.(((.((..(((....)))..))))).))).))))).))..........\n",
      "GGCUCAGUCGACGCUGCACCGCCUAGACCCGAUCGCUAAGAAGGAAAACAGUCAGAUGACCGAGUAUUCGAACGCUGCCGUAUCUCUCGCAUAUAACACA\n",
      "(((.((((....))))....))).....((..((.....)).))......(((....)))((((....((........)).....))))........... \u001b[93m[   0,    0 ]\u001b[0m -12.90\n",
      "\u001b[93m\u001b[1m.\u001b[0m((.((((....))))....))\u001b[93m\u001b[1m.\u001b[0m.....((..((.....)).))......(((....)))((((....((........)).....))))........... \u001b[93m[  -1,  -23 ]\u001b[0m  -9.40\n",
      ".\u001b[93m\u001b[1m.\u001b[0m(.((((....))))....)\u001b[93m\u001b[1m.\u001b[0m......((..((.....)).))......(((....)))((((....((........)).....))))........... \u001b[93m[  -2,  -22 ]\u001b[0m  -6.10\n",
      "..\u001b[93m\u001b[1m.\u001b[0m.((((....))))....\u001b[93m\u001b[1m.\u001b[0m.......((..((.....)).))......(((....)))((((....((........)).....))))........... \u001b[93m[  -3,  -21 ]\u001b[0m  -9.30\n",
      "\u001b[93m\u001b[1m(\u001b[0m...((((....))))...\u001b[93m\u001b[1m)\u001b[0m........((..((.....)).))......(((....)))((((....((........)).....))))........... \u001b[93m[   1,   20 ]\u001b[0m  -8.50\n",
      "(\u001b[93m\u001b[1m(\u001b[0m..((((....))))..\u001b[93m\u001b[1m)\u001b[0m)........((..((.....)).))......(((....)))((((....((........)).....))))........... \u001b[93m[   2,   19 ]\u001b[0m -12.80\n",
      "((..((((....))))..))........((..((.....)).))......(((....)))(\u001b[93m\u001b[1m.\u001b[0m((....((........)).....))\u001b[93m\u001b[1m.\u001b[0m)........... \u001b[93m[ -62,  -88 ]\u001b[0m  -6.80\n",
      "((..((((....))))..))........((..((.....)).))......(((....)))\u001b[93m\u001b[1m.\u001b[0m.((....((........)).....)).\u001b[93m\u001b[1m.\u001b[0m........... \u001b[93m[ -61,  -89 ]\u001b[0m  -7.50\n",
      "((..((((....))))..))........((..((.....)).))......(((....)))..\u001b[93m\u001b[1m.\u001b[0m(....((........)).....)\u001b[93m\u001b[1m.\u001b[0m............. \u001b[93m[ -63,  -87 ]\u001b[0m  -6.20\n",
      "((..((((....))))..))........((..((.....)).))......(((....)))...\u001b[93m\u001b[1m.\u001b[0m....((........)).....\u001b[93m\u001b[1m.\u001b[0m.............. \u001b[93m[ -64,  -86 ]\u001b[0m  -7.70\n",
      "((..((((....))))..))........((..((.....)).))......(((....)))........\u001b[93m\u001b[1m.\u001b[0m(........)\u001b[93m\u001b[1m.\u001b[0m.................... \u001b[93m[ -69,  -80 ]\u001b[0m  -6.10\n",
      "((..((((....))))..))........((..((.....)).))......(((....))).........\u001b[93m\u001b[1m.\u001b[0m........\u001b[93m\u001b[1m.\u001b[0m..................... \u001b[93m[ -70,  -79 ]\u001b[0m  -8.60\n",
      "((..((((....))))..))........((..((.....)).))......(((....))).\u001b[93m\u001b[1m(\u001b[0m......\u001b[93m\u001b[1m)\u001b[0m............................... \u001b[93m[  62,   69 ]\u001b[0m  -6.20\n",
      "((..((((....))))..))........((..((.....)).))......(((....)))\u001b[93m\u001b[1m(\u001b[0m(......)\u001b[93m\u001b[1m)\u001b[0m.............................. \u001b[93m[  61,   70 ]\u001b[0m  -8.60\n",
      "((..((((....))))..))........((..((.....)).))......(((....)))((\u001b[93m\u001b[1m(\u001b[0m....\u001b[93m\u001b[1m)\u001b[0m)).............................. \u001b[93m[  63,   68 ]\u001b[0m  -9.80\n",
      "((..((((....))))..))........((..((.....)).))......\u001b[93m\u001b[1m.\u001b[0m((....))\u001b[93m\u001b[1m.\u001b[0m(((....))).............................. \u001b[93m[ -51,  -60 ]\u001b[0m  -6.70\n",
      "((..((((....))))..))........((..((.....)).)).......\u001b[93m\u001b[1m.\u001b[0m(....)\u001b[93m\u001b[1m.\u001b[0m.(((....))).............................. \u001b[93m[ -52,  -59 ]\u001b[0m  -5.70\n",
      "((..((((....))))..))........((..((.....)).))........\u001b[93m\u001b[1m.\u001b[0m....\u001b[93m\u001b[1m.\u001b[0m..(((....))).............................. \u001b[93m[ -53,  -58 ]\u001b[0m  -8.30\n",
      "((..((((....))))..))........((..((.....)).)).............\u001b[93m\u001b[1m(\u001b[0m..(((....)))..\u001b[93m\u001b[1m)\u001b[0m........................... \u001b[93m[  58,   73 ]\u001b[0m  -7.10\n",
      "((..((((....))))..))........((..((.....)).))............\u001b[93m\u001b[1m(\u001b[0m(..(((....)))..)\u001b[93m\u001b[1m)\u001b[0m.......................... \u001b[93m[  57,   74 ]\u001b[0m  -7.10\n",
      "((..((((....))))..))........((..((.....)).))..........\u001b[93m\u001b[1m(\u001b[0m.((..(((....)))..))\u001b[93m\u001b[1m)\u001b[0m......................... \u001b[93m[  55,   75 ]\u001b[0m  -6.70\n",
      "((..((((....))))..))........((..((.....)).)).........\u001b[93m\u001b[1m(\u001b[0m(.((..(((....)))..)))\u001b[93m\u001b[1m)\u001b[0m........................ \u001b[93m[  54,   76 ]\u001b[0m  -8.10\n",
      "((..((((....))))..))........((..((.....)).))........\u001b[93m\u001b[1m(\u001b[0m((.((..(((....)))..))))\u001b[93m\u001b[1m)\u001b[0m....................... \u001b[93m[  53,   77 ]\u001b[0m -10.40\n",
      "((..((((....))))..))........\u001b[93m\u001b[1m.\u001b[0m(..((.....)).)\u001b[93m\u001b[1m.\u001b[0m........(((.((..(((....)))..)))))....................... \u001b[93m[ -29,  -44 ]\u001b[0m  -7.10\n",
      "((..((((....))))..)).........\u001b[93m\u001b[1m.\u001b[0m..((.....)).\u001b[93m\u001b[1m.\u001b[0m.........(((.((..(((....)))..)))))....................... \u001b[93m[ -30,  -43 ]\u001b[0m  -7.80\n",
      "((..((((....))))..))............\u001b[93m\u001b[1m.\u001b[0m(.....)\u001b[93m\u001b[1m.\u001b[0m...........(((.((..(((....)))..)))))....................... \u001b[93m[ -33,  -41 ]\u001b[0m  -6.60\n",
      "((..((((....))))..)).............\u001b[93m\u001b[1m.\u001b[0m.....\u001b[93m\u001b[1m.\u001b[0m............(((.((..(((....)))..)))))....................... \u001b[93m[ -34,  -40 ]\u001b[0m  -8.50\n",
      "((..((((....))))..))............................\u001b[93m\u001b[1m(\u001b[0m...(((.((..(((....)))..)))))..\u001b[93m\u001b[1m)\u001b[0m.................... \u001b[93m[  49,   80 ]\u001b[0m  -6.50\n",
      "((..((((....))))..))...........................\u001b[93m\u001b[1m(\u001b[0m(...(((.((..(((....)))..)))))..)\u001b[93m\u001b[1m)\u001b[0m................... \u001b[93m[  48,   81 ]\u001b[0m  -8.10\n",
      "((..((((....))))..))...........................((.\u001b[93m\u001b[1m(\u001b[0m.(((.((..(((....)))..))))).\u001b[93m\u001b[1m)\u001b[0m))................... \u001b[93m[  51,   79 ]\u001b[0m  -8.80\n",
      "((..((((....))))..))......................\u001b[93m\u001b[1m(\u001b[0m....((.(.(((.((..(((....)))..))))).)))..\u001b[93m\u001b[1m)\u001b[0m................ \u001b[93m[  43,   84 ]\u001b[0m  -5.40\n",
      "((..((((....))))..))......................(\u001b[93m\u001b[1m(\u001b[0m...((.(.(((.((..(((....)))..))))).))).\u001b[93m\u001b[1m)\u001b[0m)................ \u001b[93m[  44,   83 ]\u001b[0m  -7.10\n",
      "((..((((....))))..))...................\u001b[93m\u001b[1m(\u001b[0m..((...((.(.(((.((..(((....)))..))))).))).)).\u001b[93m\u001b[1m)\u001b[0m.............. \u001b[93m[  40,   86 ]\u001b[0m \u001b[91m\u001b[1m -4.80\u001b[0m\n",
      "((..((((....))))..))...................(\u001b[93m\u001b[1m(\u001b[0m.((...((.(.(((.((..(((....)))..))))).))).))\u001b[93m\u001b[1m)\u001b[0m).............. \u001b[93m[  41,   85 ]\u001b[0m  -7.80\n",
      "((..((((....))))..))..............\u001b[93m\u001b[1m(\u001b[0m....((.((...((.(.(((.((..(((....)))..))))).))).))))...\u001b[93m\u001b[1m)\u001b[0m.......... \u001b[93m[  35,   90 ]\u001b[0m  -5.60\n",
      "((..((((....))))..))..............(\u001b[93m\u001b[1m(\u001b[0m...((.((...((.(.(((.((..(((....)))..))))).))).))))..\u001b[93m\u001b[1m)\u001b[0m).......... \u001b[93m[  36,   89 ]\u001b[0m  -9.10\n",
      "((..((((....))))..))..............((..\u001b[93m\u001b[1m(\u001b[0m((.((...((.(.(((.((..(((....)))..))))).))).))))\u001b[93m\u001b[1m)\u001b[0m.)).......... \u001b[93m[  39,   87 ]\u001b[0m -10.80\n",
      "S:  -4.80 kcal/mol | B:   8.10 kcal/mol | E[start]:-12.90 E[end]:-10.80\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-4.8"
      ]
     },
     "metadata": {},
     "execution_count": 192
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "source": [
    "s = s1\n",
    "fc = RNA.fold_compound(sequence)\n",
    "pt = list(RNA.ptable(s))\n",
    "pt2 = list(RNA.ptable(s2))\n",
    "\n",
    "lasts = s\n",
    "lasti = 0\n",
    "lastj = 0\n",
    "last_move = (0,0)\n",
    "\n",
    "en_max = fc.eval_structure(s)\n",
    "\n",
    "found_moves = [(0, 0)]\n",
    "\n",
    "while True:\n",
    "\n",
    "    # check where we can go, compare with our best move. \n",
    "    pt = list(RNA.ptable(s))\n",
    "\n",
    "    # check available moves, save them, sort them    \n",
    "    avail_moves = []\n",
    "    ij_moves = []\n",
    "    found_pos = None\n",
    "\n",
    "    # collect moves from the generator function, then sort them.\n",
    "    for pos, (i,j) in enumerate(find_moves(pt, pt2)):    \n",
    "        next_en = fc.eval_move_pt(pt, i, j)\n",
    "        # mark where we found our move\n",
    "        found = (i,j) == (a,b)\n",
    "        avail_moves.append((i, j, next_en, found))\n",
    "        ij_moves.append((abs(i),abs(j)))\n",
    "\n",
    "\n",
    "    avail_moves.sort(key=lambda x: x[2])\n",
    "    found_list = [x[3] for x in avail_moves]\n",
    "    en_list = np.array([[x[2] for x in avail_moves]])\n",
    "    en_list_scaled = min_max_scaler.fit_transform(en_list.T).T[0]\n",
    "\n",
    "\n",
    "    # print (s, pt, avail_moves)\n",
    "    # print (en_list_scaled)\n",
    "\n",
    "    candidates = []\n",
    "\n",
    "    for pos, (i,j, en, found) in enumerate(avail_moves):\n",
    "\n",
    "        this_move = (abs(i), abs(j))\n",
    "        last_move = (lasti, lastj)\n",
    "\n",
    "        if lasti:\n",
    "            # print (this_move, last_move, ij_moves)\n",
    "            ijd, thisclose, lastclose = ij_distance(last_move, this_move, ij_moves)\n",
    "        else:\n",
    "            ijd, thisclose, lastclose = 0, 0, 0\n",
    "\n",
    "        cd = config_distance(pt, this_move)\n",
    "\n",
    "        en_scaled = en_list_scaled[pos]\n",
    "\n",
    "        # run the prediction\n",
    "        sample = 1, ijd, thisclose, lastclose, cd, en_list_scaled[pos], 1\n",
    "        sample_test_ds = sample_to_dataset(sample, test)\n",
    "        result_predictor = model.predict(sample_test_ds)[0][0]\n",
    "\n",
    "        candidates.append((i, j, en_scaled, result_predictor))\n",
    "        \n",
    "        # print (ijd, thisclose, lastclose, cd, en_scaled, result_predictor)\n",
    "\n",
    "    candidates.sort(key=lambda x: -x[3])\n",
    "    # candidates.sort(key=lambda x: x[2])\n",
    "\n",
    "    # chosen move\n",
    "    i = candidates[0][0]\n",
    "    j = candidates[0][1]\n",
    "\n",
    "\n",
    "\n",
    "    lasts = s\n",
    "    lasti = abs(i)\n",
    "    lastj = abs(j)\n",
    "    if i > 0:\n",
    "        s = s[:i-1] + \"(\" + s[i:j-1] + \")\" + s[j:]\n",
    "    if i < 0:\n",
    "        s = s[:-i-1] + \".\" + s[-i:-j-1] + \".\" + s[-j:]\n",
    "\n",
    "    en_new = fc.eval_structure(s)\n",
    "    if en_new > en_max:\n",
    "        en_max = en_new\n",
    "\n",
    "    # print (s, i, j, en_new, en_max)\n",
    "\n",
    "    found_moves.append((i, j))\n",
    "\n",
    "    if s==s2:\n",
    "        break\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "source": [
    "print_moves(sequence, s1, s2, found_moves)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GGCUCAGUCGACGCUGCACCGCCUAGACCCGAUCGCUAAGAAGGAAAACAGUCAGAUGACCGAGUAUUCGAACGCUGCCGUAUCUCUCGCAUAUAACACA\n",
      "(((.((((....))))....))).....((..((.....)).))......(((....)))((((....((........)).....))))........... \u001b[93m[   0,    0 ]\u001b[0m -12.90\n",
      "(((.((((....))))....))).....((..((.....)).))......(((....)))(((\u001b[93m\u001b[1m.\u001b[0m....((........)).....\u001b[93m\u001b[1m.\u001b[0m)))........... \u001b[93m[ -64,  -86 ]\u001b[0m  -9.30\n",
      "(((.((((....))))....))).....((..((.....)).))......(((....)))((\u001b[93m\u001b[1m.\u001b[0m.....((........))......\u001b[93m\u001b[1m.\u001b[0m))........... \u001b[93m[ -63,  -87 ]\u001b[0m  -7.40\n",
      "(((.((((....))))....))).....((..((.....)).))......(((....)))(\u001b[93m\u001b[1m.\u001b[0m......((........)).......\u001b[93m\u001b[1m.\u001b[0m)........... \u001b[93m[ -62,  -88 ]\u001b[0m  -4.90\n",
      "(((.((((....))))....))).....((..((.....)).))......(((....)))\u001b[93m\u001b[1m.\u001b[0m.......((........))........\u001b[93m\u001b[1m.\u001b[0m........... \u001b[93m[ -61,  -89 ]\u001b[0m  -7.80\n",
      "(((.((((....))))....))).....((..((.....)).))......(((....)))........\u001b[93m\u001b[1m.\u001b[0m(........)\u001b[93m\u001b[1m.\u001b[0m.................... \u001b[93m[ -69,  -80 ]\u001b[0m  -6.20\n",
      "(((.((((....))))....))).....((..((.....)).))......(((....))).........\u001b[93m\u001b[1m.\u001b[0m........\u001b[93m\u001b[1m.\u001b[0m..................... \u001b[93m[ -70,  -79 ]\u001b[0m  -8.70\n",
      "(((.((((....))))....))).....((..((.....)).))......(((....))).\u001b[93m\u001b[1m(\u001b[0m......\u001b[93m\u001b[1m)\u001b[0m............................... \u001b[93m[  62,   69 ]\u001b[0m  -6.30\n",
      "(((.((((....))))....))).....((..((.....)).))......(((....)))\u001b[93m\u001b[1m(\u001b[0m(......)\u001b[93m\u001b[1m)\u001b[0m.............................. \u001b[93m[  61,   70 ]\u001b[0m  -8.70\n",
      "(((.((((....))))....))).....((..((.....)).))......(((....)))((\u001b[93m\u001b[1m(\u001b[0m....\u001b[93m\u001b[1m)\u001b[0m)).............................. \u001b[93m[  63,   68 ]\u001b[0m  -9.90\n",
      "(((.((((....))))....))).....((..((.....)).))....\u001b[93m\u001b[1m(\u001b[0m.(((....)))(((....))).........\u001b[93m\u001b[1m)\u001b[0m.................... \u001b[93m[  49,   80 ]\u001b[0m  -5.90\n",
      "(((.((((....))))....))).....((..((.....)).))...\u001b[93m\u001b[1m(\u001b[0m(.(((....)))(((....))).........)\u001b[93m\u001b[1m)\u001b[0m................... \u001b[93m[  48,   81 ]\u001b[0m  -7.50\n",
      "(((.((((....))))....))).....((..\u001b[93m\u001b[1m.\u001b[0m(.....)\u001b[93m\u001b[1m.\u001b[0m.))...((.(((....)))(((....))).........))................... \u001b[93m[ -33,  -41 ]\u001b[0m  -5.50\n",
      "(((.((((....))))....))).....((...\u001b[93m\u001b[1m.\u001b[0m.....\u001b[93m\u001b[1m.\u001b[0m..))...((.(((....)))(((....))).........))................... \u001b[93m[ -34,  -40 ]\u001b[0m  -6.00\n",
      "(((.((((....))))....))).....((............))...((.\u001b[93m\u001b[1m.\u001b[0m((....))\u001b[93m\u001b[1m.\u001b[0m(((....))).........))................... \u001b[93m[ -51,  -60 ]\u001b[0m  -2.90\n",
      "(((.((((....))))....))).....((............))...((..\u001b[93m\u001b[1m.\u001b[0m(....)\u001b[93m\u001b[1m.\u001b[0m.(((....))).........))................... \u001b[93m[ -52,  -59 ]\u001b[0m  -1.90\n",
      "(((.((((....))))....))).....((............))...((...\u001b[93m\u001b[1m.\u001b[0m....\u001b[93m\u001b[1m.\u001b[0m..(((....))).........))................... \u001b[93m[ -53,  -58 ]\u001b[0m  -3.60\n",
      "(((.((((....))))....))).....((............))...((...\u001b[93m\u001b[1m(\u001b[0m.......(((....)))......\u001b[93m\u001b[1m)\u001b[0m..))................... \u001b[93m[  53,   77 ]\u001b[0m  -2.10\n",
      "(((.((((....))))....))).....((............))...((.\u001b[93m\u001b[1m(\u001b[0m.(.......(((....)))......).\u001b[93m\u001b[1m)\u001b[0m))................... \u001b[93m[  51,   79 ]\u001b[0m  -2.80\n",
      "(((.((((....))))....))).....((............))...((.(.(....\u001b[93m\u001b[1m(\u001b[0m..(((....)))..\u001b[93m\u001b[1m)\u001b[0m...).)))................... \u001b[93m[  58,   73 ]\u001b[0m  -2.30\n",
      "(((.((((....))))....))).....((............))...((.(.(...\u001b[93m\u001b[1m(\u001b[0m(..(((....)))..)\u001b[93m\u001b[1m)\u001b[0m..).)))................... \u001b[93m[  57,   74 ]\u001b[0m  -3.10\n",
      "(((.((((....))))....))).....((............))...((.(.(.\u001b[93m\u001b[1m(\u001b[0m.((..(((....)))..))\u001b[93m\u001b[1m)\u001b[0m.).)))................... \u001b[93m[  55,   75 ]\u001b[0m  -4.60\n",
      "(((.((((....))))....))).....((............))...((.(.(\u001b[93m\u001b[1m(\u001b[0m(.((..(((....)))..)))\u001b[93m\u001b[1m)\u001b[0m).)))................... \u001b[93m[  54,   76 ]\u001b[0m  -9.30\n",
      "((\u001b[93m\u001b[1m.\u001b[0m.((((....))))....\u001b[93m\u001b[1m.\u001b[0m)).....((............))...((.(.(((.((..(((....)))..))))).)))................... \u001b[93m[  -3,  -21 ]\u001b[0m  -5.80\n",
      "((..((((....)))).....)).....\u001b[93m\u001b[1m.\u001b[0m(............)\u001b[93m\u001b[1m.\u001b[0m...((.(.(((.((..(((....)))..))))).)))................... \u001b[93m[ -29,  -44 ]\u001b[0m  -2.50\n",
      "((..((((....)))).....))......\u001b[93m\u001b[1m.\u001b[0m............\u001b[93m\u001b[1m.\u001b[0m....((.(.(((.((..(((....)))..))))).)))................... \u001b[93m[ -30,  -43 ]\u001b[0m  -5.40\n",
      "((..((((....)))).....))...................\u001b[93m\u001b[1m(\u001b[0m....((.(.(((.((..(((....)))..))))).)))..\u001b[93m\u001b[1m)\u001b[0m................ \u001b[93m[  43,   84 ]\u001b[0m  -2.00\n",
      "((..((((....)))).....))...................(\u001b[93m\u001b[1m(\u001b[0m...((.(.(((.((..(((....)))..))))).))).\u001b[93m\u001b[1m)\u001b[0m)................ \u001b[93m[  44,   83 ]\u001b[0m  -3.70\n",
      "((..((((....)))).....))................\u001b[93m\u001b[1m(\u001b[0m..((...((.(.(((.((..(((....)))..))))).))).)).\u001b[93m\u001b[1m)\u001b[0m.............. \u001b[93m[  40,   86 ]\u001b[0m \u001b[91m\u001b[1m -1.40\u001b[0m\n",
      "((..((((....)))).....))................(\u001b[93m\u001b[1m(\u001b[0m.((...((.(.(((.((..(((....)))..))))).))).))\u001b[93m\u001b[1m)\u001b[0m).............. \u001b[93m[  41,   85 ]\u001b[0m  -4.40\n",
      "((..((((....)))).....))...............\u001b[93m\u001b[1m(\u001b[0m((.((...((.(.(((.((..(((....)))..))))).))).))))\u001b[93m\u001b[1m)\u001b[0m............. \u001b[93m[  39,   87 ]\u001b[0m  -5.70\n",
      "((..((((....)))).....))...........\u001b[93m\u001b[1m(\u001b[0m...(((.((...((.(.(((.((..(((....)))..))))).))).)))))..\u001b[93m\u001b[1m)\u001b[0m.......... \u001b[93m[  35,   90 ]\u001b[0m  -3.70\n",
      "((..((((....)))).....))...........(\u001b[93m\u001b[1m(\u001b[0m..(((.((...((.(.(((.((..(((....)))..))))).))).))))).\u001b[93m\u001b[1m)\u001b[0m).......... \u001b[93m[  36,   89 ]\u001b[0m  -7.40\n",
      "\u001b[93m\u001b[1m.\u001b[0m(..((((....)))).....)\u001b[93m\u001b[1m.\u001b[0m...........((..(((.((...((.(.(((.((..(((....)))..))))).))).))))).)).......... \u001b[93m[  -1,  -23 ]\u001b[0m  -3.90\n",
      ".\u001b[93m\u001b[1m.\u001b[0m..((((....)))).....\u001b[93m\u001b[1m.\u001b[0m............((..(((.((...((.(.(((.((..(((....)))..))))).))).))))).)).......... \u001b[93m[  -2,  -22 ]\u001b[0m  -7.30\n",
      ".\u001b[93m\u001b[1m(\u001b[0m..((((....))))..\u001b[93m\u001b[1m)\u001b[0m...............((..(((.((...((.(.(((.((..(((....)))..))))).))).))))).)).......... \u001b[93m[   2,   19 ]\u001b[0m  -6.80\n",
      "\u001b[93m\u001b[1m(\u001b[0m(..((((....))))..)\u001b[93m\u001b[1m)\u001b[0m..............((..(((.((...((.(.(((.((..(((....)))..))))).))).))))).)).......... \u001b[93m[   1,   20 ]\u001b[0m -10.80\n",
      "S:  -1.40 kcal/mol | B:  11.50 kcal/mol | E[start]:-12.90 E[end]:-10.80\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-1.4"
      ]
     },
     "metadata": {},
     "execution_count": 191
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}