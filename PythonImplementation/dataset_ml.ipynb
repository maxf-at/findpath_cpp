{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "source": [
    "import RNA\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import SVG, display\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "import difflib\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from helper import print_moves\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('../')\n",
    "from pretty_print_path import print_moves\n",
    "import findpath_librna\n",
    "import findpath\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "source": [
    "input_file = \"dataset_104_train.csv\"\n",
    "input_file2 = \"dataset_104_vec_train.csv\"\n",
    "\n",
    "dataframe = pd.read_csv(input_file, index_col=0)\n",
    "# dataframe['target'] = np.where(dataframe[\"3\"]==1, 1, 0)\n",
    "dataframe = dataframe.drop(labels=\"s\", axis=1)\n",
    "dataframe = dataframe.drop(labels=\"i\", axis=1)\n",
    "dataframe = dataframe.drop(labels=\"j\", axis=1)\n",
    "# dataframe = dataframe.drop(labels=\"found\", axis=1)\n",
    "\n",
    "# dataframe\n",
    "\n",
    "vec_dataframe = pd.read_csv(input_file2, index_col=0)\n",
    "# vec_dataframe\n",
    "dataframe = pd.concat([dataframe, vec_dataframe], axis=1)\n",
    "\n",
    "dataframe"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      target       ijd  thisclose  lastclose  cd    en_pos  i_shift  j_shift  \\\n",
       "0          1  0.363636   0.285714   0.142857   1  0.000000        0        0   \n",
       "1          1  0.181818   0.333333   0.142857   1  0.034483        0        1   \n",
       "2          0  0.818182   0.238095   0.142857   1  0.034483        0        0   \n",
       "3          0  0.954545   0.047619   0.142857   1  0.103448        0        0   \n",
       "4          1  0.636364   0.333333   0.142857   1  0.206897        0        0   \n",
       "...      ...       ...        ...        ...  ..       ...      ...      ...   \n",
       "4926       1  0.166667   1.000000   0.000000   0  1.000000        1        0   \n",
       "4927       1  0.200000   1.000000   1.000000   0  0.000000        0        0   \n",
       "4928       1  0.200000   1.000000   1.000000   1  0.663043        0        1   \n",
       "4929       1  0.400000   1.000000   1.000000   0  0.717391        0        1   \n",
       "4930       1  0.600000   1.000000   1.000000   0  1.000000        1        0   \n",
       "\n",
       "      insert_or_delete         0  ...            10            11  \\\n",
       "0                    0  0.379594  ...  6.618096e-04  3.768748e-02   \n",
       "1                    0  0.379594  ...  6.618096e-04  3.768748e-02   \n",
       "2                    0  0.379594  ...  6.618096e-04  3.768748e-02   \n",
       "3                    0  0.379594  ...  6.618096e-04  3.768748e-02   \n",
       "4                    0  0.379594  ...  6.618096e-04  3.768748e-02   \n",
       "...                ...       ...  ...           ...           ...   \n",
       "4926                 1  0.513713  ...  7.875288e-28  2.585954e-14   \n",
       "4927                 1  0.168993  ...  1.464808e-21  8.502979e-15   \n",
       "4928                 1  0.168993  ...  1.464808e-21  8.502979e-15   \n",
       "4929                 1  0.168993  ...  1.464808e-21  8.502979e-15   \n",
       "4930                 1  0.168993  ...  1.464808e-21  8.502979e-15   \n",
       "\n",
       "                12        13        14        15        16            17  \\\n",
       "0     3.131309e-01  0.379594  0.067139  0.001733  0.000007  3.583778e-09   \n",
       "1     3.131309e-01  0.379594  0.067139  0.001733  0.000007  3.583778e-09   \n",
       "2     3.131309e-01  0.379594  0.067139  0.001733  0.000007  3.583778e-09   \n",
       "3     3.131309e-01  0.379594  0.067139  0.001733  0.000007  3.583778e-09   \n",
       "4     3.131309e-01  0.379594  0.067139  0.001733  0.000007  3.583778e-09   \n",
       "...            ...       ...       ...       ...       ...           ...   \n",
       "4926  7.441132e-09  0.000005  0.000524  0.009191  0.028921  4.183780e-02   \n",
       "4927  2.446749e-09  0.000002  0.000172  0.003022  0.009516  1.548947e-02   \n",
       "4928  2.446749e-09  0.000002  0.000172  0.003022  0.009516  1.548947e-02   \n",
       "4929  2.446749e-09  0.000002  0.000172  0.003022  0.009516  1.548947e-02   \n",
       "4930  2.446749e-09  0.000002  0.000172  0.003022  0.009516  1.548947e-02   \n",
       "\n",
       "                18            19  \n",
       "0     2.872489e-13  3.359235e-18  \n",
       "1     2.872489e-13  3.359235e-18  \n",
       "2     2.872489e-13  3.359235e-18  \n",
       "3     2.872489e-13  3.359235e-18  \n",
       "4     2.872489e-13  3.359235e-18  \n",
       "...            ...           ...  \n",
       "4926  8.569017e-02  1.100865e-01  \n",
       "4927  9.531556e-02  4.157920e-01  \n",
       "4928  9.531556e-02  4.157920e-01  \n",
       "4929  9.531556e-02  4.157920e-01  \n",
       "4930  9.531556e-02  4.157920e-01  \n",
       "\n",
       "[4931 rows x 29 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ijd</th>\n",
       "      <th>thisclose</th>\n",
       "      <th>lastclose</th>\n",
       "      <th>cd</th>\n",
       "      <th>en_pos</th>\n",
       "      <th>i_shift</th>\n",
       "      <th>j_shift</th>\n",
       "      <th>insert_or_delete</th>\n",
       "      <th>0</th>\n",
       "      <th>...</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.379594</td>\n",
       "      <td>...</td>\n",
       "      <td>6.618096e-04</td>\n",
       "      <td>3.768748e-02</td>\n",
       "      <td>3.131309e-01</td>\n",
       "      <td>0.379594</td>\n",
       "      <td>0.067139</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>3.583778e-09</td>\n",
       "      <td>2.872489e-13</td>\n",
       "      <td>3.359235e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.379594</td>\n",
       "      <td>...</td>\n",
       "      <td>6.618096e-04</td>\n",
       "      <td>3.768748e-02</td>\n",
       "      <td>3.131309e-01</td>\n",
       "      <td>0.379594</td>\n",
       "      <td>0.067139</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>3.583778e-09</td>\n",
       "      <td>2.872489e-13</td>\n",
       "      <td>3.359235e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.379594</td>\n",
       "      <td>...</td>\n",
       "      <td>6.618096e-04</td>\n",
       "      <td>3.768748e-02</td>\n",
       "      <td>3.131309e-01</td>\n",
       "      <td>0.379594</td>\n",
       "      <td>0.067139</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>3.583778e-09</td>\n",
       "      <td>2.872489e-13</td>\n",
       "      <td>3.359235e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.379594</td>\n",
       "      <td>...</td>\n",
       "      <td>6.618096e-04</td>\n",
       "      <td>3.768748e-02</td>\n",
       "      <td>3.131309e-01</td>\n",
       "      <td>0.379594</td>\n",
       "      <td>0.067139</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>3.583778e-09</td>\n",
       "      <td>2.872489e-13</td>\n",
       "      <td>3.359235e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.379594</td>\n",
       "      <td>...</td>\n",
       "      <td>6.618096e-04</td>\n",
       "      <td>3.768748e-02</td>\n",
       "      <td>3.131309e-01</td>\n",
       "      <td>0.379594</td>\n",
       "      <td>0.067139</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>3.583778e-09</td>\n",
       "      <td>2.872489e-13</td>\n",
       "      <td>3.359235e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4926</th>\n",
       "      <td>1</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.513713</td>\n",
       "      <td>...</td>\n",
       "      <td>7.875288e-28</td>\n",
       "      <td>2.585954e-14</td>\n",
       "      <td>7.441132e-09</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.009191</td>\n",
       "      <td>0.028921</td>\n",
       "      <td>4.183780e-02</td>\n",
       "      <td>8.569017e-02</td>\n",
       "      <td>1.100865e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4927</th>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.168993</td>\n",
       "      <td>...</td>\n",
       "      <td>1.464808e-21</td>\n",
       "      <td>8.502979e-15</td>\n",
       "      <td>2.446749e-09</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.003022</td>\n",
       "      <td>0.009516</td>\n",
       "      <td>1.548947e-02</td>\n",
       "      <td>9.531556e-02</td>\n",
       "      <td>4.157920e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4928</th>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.663043</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.168993</td>\n",
       "      <td>...</td>\n",
       "      <td>1.464808e-21</td>\n",
       "      <td>8.502979e-15</td>\n",
       "      <td>2.446749e-09</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.003022</td>\n",
       "      <td>0.009516</td>\n",
       "      <td>1.548947e-02</td>\n",
       "      <td>9.531556e-02</td>\n",
       "      <td>4.157920e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4929</th>\n",
       "      <td>1</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.717391</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.168993</td>\n",
       "      <td>...</td>\n",
       "      <td>1.464808e-21</td>\n",
       "      <td>8.502979e-15</td>\n",
       "      <td>2.446749e-09</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.003022</td>\n",
       "      <td>0.009516</td>\n",
       "      <td>1.548947e-02</td>\n",
       "      <td>9.531556e-02</td>\n",
       "      <td>4.157920e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4930</th>\n",
       "      <td>1</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.168993</td>\n",
       "      <td>...</td>\n",
       "      <td>1.464808e-21</td>\n",
       "      <td>8.502979e-15</td>\n",
       "      <td>2.446749e-09</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.003022</td>\n",
       "      <td>0.009516</td>\n",
       "      <td>1.548947e-02</td>\n",
       "      <td>9.531556e-02</td>\n",
       "      <td>4.157920e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4931 rows Ã— 29 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 284
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "source": [
    "train, test = train_test_split(dataframe, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.2)\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3155 train examples\n",
      "789 validation examples\n",
      "987 test examples\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "source": [
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop('target')\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds\n",
    "\n",
    "batch_size = 5 # A small batch sized is used for demonstration purposes\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "# ## Understand the input pipeline\n",
    "# # Now that we have created the input pipeline, let's call it to see the format of the data it returns. We have used a small batch size to keep the output readable.\n",
    "\n",
    "# for feature_batch, label_batch in train_ds.take(1):\n",
    "#   print('Every feature:', list(feature_batch.keys()))\n",
    "#   print('A batch of ages:', feature_batch['4'])\n",
    "#   print('A batch of targets:', label_batch )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "source": [
    "# We will use this batch to demonstrate several types of feature columns\n",
    "example_batch = next(iter(train_ds))[0]\n",
    "\n",
    "# A utility method to create a feature column\n",
    "# and to transform a batch of data\n",
    "def demo(feature_column):\n",
    "  feature_layer = layers.DenseFeatures(feature_column)\n",
    "  print(feature_layer(example_batch).numpy())\n",
    "\n",
    "# # numeric columns\n",
    "photo_count = feature_column.numeric_column('4')\n",
    "demo(photo_count)\n",
    "photo_count2 = feature_column.numeric_column('5')\n",
    "\n",
    "crossed_feature = feature_column.crossed_column(['4', '1'], hash_bucket_size=10)\n",
    "demo(feature_column.indicator_column(crossed_feature))\n",
    "\n",
    "# # column 7 is a catigorical column\n",
    "# animal_type = feature_column.categorical_column_with_vocabulary_list(\n",
    "#       '7', [0, 1])\n",
    "# animal_type_one_hot = feature_column.indicator_column(animal_type)\n",
    "# demo(animal_type_one_hot)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[3.1255597e-01]\n",
      " [2.9405925e-04]\n",
      " [4.9473299e-03]\n",
      " [5.6313086e-01]\n",
      " [4.0208951e-01]]\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Choose which columns to use\n",
    "We have seen how to use several types of feature columns. Now we will use them to train a model. The goal of this tutorial is to show you the complete code (e.g. mechanics) needed to work with feature columns. We have selected a few columns to train our model below arbitrarily.\n",
    "\n",
    "Key point: If your aim is to build an accurate model, try a larger dataset of your own, and think carefully about which features are the most meaningful to include, and how they should be represented."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "source": [
    "feature_columns = []\n",
    "\n",
    "# ijd\tthisclose\tlastclose\tcd\ten_pos\n",
    "\n",
    "# numeric cols\n",
    "for header in ['ijd',\t'thisclose',\t'lastclose',\t'en_pos', 'i_shift',\t'j_shift',\t'insert_or_delete']:\n",
    "  feature_columns.append(feature_column.numeric_column(header))\n",
    "\n",
    "# numeric cols\n",
    "for header in ['0',\t'1',\t'2',\t'3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19']:\n",
    "  feature_columns.append(feature_column.numeric_column(header))\n",
    "\n",
    "# categoriacal indicator_columns\n",
    "indicator_column_names = ['cd']\n",
    "for col_name in indicator_column_names:\n",
    "  categorical_column = feature_column.categorical_column_with_vocabulary_list(\n",
    "      col_name, dataframe[col_name].unique())\n",
    "  indicator_column = feature_column.indicator_column(categorical_column)\n",
    "  feature_columns.append(indicator_column)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "source": [
    "# only use lowest energy as indicator\n",
    "\n",
    "feature_columns = []\n",
    "\n",
    "# numeric cols\n",
    "for header in ['en_pos']:\n",
    "  feature_columns.append(feature_column.numeric_column(header))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "source": [
    "feature_columns"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[NumericColumn(key='ijd', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='thisclose', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='lastclose', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='en_pos', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='i_shift', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='j_shift', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='insert_or_delete', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='0', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='1', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='2', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='3', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='4', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='5', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='6', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='7', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='8', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='9', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='10', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='11', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='12', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='13', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='14', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='15', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='16', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='17', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='18', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='19', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='cd', vocabulary_list=(1, 0), dtype=tf.int64, default_value=-1, num_oov_buckets=0))]"
      ]
     },
     "metadata": {},
     "execution_count": 289
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a feature layer\n",
    "Now that we have defined our feature columns, we will use a [DenseFeatures](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/DenseFeatures) layer to input them to our Keras model.\n",
    "\n",
    "Earlier, we used a small batch size to demonstrate how feature columns worked. We create a new input pipeline with a larger batch size."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "source": [
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "\n",
    "batch_size = 32\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "source": [
    "epochs = 50\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  feature_layer,\n",
    "  # layers.Dense(128, activation='relu'),\n",
    "  # layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(256, activation='relu'),\n",
    "  layers.Dense(256, activation='relu'),\n",
    "  layers.Dropout(.1),\n",
    "  layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_ds,\n",
    "          validation_data=val_ds,\n",
    "          epochs=epochs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'ijd': <tf.Tensor 'ExpandDims_23:0' shape=(None, 1) dtype=float64>, 'thisclose': <tf.Tensor 'ExpandDims_27:0' shape=(None, 1) dtype=float64>, 'lastclose': <tf.Tensor 'ExpandDims_26:0' shape=(None, 1) dtype=float64>, 'cd': <tf.Tensor 'ExpandDims_20:0' shape=(None, 1) dtype=int64>, 'en_pos': <tf.Tensor 'ExpandDims_21:0' shape=(None, 1) dtype=float64>, 'i_shift': <tf.Tensor 'ExpandDims_22:0' shape=(None, 1) dtype=int64>, 'j_shift': <tf.Tensor 'ExpandDims_25:0' shape=(None, 1) dtype=int64>, 'insert_or_delete': <tf.Tensor 'ExpandDims_24:0' shape=(None, 1) dtype=int64>, '0': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float64>, '1': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=float64>, '2': <tf.Tensor 'ExpandDims_12:0' shape=(None, 1) dtype=float64>, '3': <tf.Tensor 'ExpandDims_13:0' shape=(None, 1) dtype=float64>, '4': <tf.Tensor 'ExpandDims_14:0' shape=(None, 1) dtype=float64>, '5': <tf.Tensor 'ExpandDims_15:0' shape=(None, 1) dtype=float64>, '6': <tf.Tensor 'ExpandDims_16:0' shape=(None, 1) dtype=float64>, '7': <tf.Tensor 'ExpandDims_17:0' shape=(None, 1) dtype=float64>, '8': <tf.Tensor 'ExpandDims_18:0' shape=(None, 1) dtype=float64>, '9': <tf.Tensor 'ExpandDims_19:0' shape=(None, 1) dtype=float64>, '10': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=float64>, '11': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=float64>, '12': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=float64>, '13': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=float64>, '14': <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=float64>, '15': <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=float64>, '16': <tf.Tensor 'ExpandDims_8:0' shape=(None, 1) dtype=float64>, '17': <tf.Tensor 'ExpandDims_9:0' shape=(None, 1) dtype=float64>, '18': <tf.Tensor 'ExpandDims_10:0' shape=(None, 1) dtype=float64>, '19': <tf.Tensor 'ExpandDims_11:0' shape=(None, 1) dtype=float64>}\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'ijd': <tf.Tensor 'ExpandDims_23:0' shape=(None, 1) dtype=float64>, 'thisclose': <tf.Tensor 'ExpandDims_27:0' shape=(None, 1) dtype=float64>, 'lastclose': <tf.Tensor 'ExpandDims_26:0' shape=(None, 1) dtype=float64>, 'cd': <tf.Tensor 'ExpandDims_20:0' shape=(None, 1) dtype=int64>, 'en_pos': <tf.Tensor 'ExpandDims_21:0' shape=(None, 1) dtype=float64>, 'i_shift': <tf.Tensor 'ExpandDims_22:0' shape=(None, 1) dtype=int64>, 'j_shift': <tf.Tensor 'ExpandDims_25:0' shape=(None, 1) dtype=int64>, 'insert_or_delete': <tf.Tensor 'ExpandDims_24:0' shape=(None, 1) dtype=int64>, '0': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float64>, '1': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=float64>, '2': <tf.Tensor 'ExpandDims_12:0' shape=(None, 1) dtype=float64>, '3': <tf.Tensor 'ExpandDims_13:0' shape=(None, 1) dtype=float64>, '4': <tf.Tensor 'ExpandDims_14:0' shape=(None, 1) dtype=float64>, '5': <tf.Tensor 'ExpandDims_15:0' shape=(None, 1) dtype=float64>, '6': <tf.Tensor 'ExpandDims_16:0' shape=(None, 1) dtype=float64>, '7': <tf.Tensor 'ExpandDims_17:0' shape=(None, 1) dtype=float64>, '8': <tf.Tensor 'ExpandDims_18:0' shape=(None, 1) dtype=float64>, '9': <tf.Tensor 'ExpandDims_19:0' shape=(None, 1) dtype=float64>, '10': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=float64>, '11': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=float64>, '12': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=float64>, '13': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=float64>, '14': <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=float64>, '15': <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=float64>, '16': <tf.Tensor 'ExpandDims_8:0' shape=(None, 1) dtype=float64>, '17': <tf.Tensor 'ExpandDims_9:0' shape=(None, 1) dtype=float64>, '18': <tf.Tensor 'ExpandDims_10:0' shape=(None, 1) dtype=float64>, '19': <tf.Tensor 'ExpandDims_11:0' shape=(None, 1) dtype=float64>}\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'ijd': <tf.Tensor 'ExpandDims_23:0' shape=(None, 1) dtype=float64>, 'thisclose': <tf.Tensor 'ExpandDims_27:0' shape=(None, 1) dtype=float64>, 'lastclose': <tf.Tensor 'ExpandDims_26:0' shape=(None, 1) dtype=float64>, 'cd': <tf.Tensor 'ExpandDims_20:0' shape=(None, 1) dtype=int64>, 'en_pos': <tf.Tensor 'ExpandDims_21:0' shape=(None, 1) dtype=float64>, 'i_shift': <tf.Tensor 'ExpandDims_22:0' shape=(None, 1) dtype=int64>, 'j_shift': <tf.Tensor 'ExpandDims_25:0' shape=(None, 1) dtype=int64>, 'insert_or_delete': <tf.Tensor 'ExpandDims_24:0' shape=(None, 1) dtype=int64>, '0': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float64>, '1': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=float64>, '2': <tf.Tensor 'ExpandDims_12:0' shape=(None, 1) dtype=float64>, '3': <tf.Tensor 'ExpandDims_13:0' shape=(None, 1) dtype=float64>, '4': <tf.Tensor 'ExpandDims_14:0' shape=(None, 1) dtype=float64>, '5': <tf.Tensor 'ExpandDims_15:0' shape=(None, 1) dtype=float64>, '6': <tf.Tensor 'ExpandDims_16:0' shape=(None, 1) dtype=float64>, '7': <tf.Tensor 'ExpandDims_17:0' shape=(None, 1) dtype=float64>, '8': <tf.Tensor 'ExpandDims_18:0' shape=(None, 1) dtype=float64>, '9': <tf.Tensor 'ExpandDims_19:0' shape=(None, 1) dtype=float64>, '10': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=float64>, '11': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=float64>, '12': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=float64>, '13': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=float64>, '14': <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=float64>, '15': <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=float64>, '16': <tf.Tensor 'ExpandDims_8:0' shape=(None, 1) dtype=float64>, '17': <tf.Tensor 'ExpandDims_9:0' shape=(None, 1) dtype=float64>, '18': <tf.Tensor 'ExpandDims_10:0' shape=(None, 1) dtype=float64>, '19': <tf.Tensor 'ExpandDims_11:0' shape=(None, 1) dtype=float64>}\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'ijd': <tf.Tensor 'ExpandDims_23:0' shape=(None, 1) dtype=float64>, 'thisclose': <tf.Tensor 'ExpandDims_27:0' shape=(None, 1) dtype=float64>, 'lastclose': <tf.Tensor 'ExpandDims_26:0' shape=(None, 1) dtype=float64>, 'cd': <tf.Tensor 'ExpandDims_20:0' shape=(None, 1) dtype=int64>, 'en_pos': <tf.Tensor 'ExpandDims_21:0' shape=(None, 1) dtype=float64>, 'i_shift': <tf.Tensor 'ExpandDims_22:0' shape=(None, 1) dtype=int64>, 'j_shift': <tf.Tensor 'ExpandDims_25:0' shape=(None, 1) dtype=int64>, 'insert_or_delete': <tf.Tensor 'ExpandDims_24:0' shape=(None, 1) dtype=int64>, '0': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float64>, '1': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=float64>, '2': <tf.Tensor 'ExpandDims_12:0' shape=(None, 1) dtype=float64>, '3': <tf.Tensor 'ExpandDims_13:0' shape=(None, 1) dtype=float64>, '4': <tf.Tensor 'ExpandDims_14:0' shape=(None, 1) dtype=float64>, '5': <tf.Tensor 'ExpandDims_15:0' shape=(None, 1) dtype=float64>, '6': <tf.Tensor 'ExpandDims_16:0' shape=(None, 1) dtype=float64>, '7': <tf.Tensor 'ExpandDims_17:0' shape=(None, 1) dtype=float64>, '8': <tf.Tensor 'ExpandDims_18:0' shape=(None, 1) dtype=float64>, '9': <tf.Tensor 'ExpandDims_19:0' shape=(None, 1) dtype=float64>, '10': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=float64>, '11': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=float64>, '12': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=float64>, '13': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=float64>, '14': <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=float64>, '15': <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=float64>, '16': <tf.Tensor 'ExpandDims_8:0' shape=(None, 1) dtype=float64>, '17': <tf.Tensor 'ExpandDims_9:0' shape=(None, 1) dtype=float64>, '18': <tf.Tensor 'ExpandDims_10:0' shape=(None, 1) dtype=float64>, '19': <tf.Tensor 'ExpandDims_11:0' shape=(None, 1) dtype=float64>}\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "77/99 [======================>.......] - ETA: 0s - loss: 0.5481 - accuracy: 0.6928WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'ijd': <tf.Tensor 'ExpandDims_23:0' shape=(None, 1) dtype=float64>, 'thisclose': <tf.Tensor 'ExpandDims_27:0' shape=(None, 1) dtype=float64>, 'lastclose': <tf.Tensor 'ExpandDims_26:0' shape=(None, 1) dtype=float64>, 'cd': <tf.Tensor 'ExpandDims_20:0' shape=(None, 1) dtype=int64>, 'en_pos': <tf.Tensor 'ExpandDims_21:0' shape=(None, 1) dtype=float64>, 'i_shift': <tf.Tensor 'ExpandDims_22:0' shape=(None, 1) dtype=int64>, 'j_shift': <tf.Tensor 'ExpandDims_25:0' shape=(None, 1) dtype=int64>, 'insert_or_delete': <tf.Tensor 'ExpandDims_24:0' shape=(None, 1) dtype=int64>, '0': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float64>, '1': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=float64>, '2': <tf.Tensor 'ExpandDims_12:0' shape=(None, 1) dtype=float64>, '3': <tf.Tensor 'ExpandDims_13:0' shape=(None, 1) dtype=float64>, '4': <tf.Tensor 'ExpandDims_14:0' shape=(None, 1) dtype=float64>, '5': <tf.Tensor 'ExpandDims_15:0' shape=(None, 1) dtype=float64>, '6': <tf.Tensor 'ExpandDims_16:0' shape=(None, 1) dtype=float64>, '7': <tf.Tensor 'ExpandDims_17:0' shape=(None, 1) dtype=float64>, '8': <tf.Tensor 'ExpandDims_18:0' shape=(None, 1) dtype=float64>, '9': <tf.Tensor 'ExpandDims_19:0' shape=(None, 1) dtype=float64>, '10': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=float64>, '11': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=float64>, '12': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=float64>, '13': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=float64>, '14': <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=float64>, '15': <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=float64>, '16': <tf.Tensor 'ExpandDims_8:0' shape=(None, 1) dtype=float64>, '17': <tf.Tensor 'ExpandDims_9:0' shape=(None, 1) dtype=float64>, '18': <tf.Tensor 'ExpandDims_10:0' shape=(None, 1) dtype=float64>, '19': <tf.Tensor 'ExpandDims_11:0' shape=(None, 1) dtype=float64>}\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'ijd': <tf.Tensor 'ExpandDims_23:0' shape=(None, 1) dtype=float64>, 'thisclose': <tf.Tensor 'ExpandDims_27:0' shape=(None, 1) dtype=float64>, 'lastclose': <tf.Tensor 'ExpandDims_26:0' shape=(None, 1) dtype=float64>, 'cd': <tf.Tensor 'ExpandDims_20:0' shape=(None, 1) dtype=int64>, 'en_pos': <tf.Tensor 'ExpandDims_21:0' shape=(None, 1) dtype=float64>, 'i_shift': <tf.Tensor 'ExpandDims_22:0' shape=(None, 1) dtype=int64>, 'j_shift': <tf.Tensor 'ExpandDims_25:0' shape=(None, 1) dtype=int64>, 'insert_or_delete': <tf.Tensor 'ExpandDims_24:0' shape=(None, 1) dtype=int64>, '0': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float64>, '1': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=float64>, '2': <tf.Tensor 'ExpandDims_12:0' shape=(None, 1) dtype=float64>, '3': <tf.Tensor 'ExpandDims_13:0' shape=(None, 1) dtype=float64>, '4': <tf.Tensor 'ExpandDims_14:0' shape=(None, 1) dtype=float64>, '5': <tf.Tensor 'ExpandDims_15:0' shape=(None, 1) dtype=float64>, '6': <tf.Tensor 'ExpandDims_16:0' shape=(None, 1) dtype=float64>, '7': <tf.Tensor 'ExpandDims_17:0' shape=(None, 1) dtype=float64>, '8': <tf.Tensor 'ExpandDims_18:0' shape=(None, 1) dtype=float64>, '9': <tf.Tensor 'ExpandDims_19:0' shape=(None, 1) dtype=float64>, '10': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=float64>, '11': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=float64>, '12': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=float64>, '13': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=float64>, '14': <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=float64>, '15': <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=float64>, '16': <tf.Tensor 'ExpandDims_8:0' shape=(None, 1) dtype=float64>, '17': <tf.Tensor 'ExpandDims_9:0' shape=(None, 1) dtype=float64>, '18': <tf.Tensor 'ExpandDims_10:0' shape=(None, 1) dtype=float64>, '19': <tf.Tensor 'ExpandDims_11:0' shape=(None, 1) dtype=float64>}\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "99/99 [==============================] - 2s 6ms/step - loss: 0.5398 - accuracy: 0.7036 - val_loss: 0.5220 - val_accuracy: 0.7224\n",
      "Epoch 2/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.4851 - accuracy: 0.7569 - val_loss: 0.4943 - val_accuracy: 0.7579\n",
      "Epoch 3/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.4462 - accuracy: 0.7803 - val_loss: 0.4583 - val_accuracy: 0.7643\n",
      "Epoch 4/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.4127 - accuracy: 0.7981 - val_loss: 0.4376 - val_accuracy: 0.7630\n",
      "Epoch 5/50\n",
      "99/99 [==============================] - 0s 3ms/step - loss: 0.3927 - accuracy: 0.8079 - val_loss: 0.4334 - val_accuracy: 0.7693\n",
      "Epoch 6/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.3605 - accuracy: 0.8273 - val_loss: 0.4024 - val_accuracy: 0.8010\n",
      "Epoch 7/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.3367 - accuracy: 0.8466 - val_loss: 0.4055 - val_accuracy: 0.7909\n",
      "Epoch 8/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.3209 - accuracy: 0.8542 - val_loss: 0.3627 - val_accuracy: 0.8276\n",
      "Epoch 9/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.2970 - accuracy: 0.8640 - val_loss: 0.3615 - val_accuracy: 0.8340\n",
      "Epoch 10/50\n",
      "99/99 [==============================] - 0s 3ms/step - loss: 0.2778 - accuracy: 0.8754 - val_loss: 0.3670 - val_accuracy: 0.8378\n",
      "Epoch 11/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.2685 - accuracy: 0.8799 - val_loss: 0.3629 - val_accuracy: 0.8390\n",
      "Epoch 12/50\n",
      "99/99 [==============================] - 0s 3ms/step - loss: 0.2555 - accuracy: 0.8875 - val_loss: 0.3692 - val_accuracy: 0.8289\n",
      "Epoch 13/50\n",
      "99/99 [==============================] - 0s 3ms/step - loss: 0.2488 - accuracy: 0.8868 - val_loss: 0.3442 - val_accuracy: 0.8492\n",
      "Epoch 14/50\n",
      "99/99 [==============================] - 0s 3ms/step - loss: 0.2290 - accuracy: 0.8986 - val_loss: 0.3757 - val_accuracy: 0.8378\n",
      "Epoch 15/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.2257 - accuracy: 0.8964 - val_loss: 0.3728 - val_accuracy: 0.8466\n",
      "Epoch 16/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.2192 - accuracy: 0.9052 - val_loss: 0.3978 - val_accuracy: 0.8454\n",
      "Epoch 17/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.2036 - accuracy: 0.9071 - val_loss: 0.3659 - val_accuracy: 0.8517\n",
      "Epoch 18/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.1887 - accuracy: 0.9147 - val_loss: 0.4034 - val_accuracy: 0.8416\n",
      "Epoch 19/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.1892 - accuracy: 0.9151 - val_loss: 0.3856 - val_accuracy: 0.8365\n",
      "Epoch 20/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.1822 - accuracy: 0.9182 - val_loss: 0.3724 - val_accuracy: 0.8454\n",
      "Epoch 21/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.1730 - accuracy: 0.9242 - val_loss: 0.3967 - val_accuracy: 0.8492\n",
      "Epoch 22/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.1643 - accuracy: 0.9277 - val_loss: 0.3835 - val_accuracy: 0.8517\n",
      "Epoch 23/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.1623 - accuracy: 0.9252 - val_loss: 0.3945 - val_accuracy: 0.8454\n",
      "Epoch 24/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.1541 - accuracy: 0.9287 - val_loss: 0.4803 - val_accuracy: 0.8289\n",
      "Epoch 25/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.1418 - accuracy: 0.9407 - val_loss: 0.4010 - val_accuracy: 0.8441\n",
      "Epoch 26/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.1426 - accuracy: 0.9404 - val_loss: 0.4265 - val_accuracy: 0.8454\n",
      "Epoch 27/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.1322 - accuracy: 0.9410 - val_loss: 0.3980 - val_accuracy: 0.8631\n",
      "Epoch 28/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.1285 - accuracy: 0.9448 - val_loss: 0.4591 - val_accuracy: 0.8466\n",
      "Epoch 29/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.1303 - accuracy: 0.9445 - val_loss: 0.3789 - val_accuracy: 0.8657\n",
      "Epoch 30/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.1174 - accuracy: 0.9493 - val_loss: 0.4136 - val_accuracy: 0.8606\n",
      "Epoch 31/50\n",
      "99/99 [==============================] - 0s 3ms/step - loss: 0.1147 - accuracy: 0.9502 - val_loss: 0.4138 - val_accuracy: 0.8542\n",
      "Epoch 32/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.1141 - accuracy: 0.9502 - val_loss: 0.4111 - val_accuracy: 0.8619\n",
      "Epoch 33/50\n",
      "99/99 [==============================] - 0s 3ms/step - loss: 0.1090 - accuracy: 0.9521 - val_loss: 0.4351 - val_accuracy: 0.8631\n",
      "Epoch 34/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.1007 - accuracy: 0.9575 - val_loss: 0.4142 - val_accuracy: 0.8568\n",
      "Epoch 35/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.1000 - accuracy: 0.9575 - val_loss: 0.4855 - val_accuracy: 0.8428\n",
      "Epoch 36/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.1066 - accuracy: 0.9534 - val_loss: 0.4312 - val_accuracy: 0.8669\n",
      "Epoch 37/50\n",
      "99/99 [==============================] - 0s 3ms/step - loss: 0.0917 - accuracy: 0.9623 - val_loss: 0.4592 - val_accuracy: 0.8619\n",
      "Epoch 38/50\n",
      "99/99 [==============================] - 0s 3ms/step - loss: 0.0997 - accuracy: 0.9572 - val_loss: 0.4463 - val_accuracy: 0.8504\n",
      "Epoch 39/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.0899 - accuracy: 0.9648 - val_loss: 0.4837 - val_accuracy: 0.8568\n",
      "Epoch 40/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.0944 - accuracy: 0.9594 - val_loss: 0.4819 - val_accuracy: 0.8530\n",
      "Epoch 41/50\n",
      "99/99 [==============================] - 0s 3ms/step - loss: 0.0831 - accuracy: 0.9661 - val_loss: 0.4953 - val_accuracy: 0.8492\n",
      "Epoch 42/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.0882 - accuracy: 0.9604 - val_loss: 0.4506 - val_accuracy: 0.8619\n",
      "Epoch 43/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.0798 - accuracy: 0.9645 - val_loss: 0.4736 - val_accuracy: 0.8669\n",
      "Epoch 44/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.0942 - accuracy: 0.9604 - val_loss: 0.4524 - val_accuracy: 0.8644\n",
      "Epoch 45/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.0752 - accuracy: 0.9689 - val_loss: 0.4870 - val_accuracy: 0.8682\n",
      "Epoch 46/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.0772 - accuracy: 0.9718 - val_loss: 0.5076 - val_accuracy: 0.8580\n",
      "Epoch 47/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.0690 - accuracy: 0.9718 - val_loss: 0.4960 - val_accuracy: 0.8695\n",
      "Epoch 48/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.0867 - accuracy: 0.9642 - val_loss: 0.5050 - val_accuracy: 0.8657\n",
      "Epoch 49/50\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 0.0872 - accuracy: 0.9607 - val_loss: 0.5171 - val_accuracy: 0.8682\n",
      "Epoch 50/50\n",
      "99/99 [==============================] - 0s 3ms/step - loss: 0.0672 - accuracy: 0.9746 - val_loss: 0.5314 - val_accuracy: 0.8695\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f35f714b190>"
      ]
     },
     "metadata": {},
     "execution_count": 291
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(\"Accuracy\", accuracy)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "31/31 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.8764\n",
      "Accuracy 0.8763931393623352\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(\"Accuracy\", accuracy)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "31/31 [==============================] - 0s 2ms/step - loss: 0.4810 - accuracy: 0.8480\n",
      "Accuracy 0.848024308681488\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(\"Accuracy\", accuracy)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "31/31 [==============================] - 0s 964us/step - loss: 0.5752 - accuracy: 0.7214\n",
      "Accuracy 0.7213779091835022\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 31/31 [==============================] - 0s 1ms/step - loss: 0.6488 - accuracy: 0.7335\n",
    "# Accuracy 0.7335359454154968\n",
    "\n",
    "# 31/31 [==============================] - 0s 862us/step - loss: 0.5798 - accuracy: 0.6950\n",
    "# Accuracy 0.695035457611084"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Save the entire model as a SavedModel.\n",
    "# !mkdir -p saved_model\n",
    "# model.save('saved_model/my_model')\n",
    "\n",
    "# from tensorflow.keras.models import Sequential, save_model, load_model\n",
    "\n",
    "# filepath = './saved_model'\n",
    "# save_model(model, filepath)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Morgan & Higgs: Findpath with SW 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "input_file = \"dataset_100.csv\"\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "sequence, s1, s2 = df.loc[0]\n",
    "\n",
    "\n",
    "search_width_multiplier = 4\n",
    "fp = findpath.findpath_single(sequence, s1, s2, search_width_multiplier=search_width_multiplier, mp=True)\n",
    "result = fp.get_en()/100.0\n",
    "path = fp.get_path()\n",
    "\n",
    "s = s1\n",
    "pt2 = list(RNA.ptable(s2))\n",
    "fc = RNA.fold_compound(sequence)\n",
    "\n",
    "\n",
    "def find_moves(s_ptable, t_ptable):\n",
    "    \"\"\"\n",
    "    generator function, yields possible structures 1 move away\n",
    "    from the original structure by finding fitting i and j with\n",
    "    RNA pair and loop tables\n",
    "    s_ptable: current ptable\n",
    "    t_ptable: s2 end ptable\n",
    "    \"\"\"\n",
    "    # loop table\n",
    "    ls = RNA.loopidx_from_ptable(s_ptable)\n",
    "    for i in range(len(s_ptable)):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if s_ptable[i] == 0 and t_ptable[i] > i:\n",
    "            j = t_ptable[i]\n",
    "            # found j has to be empty and currently on the same loop as i\n",
    "            if s_ptable[j] == 0 and ls[i] == ls[j]:\n",
    "                yield i, j\n",
    "        # test for bp removal: i has to be paired with a different j in s2\n",
    "        j = s_ptable[i]\n",
    "        # dont remove things which are present in s2\n",
    "        if s_ptable[i] > i and s_ptable[i] != s_ptable[j] and\\\n",
    "                s_ptable[i] != t_ptable[i] and s_ptable[j] != t_ptable[j]:\n",
    "            yield -i, -j\n",
    "\n",
    "\n",
    "def fp_call(sequence, s1, s2, search_width_multiplier = 20):    \n",
    "    fp = findpath.findpath_single(sequence, s1, s2, search_width_multiplier=search_width_multiplier, mp=True)\n",
    "    result = fp.get_en()/100.0\n",
    "    path = fp.get_path()\n",
    "    # return result, path\n",
    "    return result, path\n",
    "\n",
    "\n",
    "def ij_distance(last_move, this_move, ij_moves):\n",
    "    # how far is the last move away from the current move.\n",
    "    # it is likely, that the next move is close to the last one\n",
    "    # there are better distance metrices probably...\n",
    "\n",
    "    # ij move list is supposed to be sorted, find indices\n",
    "    \n",
    "    ijmoves = ij_moves + [last_move]\n",
    "    ijmoves.sort(key=lambda x: (x[0], x[1]))\n",
    "\n",
    "    pos_old = ijmoves.index(last_move)\n",
    "    pos_new = ijmoves.index(this_move)\n",
    "\n",
    "    distance = abs(pos_old-pos_new)/len(ijmoves)\n",
    "    \n",
    "    \n",
    "    # moves left in vicinity out of total moves\n",
    "    \n",
    "    thisi, thisj = this_move\n",
    "    lasti, lastj = last_move\n",
    "    thisclose = 0\n",
    "    lastclose = 0\n",
    "\n",
    "    for i, j in ij_moves:\n",
    "        if (abs(i-thisi) < 5) and (abs(j-thisj) < 5):\n",
    "            thisclose += 1\n",
    "        if (abs(i-lasti) < 5) and (abs(j-lastj) < 5):\n",
    "            lastclose += 1\n",
    "    \n",
    "\n",
    "    thisclose /= len(ij_moves)\n",
    "    lastclose /= len(ij_moves)\n",
    "\n",
    "    # print (\"thisclose\", thisclose, lastclose, distance)\n",
    "    return distance, thisclose, lastclose\n",
    "\n",
    "    print (distance)\n",
    "\n",
    "# sample call\n",
    "ij_moves = [(3, 62), (4, 61), (6, 60), (7, 59), (9, 57), (10, 56), (11, 55), (12, 54), (15, 52), (16, 51), (17, 50), (18, 39), (19, 38), (20, 36), (21, 35), (22, 34), (23, 33), (24, 32), (42, 49), (43, 48), (64, 99)]\n",
    "last_move = (2, 63)\n",
    "this_move = (6, 60)\n",
    "ij_distance(last_move, this_move, ij_moves)\n",
    "\n",
    "# \n",
    "\n",
    "def config_distance(pt, move):\n",
    "    \"\"\"\n",
    "    are we extending / removing the outside/inside layer of a loop or adding something in the middle?\n",
    "    \"\"\"\n",
    "    i = move[0]\n",
    "    j = move[1]\n",
    "    points = 0\n",
    "\n",
    "    # if we're extending from outside to inside, the position i+1 and j-1 should be ideally unpaired\n",
    "    # inside to outside: i-1 and j+1 should be ideally unpaired\n",
    "\n",
    "    if i>0:\n",
    "        # print (\"add\") \n",
    "        # outside/inside paired?        \n",
    "        if j+1 < pt[0] and i-1 > 0: # outside - boundary check\n",
    "            if pt[i-1] == j+1:\n",
    "                points += 1\n",
    "        if pt[i+1] == j-1:\n",
    "            points += 1\n",
    "    if i<0:\n",
    "        # print (\"del\")\n",
    "        i, j = -i, -j\n",
    "        # outside/inside paired?\n",
    "        if j+1 < pt[0] and i-1 > 0: # outside - boundary check\n",
    "            if pt[i-1] == j+1:\n",
    "                points += 1\n",
    "        if pt[i+1] == j-1:\n",
    "            points += 1\n",
    "        elif pt[i+1] == 0 and pt[j-1] == 0:\n",
    "            pass\n",
    "\n",
    "    if points == 2:\n",
    "        points = 0\n",
    "    return points\n",
    "\n",
    "\n",
    "s = s1\n",
    "lasts = s\n",
    "lasti = None\n",
    "lastj = None\n",
    "\n",
    "for e, (a,b, en) in enumerate(path):\n",
    "    if (a,b) == (0,0):\n",
    "        continue  \n",
    "\n",
    "    # check where we can go, compare with our best move. \n",
    "    pt = list(RNA.ptable(s))\n",
    "\n",
    "    # check available moves, save them, sort them    \n",
    "    avail_moves = []\n",
    "    ij_moves = []\n",
    "    found_pos = None\n",
    "\n",
    "    for pos, (i,j) in enumerate(find_moves(pt, pt2)):    \n",
    "        next_en = fc.eval_move_pt(pt, i, j)\n",
    "        # mark where we found our move\n",
    "        found = (i,j) == (a,b)\n",
    "        avail_moves.append((i, j, next_en, found))\n",
    "        ij_moves.append((abs(i),abs(j)))\n",
    "\n",
    "    # sort moves independent of delete insert moves\n",
    "\n",
    "    ij_moves.sort(key=lambda x: (x[0], x[1]))\n",
    "\n",
    "\n",
    "    avail_moves.sort(key=lambda x: x[2])\n",
    "    found_list = [x[3] for x in avail_moves]\n",
    "    en_list = np.array([[x[2] for x in avail_moves]])\n",
    "    en_list_scaled = min_max_scaler.fit_transform(en_list.T).T[0]\n",
    "    \n",
    "\n",
    "    # find where our move is after sorting\n",
    "    found_pos = found_list.index(True)\n",
    "    rel_pos = found_pos * 1.0 / len(found_list)\n",
    "\n",
    "    # print (e, a,b, 'found at pos:', found_pos, 'of', len(avail_moves), ':',  1-rel_pos)\n",
    "    # print (avail_moves, a, b)\n",
    "\n",
    "    # if s != lasts:\n",
    "        # print (\"---\") \n",
    "        # print (s)\n",
    "        # print (\"---\") \n",
    "\n",
    "    # for every move we take we have to run a new findpath, see if this move will yield the ideal result\n",
    "    \n",
    "    for pos, (i,j, en, found) in enumerate(avail_moves):\n",
    "        if i > 0:\n",
    "            snew = s[:i-1] + \"(\" + s[i:j-1] + \")\" + s[j:]\n",
    "        if i < 0:\n",
    "            snew = s[:-i-1] + \".\" + s[-i:-j-1] + \".\" + s[-j:]\n",
    "        ptnew = list(RNA.ptable(snew))\n",
    "\n",
    "        result_new, path = fp_call(sequence, snew, s2)\n",
    "\n",
    "        if result_new <= result:\n",
    "            pos_result = 1\n",
    "        else:\n",
    "            pos_result = 0\n",
    "\n",
    "        if found: found = \"<-- taken\"\n",
    "        else: found = \"\"\n",
    "\n",
    "        this_move = (abs(i), abs(j))\n",
    "        last_move = (lasti, lastj)\n",
    "\n",
    "        if lasti:\n",
    "            # print (this_move, last_move, ij_moves)\n",
    "            ijd, thisclose, lastclose = ij_distance(last_move, this_move, ij_moves)\n",
    "        else:\n",
    "            ijd, thisclose, lastclose = 0, 0, 0\n",
    "\n",
    "        cd = config_distance(pt, this_move)\n",
    "\n",
    "        if lasti:\n",
    "            print (f' {snew[0:20]}, {i}, {j}, {result_new}/{pos_result}: {ijd:2.2f}, {thisclose:2.2f}, {lastclose:2.2f}, {cd}, {en}, {en_list_scaled[pos]:2.2f} {found}')\n",
    "\n",
    "            sample = 1, ijd, thisclose, lastclose, cd, en_list_scaled[pos], 1\n",
    "            sample_test_ds = sample_to_dataset(sample, test)\n",
    "            result_predictor = model.predict(sample_test_ds)[0][0]\n",
    "\n",
    "            print (f' {1}, {ijd:2.2f}, {thisclose:2.2f}, {lastclose:2.2f}, {cd}, {en_list_scaled[pos]:2.2f}, 1: prediction: {result_predictor}')\n",
    "\n",
    "\n",
    "\n",
    "    # if e==63:\n",
    "    #     print (avail_moves)\n",
    "\n",
    "    # update s for the next iteration\n",
    "    \n",
    "    lasts = s\n",
    "    lasti = abs(a)\n",
    "    lastj = abs(b)\n",
    "    if a > 0:\n",
    "        s = s[:a-1] + \"(\" + s[a:b-1] + \")\" + s[b:]\n",
    "    if a < 0:\n",
    "        s = s[:-a-1] + \".\" + s[-a:-b-1] + \".\" + s[-b:]\n",
    "\n",
    "    if e>1: \n",
    "        break\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " ..((.((.(((...((((((, -12, -54, -14.2/1: 0.36, 0.29, 0.14, 1, 90, 0.00 <-- taken\n",
      " 1, 0.36, 0.29, 0.14, 1, 0.00, 1: prediction: 0.6828221082687378\n",
      " ..((.(..((((..((((((, -7, -59, -14.2/1: 0.18, 0.33, 0.14, 1, 110, 0.03 \n",
      " 1, 0.18, 0.33, 0.14, 1, 0.03, 1: prediction: -0.31268346309661865\n",
      " ..((.((.((((..((((((, -24, -32, -13.3/0: 0.82, 0.24, 0.14, 1, 110, 0.03 \n",
      " 1, 0.82, 0.24, 0.14, 1, 0.03, 1: prediction: -2.4331817626953125\n",
      " ..((.((.((((..((((((, -64, -99, -12.7/0: 0.95, 0.05, 0.14, 1, 150, 0.10 \n",
      " 1, 0.95, 0.05, 0.14, 1, 0.10, 1: prediction: -8.228426933288574\n",
      " ..((.((.((((..(((((., -20, -36, -14.2/1: 0.64, 0.33, 0.14, 1, 210, 0.21 \n",
      " 1, 0.64, 0.33, 0.14, 1, 0.21, 1: prediction: -1.486687183380127\n",
      " ..((.((.((((..((((.(, -19, -38, -14.2/1: 0.59, 0.24, 0.14, 1, 300, 0.36 \n",
      " 1, 0.59, 0.24, 0.14, 1, 0.36, 1: prediction: -1.3077186346054077\n",
      " ..((.((..(((..((((((, -9, -57, -14.2/1: 0.23, 0.29, 0.14, 1, 310, 0.38 \n",
      " 1, 0.23, 0.29, 0.14, 1, 0.38, 1: prediction: -0.09669250249862671\n",
      " ..((.((.((((...(((((, -15, -52, -14.2/1: 0.41, 0.24, 0.14, 1, 310, 0.38 \n",
      " 1, 0.41, 0.24, 0.14, 1, 0.38, 1: prediction: -1.4521899223327637\n",
      " ...(.((.((((..((((((, -3, -62, -14.2/1: 0.05, 0.19, 0.14, 1, 330, 0.41 \n",
      " 1, 0.05, 0.19, 0.14, 1, 0.41, 1: prediction: 0.7475156784057617\n",
      " ..((.((.((((..((((((, 42, 49, -14.2/1: 0.86, 0.10, 0.14, 0, 340, 0.43 \n",
      " 1, 0.86, 0.10, 0.14, 0, 0.43, 1: prediction: -1.5531705617904663\n",
      " ..((..(.((((..((((((, -6, -60, -14.2/1: 0.14, 0.29, 0.14, 1, 350, 0.45 \n",
      " 1, 0.14, 0.29, 0.14, 1, 0.45, 1: prediction: -0.18936875462532043\n",
      " ..(..((.((((..((((((, -4, -61, -14.2/1: 0.09, 0.19, 0.14, 1, 390, 0.52 \n",
      " 1, 0.09, 0.19, 0.14, 1, 0.52, 1: prediction: 0.6279747486114502\n",
      " ..((.((.((.(..((((((, -11, -55, -14.2/1: 0.32, 0.29, 0.14, 0, 390, 0.52 \n",
      " 1, 0.32, 0.29, 0.14, 0, 0.52, 1: prediction: -0.9867163896560669\n",
      " ..((.((.((((..((.(((, -17, -50, -14.2/1: 0.50, 0.14, 0.14, 1, 390, 0.52 \n",
      " 1, 0.50, 0.14, 0.14, 1, 0.52, 1: prediction: -1.5610805749893188\n",
      " ..((.((.((((..(((.((, -18, -39, -14.2/1: 0.55, 0.19, 0.14, 1, 390, 0.52 \n",
      " 1, 0.55, 0.19, 0.14, 1, 0.52, 1: prediction: -1.482547402381897\n",
      " ..((.((.((((..((((((, -23, -33, -13.3/0: 0.77, 0.24, 0.14, 0, 420, 0.57 \n",
      " 1, 0.77, 0.24, 0.14, 0, 0.57, 1: prediction: -2.5425515174865723\n",
      " ..((.((.((((..((((((, 43, 48, -14.2/1: 0.91, 0.10, 0.14, 0, 430, 0.59 \n",
      " 1, 0.91, 0.10, 0.14, 0, 0.59, 1: prediction: 0.5762128829956055\n",
      " ..((.((.((((..(.((((, -16, -51, -14.2/1: 0.45, 0.19, 0.14, 0, 500, 0.71 \n",
      " 1, 0.45, 0.19, 0.14, 0, 0.71, 1: prediction: -2.3422980308532715\n",
      " ..((.((.((((..((((((, -21, -35, -13.3/0: 0.68, 0.33, 0.14, 0, 600, 0.88 \n",
      " 1, 0.68, 0.33, 0.14, 0, 0.88, 1: prediction: -3.1054506301879883\n",
      " ..((.((.((((..((((((, -22, -34, -13.3/0: 0.73, 0.29, 0.14, 0, 660, 0.98 \n",
      " 1, 0.73, 0.29, 0.14, 0, 0.98, 1: prediction: -3.650803565979004\n",
      " ..((.((.(.((..((((((, -10, -56, -14.2/1: 0.27, 0.29, 0.14, 0, 670, 1.00 \n",
      " 1, 0.27, 0.29, 0.14, 0, 1.00, 1: prediction: -2.473306179046631\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# A few random samples\n",
    "\n",
    "\n",
    "# Generate predictions for samples\n",
    "predictions = model.predict(test_ds)\n",
    "\n",
    "predictions\n",
    "# sample_test\n",
    "# test_ds"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-1.3807142],\n",
       "       [-0.6300998],\n",
       "       [-2.0548708],\n",
       "       ...,\n",
       "       [-1.4828502],\n",
       "       [-1.810354 ],\n",
       "       [-1.9349219]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "# A utility method to create a tf.data from a dictionary\n",
    "def dict_to_dataset(inputdict, labels, shuffle=True, batch_size=32):\n",
    "  # dataframe = dataframe.copy()\n",
    "  # labels = dataframe.pop('target')\n",
    "  ds = tf.data.Dataset.from_tensor_slices((inputdict, labels))\n",
    "  # ds = tf.data.Dataset.from_tensor_slices([45240,  0.5,  0.142857,  0.047619,  0,  0.930233,       0])\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))  \n",
    "  print ('ds', ds)\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds\n",
    "\n",
    "def sample_to_dataset(sample, test):\n",
    "\n",
    "  sample_test = test.iloc[0:1].copy()\n",
    "  row0 = sample_test.index[0]\n",
    "  sample_test.at[row0 ,'Unnamed: 0'] = 2 # irrelevant\n",
    "  sample_test.at[row0 ,'4'] = sample[1]\n",
    "  sample_test.at[row0 ,'5'] = sample[2]\n",
    "  sample_test.at[row0 ,'6'] = sample[3]\n",
    "  sample_test.at[row0 ,'7'] = sample[4]\n",
    "  sample_test.at[row0 ,'8'] = sample[5]\n",
    "  sample_test.at[row0 ,'target'] = 2 # irrelevant\n",
    "\n",
    "  sample_test_ds = df_to_dataset(sample_test, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "  return sample_test_ds\n",
    "\n",
    "\n",
    "\n",
    "sample = [1, 0.82, 0.44, 0.24, 1, 0.05, 1]\n",
    "sample = 1, 0.36, 0.29, 0.14, 1, 0.00, 1\n",
    "sample_test_ds = sample_to_dataset(sample, test)\n",
    "result_predictor = model.predict(sample_test_ds)[0][0]\n",
    "print (result_predictor)\n",
    "\n",
    "\n",
    "predictions = model.predict(sample_test_ds)\n",
    "predictions"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.7363248\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.7363248]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "source": [
    "filename_samples = f'./dataset_100.csv'\n",
    "samples_df = pd.read_csv(filename_samples)\n",
    "\n",
    "sequence = ''\n",
    "s1 = ''\n",
    "s2 = ''\n",
    "\n",
    "for index, row in samples_df.iterrows():\n",
    "    if index != 56:\n",
    "        continue\n",
    "    sequence = row.sequence\n",
    "    s1 = row.s1\n",
    "    s2 = row.s2\n",
    "\n",
    "\n",
    "print (sequence)\n",
    "print (s1)\n",
    "print (s2)\n",
    "\n",
    "fc = RNA.fold_compound(sequence)\n",
    "en1 = fc.eval_structure(s1)\n",
    "en2 = fc.eval_structure(s2)\n",
    "\n",
    "def fp_call_sw(sequence, s1, s2, search_width):    \n",
    "    fp = findpath.findpath_single(sequence, s1, s2, search_width=search_width, mp=True)\n",
    "    result = fp.get_en()/100.0\n",
    "    path = fp.get_path()\n",
    "    # return result, path\n",
    "    return result, path\n",
    "\n",
    "\n",
    "sw = 1\n",
    "sw = 100\n",
    "\n",
    "r, p = fp_call_sw(sequence, s1, s2, sw)\n",
    "p = [(i[0], i[1]) for i in p]\n",
    "\n",
    "print_moves(sequence, s1, s2, p)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GGCUCAGUCGACGCUGCACCGCCUAGACCCGAUCGCUAAGAAGGAAAACAGUCAGAUGACCGAGUAUUCGAACGCUGCCGUAUCUCUCGCAUAUAACACA\n",
      "(((.((((....))))....))).....((..((.....)).))......(((....)))((((....((........)).....))))...........\n",
      "((..((((....))))..))..............((..(((.((...((.(.(((.((..(((....)))..))))).))).))))).))..........\n",
      "GGCUCAGUCGACGCUGCACCGCCUAGACCCGAUCGCUAAGAAGGAAAACAGUCAGAUGACCGAGUAUUCGAACGCUGCCGUAUCUCUCGCAUAUAACACA\n",
      "(((.((((....))))....))).....((..((.....)).))......(((....)))((((....((........)).....))))........... \u001b[93m[   0,    0 ]\u001b[0m -12.90\n",
      "\u001b[93m\u001b[1m.\u001b[0m((.((((....))))....))\u001b[93m\u001b[1m.\u001b[0m.....((..((.....)).))......(((....)))((((....((........)).....))))........... \u001b[93m[  -1,  -23 ]\u001b[0m  -9.40\n",
      ".\u001b[93m\u001b[1m.\u001b[0m(.((((....))))....)\u001b[93m\u001b[1m.\u001b[0m......((..((.....)).))......(((....)))((((....((........)).....))))........... \u001b[93m[  -2,  -22 ]\u001b[0m  -6.10\n",
      "..\u001b[93m\u001b[1m.\u001b[0m.((((....))))....\u001b[93m\u001b[1m.\u001b[0m.......((..((.....)).))......(((....)))((((....((........)).....))))........... \u001b[93m[  -3,  -21 ]\u001b[0m  -9.30\n",
      "\u001b[93m\u001b[1m(\u001b[0m...((((....))))...\u001b[93m\u001b[1m)\u001b[0m........((..((.....)).))......(((....)))((((....((........)).....))))........... \u001b[93m[   1,   20 ]\u001b[0m  -8.50\n",
      "(\u001b[93m\u001b[1m(\u001b[0m..((((....))))..\u001b[93m\u001b[1m)\u001b[0m)........((..((.....)).))......(((....)))((((....((........)).....))))........... \u001b[93m[   2,   19 ]\u001b[0m -12.80\n",
      "((..((((....))))..))........((..((.....)).))......(((....)))(\u001b[93m\u001b[1m.\u001b[0m((....((........)).....))\u001b[93m\u001b[1m.\u001b[0m)........... \u001b[93m[ -62,  -88 ]\u001b[0m  -6.80\n",
      "((..((((....))))..))........((..((.....)).))......(((....)))\u001b[93m\u001b[1m.\u001b[0m.((....((........)).....)).\u001b[93m\u001b[1m.\u001b[0m........... \u001b[93m[ -61,  -89 ]\u001b[0m  -7.50\n",
      "((..((((....))))..))........((..((.....)).))......(((....)))..\u001b[93m\u001b[1m.\u001b[0m(....((........)).....)\u001b[93m\u001b[1m.\u001b[0m............. \u001b[93m[ -63,  -87 ]\u001b[0m  -6.20\n",
      "((..((((....))))..))........((..((.....)).))......(((....)))...\u001b[93m\u001b[1m.\u001b[0m....((........)).....\u001b[93m\u001b[1m.\u001b[0m.............. \u001b[93m[ -64,  -86 ]\u001b[0m  -7.70\n",
      "((..((((....))))..))........((..((.....)).))......(((....)))........\u001b[93m\u001b[1m.\u001b[0m(........)\u001b[93m\u001b[1m.\u001b[0m.................... \u001b[93m[ -69,  -80 ]\u001b[0m  -6.10\n",
      "((..((((....))))..))........((..((.....)).))......(((....))).........\u001b[93m\u001b[1m.\u001b[0m........\u001b[93m\u001b[1m.\u001b[0m..................... \u001b[93m[ -70,  -79 ]\u001b[0m  -8.60\n",
      "((..((((....))))..))........((..((.....)).))......(((....))).\u001b[93m\u001b[1m(\u001b[0m......\u001b[93m\u001b[1m)\u001b[0m............................... \u001b[93m[  62,   69 ]\u001b[0m  -6.20\n",
      "((..((((....))))..))........((..((.....)).))......(((....)))\u001b[93m\u001b[1m(\u001b[0m(......)\u001b[93m\u001b[1m)\u001b[0m.............................. \u001b[93m[  61,   70 ]\u001b[0m  -8.60\n",
      "((..((((....))))..))........((..((.....)).))......(((....)))((\u001b[93m\u001b[1m(\u001b[0m....\u001b[93m\u001b[1m)\u001b[0m)).............................. \u001b[93m[  63,   68 ]\u001b[0m  -9.80\n",
      "((..((((....))))..))........((..((.....)).))......\u001b[93m\u001b[1m.\u001b[0m((....))\u001b[93m\u001b[1m.\u001b[0m(((....))).............................. \u001b[93m[ -51,  -60 ]\u001b[0m  -6.70\n",
      "((..((((....))))..))........((..((.....)).)).......\u001b[93m\u001b[1m.\u001b[0m(....)\u001b[93m\u001b[1m.\u001b[0m.(((....))).............................. \u001b[93m[ -52,  -59 ]\u001b[0m  -5.70\n",
      "((..((((....))))..))........((..((.....)).))........\u001b[93m\u001b[1m.\u001b[0m....\u001b[93m\u001b[1m.\u001b[0m..(((....))).............................. \u001b[93m[ -53,  -58 ]\u001b[0m  -8.30\n",
      "((..((((....))))..))........((..((.....)).)).............\u001b[93m\u001b[1m(\u001b[0m..(((....)))..\u001b[93m\u001b[1m)\u001b[0m........................... \u001b[93m[  58,   73 ]\u001b[0m  -7.10\n",
      "((..((((....))))..))........((..((.....)).))............\u001b[93m\u001b[1m(\u001b[0m(..(((....)))..)\u001b[93m\u001b[1m)\u001b[0m.......................... \u001b[93m[  57,   74 ]\u001b[0m  -7.10\n",
      "((..((((....))))..))........((..((.....)).))..........\u001b[93m\u001b[1m(\u001b[0m.((..(((....)))..))\u001b[93m\u001b[1m)\u001b[0m......................... \u001b[93m[  55,   75 ]\u001b[0m  -6.70\n",
      "((..((((....))))..))........((..((.....)).)).........\u001b[93m\u001b[1m(\u001b[0m(.((..(((....)))..)))\u001b[93m\u001b[1m)\u001b[0m........................ \u001b[93m[  54,   76 ]\u001b[0m  -8.10\n",
      "((..((((....))))..))........((..((.....)).))........\u001b[93m\u001b[1m(\u001b[0m((.((..(((....)))..))))\u001b[93m\u001b[1m)\u001b[0m....................... \u001b[93m[  53,   77 ]\u001b[0m -10.40\n",
      "((..((((....))))..))........\u001b[93m\u001b[1m.\u001b[0m(..((.....)).)\u001b[93m\u001b[1m.\u001b[0m........(((.((..(((....)))..)))))....................... \u001b[93m[ -29,  -44 ]\u001b[0m  -7.10\n",
      "((..((((....))))..)).........\u001b[93m\u001b[1m.\u001b[0m..((.....)).\u001b[93m\u001b[1m.\u001b[0m.........(((.((..(((....)))..)))))....................... \u001b[93m[ -30,  -43 ]\u001b[0m  -7.80\n",
      "((..((((....))))..))............\u001b[93m\u001b[1m.\u001b[0m(.....)\u001b[93m\u001b[1m.\u001b[0m...........(((.((..(((....)))..)))))....................... \u001b[93m[ -33,  -41 ]\u001b[0m  -6.60\n",
      "((..((((....))))..)).............\u001b[93m\u001b[1m.\u001b[0m.....\u001b[93m\u001b[1m.\u001b[0m............(((.((..(((....)))..)))))....................... \u001b[93m[ -34,  -40 ]\u001b[0m  -8.50\n",
      "((..((((....))))..))............................\u001b[93m\u001b[1m(\u001b[0m...(((.((..(((....)))..)))))..\u001b[93m\u001b[1m)\u001b[0m.................... \u001b[93m[  49,   80 ]\u001b[0m  -6.50\n",
      "((..((((....))))..))...........................\u001b[93m\u001b[1m(\u001b[0m(...(((.((..(((....)))..)))))..)\u001b[93m\u001b[1m)\u001b[0m................... \u001b[93m[  48,   81 ]\u001b[0m  -8.10\n",
      "((..((((....))))..))...........................((.\u001b[93m\u001b[1m(\u001b[0m.(((.((..(((....)))..))))).\u001b[93m\u001b[1m)\u001b[0m))................... \u001b[93m[  51,   79 ]\u001b[0m  -8.80\n",
      "((..((((....))))..))......................\u001b[93m\u001b[1m(\u001b[0m....((.(.(((.((..(((....)))..))))).)))..\u001b[93m\u001b[1m)\u001b[0m................ \u001b[93m[  43,   84 ]\u001b[0m  -5.40\n",
      "((..((((....))))..))......................(\u001b[93m\u001b[1m(\u001b[0m...((.(.(((.((..(((....)))..))))).))).\u001b[93m\u001b[1m)\u001b[0m)................ \u001b[93m[  44,   83 ]\u001b[0m  -7.10\n",
      "((..((((....))))..))...................\u001b[93m\u001b[1m(\u001b[0m..((...((.(.(((.((..(((....)))..))))).))).)).\u001b[93m\u001b[1m)\u001b[0m.............. \u001b[93m[  40,   86 ]\u001b[0m \u001b[91m\u001b[1m -4.80\u001b[0m\n",
      "((..((((....))))..))...................(\u001b[93m\u001b[1m(\u001b[0m.((...((.(.(((.((..(((....)))..))))).))).))\u001b[93m\u001b[1m)\u001b[0m).............. \u001b[93m[  41,   85 ]\u001b[0m  -7.80\n",
      "((..((((....))))..))..............\u001b[93m\u001b[1m(\u001b[0m....((.((...((.(.(((.((..(((....)))..))))).))).))))...\u001b[93m\u001b[1m)\u001b[0m.......... \u001b[93m[  35,   90 ]\u001b[0m  -5.60\n",
      "((..((((....))))..))..............(\u001b[93m\u001b[1m(\u001b[0m...((.((...((.(.(((.((..(((....)))..))))).))).))))..\u001b[93m\u001b[1m)\u001b[0m).......... \u001b[93m[  36,   89 ]\u001b[0m  -9.10\n",
      "((..((((....))))..))..............((..\u001b[93m\u001b[1m(\u001b[0m((.((...((.(.(((.((..(((....)))..))))).))).))))\u001b[93m\u001b[1m)\u001b[0m.)).......... \u001b[93m[  39,   87 ]\u001b[0m -10.80\n",
      "S:  -4.80 kcal/mol | B:   8.10 kcal/mol | E[start]:-12.90 E[end]:-10.80\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-4.8"
      ]
     },
     "metadata": {},
     "execution_count": 192
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "source": [
    "s = s1\n",
    "fc = RNA.fold_compound(sequence)\n",
    "pt = list(RNA.ptable(s))\n",
    "pt2 = list(RNA.ptable(s2))\n",
    "\n",
    "lasts = s\n",
    "lasti = 0\n",
    "lastj = 0\n",
    "last_move = (0,0)\n",
    "\n",
    "en_max = fc.eval_structure(s)\n",
    "\n",
    "found_moves = [(0, 0)]\n",
    "\n",
    "while True:\n",
    "\n",
    "    # check where we can go, compare with our best move. \n",
    "    pt = list(RNA.ptable(s))\n",
    "\n",
    "    # check available moves, save them, sort them    \n",
    "    avail_moves = []\n",
    "    ij_moves = []\n",
    "    found_pos = None\n",
    "\n",
    "    # collect moves from the generator function, then sort them.\n",
    "    for pos, (i,j) in enumerate(find_moves(pt, pt2)):    \n",
    "        next_en = fc.eval_move_pt(pt, i, j)\n",
    "        # mark where we found our move\n",
    "        found = (i,j) == (a,b)\n",
    "        avail_moves.append((i, j, next_en, found))\n",
    "        ij_moves.append((abs(i),abs(j)))\n",
    "\n",
    "\n",
    "    avail_moves.sort(key=lambda x: x[2])\n",
    "    found_list = [x[3] for x in avail_moves]\n",
    "    en_list = np.array([[x[2] for x in avail_moves]])\n",
    "    en_list_scaled = min_max_scaler.fit_transform(en_list.T).T[0]\n",
    "\n",
    "\n",
    "    # print (s, pt, avail_moves)\n",
    "    # print (en_list_scaled)\n",
    "\n",
    "    candidates = []\n",
    "\n",
    "    for pos, (i,j, en, found) in enumerate(avail_moves):\n",
    "\n",
    "        this_move = (abs(i), abs(j))\n",
    "        last_move = (lasti, lastj)\n",
    "\n",
    "        if lasti:\n",
    "            # print (this_move, last_move, ij_moves)\n",
    "            ijd, thisclose, lastclose = ij_distance(last_move, this_move, ij_moves)\n",
    "        else:\n",
    "            ijd, thisclose, lastclose = 0, 0, 0\n",
    "\n",
    "        cd = config_distance(pt, this_move)\n",
    "\n",
    "        en_scaled = en_list_scaled[pos]\n",
    "\n",
    "        # run the prediction\n",
    "        sample = 1, ijd, thisclose, lastclose, cd, en_list_scaled[pos], 1\n",
    "        sample_test_ds = sample_to_dataset(sample, test)\n",
    "        result_predictor = model.predict(sample_test_ds)[0][0]\n",
    "\n",
    "        candidates.append((i, j, en_scaled, result_predictor))\n",
    "        \n",
    "        # print (ijd, thisclose, lastclose, cd, en_scaled, result_predictor)\n",
    "\n",
    "    candidates.sort(key=lambda x: -x[3])\n",
    "    # candidates.sort(key=lambda x: x[2])\n",
    "\n",
    "    # chosen move\n",
    "    i = candidates[0][0]\n",
    "    j = candidates[0][1]\n",
    "\n",
    "\n",
    "\n",
    "    lasts = s\n",
    "    lasti = abs(i)\n",
    "    lastj = abs(j)\n",
    "    if i > 0:\n",
    "        s = s[:i-1] + \"(\" + s[i:j-1] + \")\" + s[j:]\n",
    "    if i < 0:\n",
    "        s = s[:-i-1] + \".\" + s[-i:-j-1] + \".\" + s[-j:]\n",
    "\n",
    "    en_new = fc.eval_structure(s)\n",
    "    if en_new > en_max:\n",
    "        en_max = en_new\n",
    "\n",
    "    # print (s, i, j, en_new, en_max)\n",
    "\n",
    "    found_moves.append((i, j))\n",
    "\n",
    "    if s==s2:\n",
    "        break\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "source": [
    "print_moves(sequence, s1, s2, found_moves)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GGCUCAGUCGACGCUGCACCGCCUAGACCCGAUCGCUAAGAAGGAAAACAGUCAGAUGACCGAGUAUUCGAACGCUGCCGUAUCUCUCGCAUAUAACACA\n",
      "(((.((((....))))....))).....((..((.....)).))......(((....)))((((....((........)).....))))........... \u001b[93m[   0,    0 ]\u001b[0m -12.90\n",
      "(((.((((....))))....))).....((..((.....)).))......(((....)))(((\u001b[93m\u001b[1m.\u001b[0m....((........)).....\u001b[93m\u001b[1m.\u001b[0m)))........... \u001b[93m[ -64,  -86 ]\u001b[0m  -9.30\n",
      "(((.((((....))))....))).....((..((.....)).))......(((....)))((\u001b[93m\u001b[1m.\u001b[0m.....((........))......\u001b[93m\u001b[1m.\u001b[0m))........... \u001b[93m[ -63,  -87 ]\u001b[0m  -7.40\n",
      "(((.((((....))))....))).....((..((.....)).))......(((....)))(\u001b[93m\u001b[1m.\u001b[0m......((........)).......\u001b[93m\u001b[1m.\u001b[0m)........... \u001b[93m[ -62,  -88 ]\u001b[0m  -4.90\n",
      "(((.((((....))))....))).....((..((.....)).))......(((....)))\u001b[93m\u001b[1m.\u001b[0m.......((........))........\u001b[93m\u001b[1m.\u001b[0m........... \u001b[93m[ -61,  -89 ]\u001b[0m  -7.80\n",
      "(((.((((....))))....))).....((..((.....)).))......(((....)))........\u001b[93m\u001b[1m.\u001b[0m(........)\u001b[93m\u001b[1m.\u001b[0m.................... \u001b[93m[ -69,  -80 ]\u001b[0m  -6.20\n",
      "(((.((((....))))....))).....((..((.....)).))......(((....))).........\u001b[93m\u001b[1m.\u001b[0m........\u001b[93m\u001b[1m.\u001b[0m..................... \u001b[93m[ -70,  -79 ]\u001b[0m  -8.70\n",
      "(((.((((....))))....))).....((..((.....)).))......(((....))).\u001b[93m\u001b[1m(\u001b[0m......\u001b[93m\u001b[1m)\u001b[0m............................... \u001b[93m[  62,   69 ]\u001b[0m  -6.30\n",
      "(((.((((....))))....))).....((..((.....)).))......(((....)))\u001b[93m\u001b[1m(\u001b[0m(......)\u001b[93m\u001b[1m)\u001b[0m.............................. \u001b[93m[  61,   70 ]\u001b[0m  -8.70\n",
      "(((.((((....))))....))).....((..((.....)).))......(((....)))((\u001b[93m\u001b[1m(\u001b[0m....\u001b[93m\u001b[1m)\u001b[0m)).............................. \u001b[93m[  63,   68 ]\u001b[0m  -9.90\n",
      "(((.((((....))))....))).....((..((.....)).))....\u001b[93m\u001b[1m(\u001b[0m.(((....)))(((....))).........\u001b[93m\u001b[1m)\u001b[0m.................... \u001b[93m[  49,   80 ]\u001b[0m  -5.90\n",
      "(((.((((....))))....))).....((..((.....)).))...\u001b[93m\u001b[1m(\u001b[0m(.(((....)))(((....))).........)\u001b[93m\u001b[1m)\u001b[0m................... \u001b[93m[  48,   81 ]\u001b[0m  -7.50\n",
      "(((.((((....))))....))).....((..\u001b[93m\u001b[1m.\u001b[0m(.....)\u001b[93m\u001b[1m.\u001b[0m.))...((.(((....)))(((....))).........))................... \u001b[93m[ -33,  -41 ]\u001b[0m  -5.50\n",
      "(((.((((....))))....))).....((...\u001b[93m\u001b[1m.\u001b[0m.....\u001b[93m\u001b[1m.\u001b[0m..))...((.(((....)))(((....))).........))................... \u001b[93m[ -34,  -40 ]\u001b[0m  -6.00\n",
      "(((.((((....))))....))).....((............))...((.\u001b[93m\u001b[1m.\u001b[0m((....))\u001b[93m\u001b[1m.\u001b[0m(((....))).........))................... \u001b[93m[ -51,  -60 ]\u001b[0m  -2.90\n",
      "(((.((((....))))....))).....((............))...((..\u001b[93m\u001b[1m.\u001b[0m(....)\u001b[93m\u001b[1m.\u001b[0m.(((....))).........))................... \u001b[93m[ -52,  -59 ]\u001b[0m  -1.90\n",
      "(((.((((....))))....))).....((............))...((...\u001b[93m\u001b[1m.\u001b[0m....\u001b[93m\u001b[1m.\u001b[0m..(((....))).........))................... \u001b[93m[ -53,  -58 ]\u001b[0m  -3.60\n",
      "(((.((((....))))....))).....((............))...((...\u001b[93m\u001b[1m(\u001b[0m.......(((....)))......\u001b[93m\u001b[1m)\u001b[0m..))................... \u001b[93m[  53,   77 ]\u001b[0m  -2.10\n",
      "(((.((((....))))....))).....((............))...((.\u001b[93m\u001b[1m(\u001b[0m.(.......(((....)))......).\u001b[93m\u001b[1m)\u001b[0m))................... \u001b[93m[  51,   79 ]\u001b[0m  -2.80\n",
      "(((.((((....))))....))).....((............))...((.(.(....\u001b[93m\u001b[1m(\u001b[0m..(((....)))..\u001b[93m\u001b[1m)\u001b[0m...).)))................... \u001b[93m[  58,   73 ]\u001b[0m  -2.30\n",
      "(((.((((....))))....))).....((............))...((.(.(...\u001b[93m\u001b[1m(\u001b[0m(..(((....)))..)\u001b[93m\u001b[1m)\u001b[0m..).)))................... \u001b[93m[  57,   74 ]\u001b[0m  -3.10\n",
      "(((.((((....))))....))).....((............))...((.(.(.\u001b[93m\u001b[1m(\u001b[0m.((..(((....)))..))\u001b[93m\u001b[1m)\u001b[0m.).)))................... \u001b[93m[  55,   75 ]\u001b[0m  -4.60\n",
      "(((.((((....))))....))).....((............))...((.(.(\u001b[93m\u001b[1m(\u001b[0m(.((..(((....)))..)))\u001b[93m\u001b[1m)\u001b[0m).)))................... \u001b[93m[  54,   76 ]\u001b[0m  -9.30\n",
      "((\u001b[93m\u001b[1m.\u001b[0m.((((....))))....\u001b[93m\u001b[1m.\u001b[0m)).....((............))...((.(.(((.((..(((....)))..))))).)))................... \u001b[93m[  -3,  -21 ]\u001b[0m  -5.80\n",
      "((..((((....)))).....)).....\u001b[93m\u001b[1m.\u001b[0m(............)\u001b[93m\u001b[1m.\u001b[0m...((.(.(((.((..(((....)))..))))).)))................... \u001b[93m[ -29,  -44 ]\u001b[0m  -2.50\n",
      "((..((((....)))).....))......\u001b[93m\u001b[1m.\u001b[0m............\u001b[93m\u001b[1m.\u001b[0m....((.(.(((.((..(((....)))..))))).)))................... \u001b[93m[ -30,  -43 ]\u001b[0m  -5.40\n",
      "((..((((....)))).....))...................\u001b[93m\u001b[1m(\u001b[0m....((.(.(((.((..(((....)))..))))).)))..\u001b[93m\u001b[1m)\u001b[0m................ \u001b[93m[  43,   84 ]\u001b[0m  -2.00\n",
      "((..((((....)))).....))...................(\u001b[93m\u001b[1m(\u001b[0m...((.(.(((.((..(((....)))..))))).))).\u001b[93m\u001b[1m)\u001b[0m)................ \u001b[93m[  44,   83 ]\u001b[0m  -3.70\n",
      "((..((((....)))).....))................\u001b[93m\u001b[1m(\u001b[0m..((...((.(.(((.((..(((....)))..))))).))).)).\u001b[93m\u001b[1m)\u001b[0m.............. \u001b[93m[  40,   86 ]\u001b[0m \u001b[91m\u001b[1m -1.40\u001b[0m\n",
      "((..((((....)))).....))................(\u001b[93m\u001b[1m(\u001b[0m.((...((.(.(((.((..(((....)))..))))).))).))\u001b[93m\u001b[1m)\u001b[0m).............. \u001b[93m[  41,   85 ]\u001b[0m  -4.40\n",
      "((..((((....)))).....))...............\u001b[93m\u001b[1m(\u001b[0m((.((...((.(.(((.((..(((....)))..))))).))).))))\u001b[93m\u001b[1m)\u001b[0m............. \u001b[93m[  39,   87 ]\u001b[0m  -5.70\n",
      "((..((((....)))).....))...........\u001b[93m\u001b[1m(\u001b[0m...(((.((...((.(.(((.((..(((....)))..))))).))).)))))..\u001b[93m\u001b[1m)\u001b[0m.......... \u001b[93m[  35,   90 ]\u001b[0m  -3.70\n",
      "((..((((....)))).....))...........(\u001b[93m\u001b[1m(\u001b[0m..(((.((...((.(.(((.((..(((....)))..))))).))).))))).\u001b[93m\u001b[1m)\u001b[0m).......... \u001b[93m[  36,   89 ]\u001b[0m  -7.40\n",
      "\u001b[93m\u001b[1m.\u001b[0m(..((((....)))).....)\u001b[93m\u001b[1m.\u001b[0m...........((..(((.((...((.(.(((.((..(((....)))..))))).))).))))).)).......... \u001b[93m[  -1,  -23 ]\u001b[0m  -3.90\n",
      ".\u001b[93m\u001b[1m.\u001b[0m..((((....)))).....\u001b[93m\u001b[1m.\u001b[0m............((..(((.((...((.(.(((.((..(((....)))..))))).))).))))).)).......... \u001b[93m[  -2,  -22 ]\u001b[0m  -7.30\n",
      ".\u001b[93m\u001b[1m(\u001b[0m..((((....))))..\u001b[93m\u001b[1m)\u001b[0m...............((..(((.((...((.(.(((.((..(((....)))..))))).))).))))).)).......... \u001b[93m[   2,   19 ]\u001b[0m  -6.80\n",
      "\u001b[93m\u001b[1m(\u001b[0m(..((((....))))..)\u001b[93m\u001b[1m)\u001b[0m..............((..(((.((...((.(.(((.((..(((....)))..))))).))).))))).)).......... \u001b[93m[   1,   20 ]\u001b[0m -10.80\n",
      "S:  -1.40 kcal/mol | B:  11.50 kcal/mol | E[start]:-12.90 E[end]:-10.80\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-1.4"
      ]
     },
     "metadata": {},
     "execution_count": 195
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}